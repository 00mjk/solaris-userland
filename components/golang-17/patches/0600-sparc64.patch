In-Progress SPARC64 Port Work based on Go 1.7

Not yet suitable for upstream as not all tests pass (though all but one of
the standard runtime tests do) and upstream is now on 1.8 which requires a
new compiler backend.

diff --git a/doc/asm.html b/doc/asm.html
index 3e03c54..2b71c56 100644
--- a/doc/asm.html
+++ b/doc/asm.html
@@ -838,6 +838,61 @@ It is a scaled mode as on the x86, but the only scale allowed is <code>1</code>.
 
 </ul>
 
+<h3 id="sparc64">64-bit SPARC, a.k.a. sparc64</h3>
+
+<p>
+The 64-bit SPARC port is in an experimental state.
+</p>
+
+<!-- TODO (shawn): needs update; we do use register windows (but not in the
+     general sense) now and we now use the traditional names for the R1-R31
+     registers to ease debugging although we do not use them in the
+     traditional manner.
+<p>
+The port does not use register windows, the toolchain uses global register names. The integer registers are <code>R1</code>, <code>R2</code>, ..., <code>R31</code>. <code>ZR</code> is hardwired to zero. There are no <code>%g</code>, <code>%i</code>, <code>%o</code>, or <code>%l</code> registers.
+</p>
+-->
+
+<p>
+SPARC has very unusual stack layout requirements, which forces the port to use a stack layout that is System V-compatible, but with register window <code>save</code> and <code>restore</code> instructions replaced by their manual counterparts. This is invisible to the programmer as long as he is using the <code>SP</code> and <code>FP</code> pseudo-registers. One advantage of this stack-layout is that native debugging tools understand it.
+</p>
+
+<p>
+The single-precision (32-bit) and double-precision (64-bit) floating-point registers use different names, as it is conventional for this architecture.
+There are 32 single-precision floating point registers: <code>F0</code>, <code>F1</code>, <code>F2</code>, ..., <code>F31</code>.
+There are also 32 double-precision floating point registers: <code>D0</code>, <code>D2</code>, <code>D4</code>, ..., <code>D62</code>.
+Notice that 64-bit floating point registers use even names, there is no <code>D1</code>, for example, and register numbering goes up to <code>D62</code>.
+</p>
+
+<p>
+The first half (<code>D0</code> to <code>D30</code>) of the double-precision registers alias the single-precision registers. For example <code>D6</code> aliases <code>F6</code> and <code>F7</code>. <code>D18</code> aliases <code>F18</code> and <code>F19</code>. The upper half of the double-precision registers does not alias the single-precision registers. That is, <code>D48</code> is not aliased to any 32-bit registers.
+</p>
+
+<p>
+The toolchain does not support quad-precision (128-bit) floating point yet.
+</p>
+
+<p>
+SPARC64 has two sets of flags, and operations that set flags modify both sets. One set of flags (<code>ICC</code>) is set after interpreting the result as a 32-bit value, the other one (<code>XCC</code>) is set after interpreting the result as a 64-bit value. Unlike other architectures, conditional jumps are two-operand instructions; the first operand specifies which set of condition flags to use (<code>ICC</code> or <code>XCC</code>). To simplify the compiler, the toolchain introduces single-operand aliases for these instructions. For example, <code>BNEW label</code> is an alias for <code>BNE ICC, label</code>, and <code>BGED label</code> is an alias for <code>BGE XCC, label</code>.
+</p>
+
+<p>
+Addressing modes:
+</p>
+
+<ul>
+
+<li>
+<code>(R5)(R6*1)</code>: The location at <code>R5</code> plus <code>R6</code>. It is a scaled
+mode as on the x86, but the only scale allowed is <code>1</code>.
+</li>
+
+<li>
+<code>(R5+R6)</code>: Alias for <code>(R5)(R6*1)</code>
+</li>
+
+</ul>
+
 <h3 id="unsupported_opcodes">Unsupported opcodes</h3>
 
 <p>
diff --git a/doc/go1.7.html b/doc/go1.7.html
index 6839c5e..3410f41 100644
--- a/doc/go1.7.html
+++ b/doc/go1.7.html
@@ -357,7 +357,7 @@ the code generation changes alone typically reduce program CPU time by 5-35%.
 </p>
 
 <p>
-<!-- git log &#45&#45grep '-[0-9][0-9]\.[0-9][0-9]%' go1.6.. -->
+<!-- git log -''-grep '-[0-9][0-9]\.[0-9][0-9]%' go1.6.. -->
 There have been significant optimizations bringing more than 10% improvements
 to implementations in the
 <a href="/pkg/crypto/sha1/"><code>crypto/sha1</code></a>,
@@ -552,10 +552,9 @@ The
 
 <dd>
 <p>
-As noted above,
-there are significant performance optimizations throughout the package.
+There are many performance optimizations throughout the package.
 Decompression speed is improved by about 10%,
-while compression speed for <code>DefaultCompression</code> is roughly doubled.
+while compression for <code>DefaultCompression</code> is twice as fast.
 </p>
 
 <p>
diff --git a/src/cmd/asm/internal/arch/arch.go b/src/cmd/asm/internal/arch/arch.go
index 4b5b46a..ee82630 100644
--- a/src/cmd/asm/internal/arch/arch.go
+++ b/src/cmd/asm/internal/arch/arch.go
@@ -10,6 +10,7 @@ import (
 	"cmd/internal/obj/arm64"
 	"cmd/internal/obj/mips"
 	"cmd/internal/obj/ppc64"
+	"cmd/internal/obj/sparc64"
 	"cmd/internal/obj/s390x"
 	"cmd/internal/obj/x86"
 	"fmt"
@@ -79,6 +80,8 @@ func Set(GOARCH string) *Arch {
 		a := archS390x()
 		a.LinkArch = &s390x.Links390x
 		return a
+	case "sparc64":
+		return archSparc64()
 	}
 	return nil
 }
@@ -477,3 +480,67 @@ func archS390x() *Arch {
 		IsJump:         jumpS390x,
 	}
 }
+
+func archSparc64() *Arch {
+	register := make(map[string]int16)
+	// Create maps for easy lookup of instruction names etc.
+	// Note that there is no list of names as there is for 386 and amd64.
+	for i := sparc64.REG_G0; i <= sparc64.REG_I7; i++ {
+		register[sparc64.Rconv(i)] = int16(i)
+	}
+	for i := sparc64.REG_F0; i <= sparc64.REG_F31; i++ {
+		register[sparc64.Rconv(i)] = int16(i)
+	}
+	for i := sparc64.REG_D0; i <= sparc64.REG_D62; i++ {
+		register[sparc64.Rconv(i)] = int16(i)
+	}
+	register["BSP"] = sparc64.REG_BSP
+	register["BFP"] = sparc64.REG_BFP
+	// Avoid accidental confusion between RSP and BSP, always use BSP.
+	delete(register, "RSP")
+	delete(register, "RFP")
+	delete(register, "G6")
+	register["ICC"] = sparc64.REG_ICC
+	register["XCC"] = sparc64.REG_XCC
+	register["FCC0"] = sparc64.REG_FCC0
+	register["FCC1"] = sparc64.REG_FCC1
+	register["FCC2"] = sparc64.REG_FCC2
+	register["FCC3"] = sparc64.REG_FCC3
+	register["CCR"] = sparc64.REG_CCR
+	register["TICK"] = sparc64.REG_TICK
+	register["RPC"] = sparc64.REG_RPC
+	// Pseudo-registers.
+	register["SB"] = RSB
+	register["FP"] = RFP
+	register["PC"] = RPC
+	register["SP"] = RSP
+	registerPrefix := map[string]bool{
+		"D": true,
+		"F": true,
+		"R": true,
+		"O": true,
+		"I": true,
+		"L": true,
+	}
+
+	instructions := make(map[string]obj.As)
+	for i, s := range obj.Anames {
+		instructions[s] = obj.As(i)
+	}
+	for i, s := range sparc64.Anames {
+		if obj.As(i) >= obj.A_ARCHSPECIFIC {
+			instructions[s] = obj.As(i) + obj.ABaseSPARC64
+		}
+	}
+	// Annoying aliases.
+	instructions["BL"] = sparc64.ABL
+
+	return &Arch{
+		LinkArch:       &sparc64.Linksparc64,
+		Instructions:   instructions,
+		Register:       register,
+		RegisterPrefix: registerPrefix,
+		RegisterNumber: sparc64RegisterNumber,
+		IsJump:         jumpSparc64,
+	}
+}
diff --git a/src/cmd/asm/internal/arch/sparc64.go b/src/cmd/asm/internal/arch/sparc64.go
new file mode 100644
index 0000000..57b073e
--- /dev/null
+++ b/src/cmd/asm/internal/arch/sparc64.go
@@ -0,0 +1,168 @@
+// Copyright 2015 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// This file encapsulates some of the odd characteristics of the SPARC64
+// instruction set, to minimize its interaction with the core of the
+// assembler.
+
+package arch
+
+import (
+	"cmd/internal/obj"
+	"cmd/internal/obj/sparc64"
+	"strings"
+)
+
+var sparc64Jump = map[string]bool{
+	"BN":    true,
+	"BNE":   true,
+	"BE":    true,
+	"BG":    true,
+	"BLE":   true,
+	"BGE":   true,
+	"BL":    true,
+	"BGU":   true,
+	"BLEU":  true,
+	"BCC":   true,
+	"BCS":   true,
+	"BPOS":  true,
+	"BNEG":  true,
+	"BVC":   true,
+	"BVS":   true,
+	"BNW":   true,
+	"BNEW":  true,
+	"BEW":   true,
+	"BGW":   true,
+	"BLEW":  true,
+	"BGEW":  true,
+	"BLW":   true,
+	"BGUW":  true,
+	"BLEUW": true,
+	"BCCW":  true,
+	"BCSW":  true,
+	"BPOSW": true,
+	"BNEGW": true,
+	"BVCW":  true,
+	"BVSW":  true,
+	"BND":   true,
+	"BNED":  true,
+	"BED":   true,
+	"BGD":   true,
+	"BLED":  true,
+	"BGED":  true,
+	"BLD":   true,
+	"BGUD":  true,
+	"BLEUD": true,
+	"BCCD":  true,
+	"BCSD":  true,
+	"BPOSD": true,
+	"BNEGD": true,
+	"BVCD":  true,
+	"BVSD":  true,
+	"BRZ":   true,
+	"BRLEZ": true,
+	"BRLZ":  true,
+	"BRNZ":  true,
+	"BRGZ":  true,
+	"BRGEZ": true,
+	"CALL":  true,
+	"FBA":   true,
+	"FBN":   true,
+	"FBU":   true,
+	"FBG":   true,
+	"FBUG":  true,
+	"FBL":   true,
+	"FBUL":  true,
+	"FBLG":  true,
+	"FBNE":  true,
+	"FBE":   true,
+	"FBUE":  true,
+	"FBGE":  true,
+	"FBUGE": true,
+	"FBLE":  true,
+	"FBULE": true,
+	"FBO":   true,
+	"JMP":   true,
+	"JMPL":  true,
+}
+
+// IsSPARC64CMP reports whether the op (as defined by an arm.A* constant) is
+// one of the comparison instructions that require special handling.
+func IsSPARC64CMP(op obj.As) bool {
+	switch op {
+	case sparc64.ACMP, sparc64.AFCMPD, sparc64.AFCMPS:
+		return true
+	}
+	return false
+}
+
+func jumpSparc64(word string) bool {
+	return sparc64Jump[word]
+}
+
+// SPARC64Suffix handles the special suffix for the SPARC64.
+// It returns a boolean to indicate success; failure means
+// cond was unrecognized.
+func SPARC64Suffix(prog *obj.Prog, cond string) bool {
+	if cond == "" {
+		return true
+	}
+	bits, ok := ParseSPARC64Suffix(cond)
+	if !ok {
+		return false
+	}
+	prog.Scond = bits
+	return true
+}
+
+// ParseSPARC64Suffix parses the suffix attached to an SPARC64 instruction.
+func ParseSPARC64Suffix(cond string) (uint8, bool) {
+	if cond == "" {
+		return 0, true
+	}
+	if strings.HasPrefix(cond, ".") {
+		cond = cond[1:]
+	}
+	names := strings.Split(cond, ".")
+	if len(names) != 1 {
+		return 0, false
+	}
+	if names[0] == "PN" {
+		return 1, true
+	}
+	return 0, false
+}
+
+func sparc64RegisterNumber(name string, n int16) (int16, bool) {
+	switch name {
+	case "D":
+		if 0 <= n && n <= 30 && n%2 == 0 {
+			return sparc64.REG_D0 + n, true
+		}
+		if 32 <= n && n <= 62 && n%2 == 0 {
+			return sparc64.REG_D0 + n - 31, true
+		}
+	case "F":
+		if 0 <= n && n <= 31 {
+			return sparc64.REG_F0 + n, true
+		}
+	case "G":
+		if 0 <= n && n <= 5 { // not 6, 7
+			return sparc64.REG_G0 + n, true
+		}
+	case "O":
+		if 0 <= n && n <= 5 { // not 6, 7
+			return sparc64.REG_O0 + n, true
+		}
+	case "L":
+		if 0 <= n && n <= 7 {
+			return sparc64.REG_L0 + n, true
+		}
+	case "I":
+		if 0 <= n && n <= 5 { // not 6, 7
+			return sparc64.REG_I0 + n, true
+		}
+	}
+	return 0, false
+}
diff --git a/src/cmd/asm/internal/asm/asm.go b/src/cmd/asm/internal/asm/asm.go
index c9c6420..c963991 100644
--- a/src/cmd/asm/internal/asm/asm.go
+++ b/src/cmd/asm/internal/asm/asm.go
@@ -37,6 +37,12 @@ func (p *Parser) append(prog *obj.Prog, cond string, doLabel bool) {
 				return
 			}
 
+		case sys.SPARC64:
+			if !arch.SPARC64Suffix(prog, cond) {
+				p.errorf("unrecognized suffix .%q", cond)
+				return
+			}
+
 		default:
 			p.errorf("unrecognized suffix .%q", cond)
 			return
@@ -543,6 +549,10 @@ func (p *Parser) asmInstruction(op obj.As, cond string, a []obj.Addr) {
 				prog.Reg = p.getRegister(prog, op, &a[1])
 				break
 			}
+		} else if p.arch.Family == sys.SPARC64 && arch.IsSPARC64CMP(op) {
+			prog.From = a[0]
+			prog.Reg = p.getRegister(prog, op, &a[1])
+			break
 		}
 		prog.From = a[0]
 		prog.To = a[1]
@@ -631,6 +641,22 @@ func (p *Parser) asmInstruction(op obj.As, cond string, a []obj.Addr) {
 				prog.From = a[0]
 			}
 			prog.To = a[2]
+		case sys.SPARC64:
+			// Choices are:
+			// reg reg reg
+			// reg something reg
+			// If the second argument is a register, use Reg,
+			// otherwise use From3.
+			switch a[1].Type {
+			case obj.TYPE_REG:
+				prog.From = a[0]
+				prog.Reg = p.getRegister(prog, op, &a[1])
+				prog.To = a[2]
+			default:
+				prog.From = a[0]
+				prog.From3 = newAddr(a[1])
+				prog.To = a[2]
+			}
 		default:
 			p.errorf("TODO: implement three-operand instructions for this architecture")
 			return
diff --git a/src/cmd/asm/internal/asm/parse.go b/src/cmd/asm/internal/asm/parse.go
index 40206e6..6017b3a 100644
--- a/src/cmd/asm/internal/asm/parse.go
+++ b/src/cmd/asm/internal/asm/parse.go
@@ -131,12 +131,12 @@ func (p *Parser) line() bool {
 		for {
 			tok = p.lex.Next()
 			if len(operands) == 0 && len(items) == 0 {
-				if p.arch.InFamily(sys.ARM, sys.ARM64) && tok == '.' {
-					// ARM conditionals.
+				if p.arch.InFamily(sys.ARM, sys.ARM64, sys.SPARC64) && tok == '.' {
+					// ARM and SPARC64 conditionals.
 					tok = p.lex.Next()
 					str := p.lex.Text()
 					if tok != scanner.Ident {
-						p.errorf("ARM condition expected identifier, found %s", str)
+						p.errorf("ARM/SPARC64 condition expected identifier, found %s", str)
 					}
 					cond = cond + "." + str
 					continue
@@ -484,7 +484,7 @@ func (p *Parser) register(name string, prefix rune) (r1, r2 int16, scale int8, o
 				return
 			}
 		case '+':
-			if p.arch.Family != sys.PPC64 {
+			if !p.arch.InFamily(sys.PPC64, sys.SPARC64) {
 				p.errorf("(register+register) not supported on this architecture")
 				return
 			}
@@ -662,13 +662,20 @@ func (p *Parser) registerIndirect(a *obj.Addr, prefix rune) {
 			// Nothing may follow
 			return
 		}
-		if p.arch.Family == sys.PPC64 {
-			// Special form for PPC64: (R1+R2); alias for (R1)(R2*1).
-			if prefix != 0 || scale != 0 {
+		if p.arch.InFamily(sys.PPC64, sys.SPARC64) {
+			// Special form for PPC64, SPARC64: (R1+R2); alias for (R1)(R2*1).
+			if scale != 0 {
+				p.errorf("illegal address mode for register+register")
+				return
+			}
+			if prefix != 0 && prefix != '$' {
 				p.errorf("illegal address mode for register+register")
 				return
 			}
 			a.Type = obj.TYPE_MEM
+			if prefix == '$' {
+				a.Type = obj.TYPE_ADDR
+			}
 			a.Scale = 1
 			a.Index = r2
 			// Nothing may follow.
diff --git a/src/cmd/cgo/gcc.go b/src/cmd/cgo/gcc.go
index fc1d011..7f4202f 100644
--- a/src/cmd/cgo/gcc.go
+++ b/src/cmd/cgo/gcc.go
@@ -1187,6 +1187,8 @@ func (p *Package) gccMachine() []string {
 		return []string{"-m64"}
 	case "mips64", "mips64le":
 		return []string{"-mabi=64"}
+	case "sparc64":
+		return []string{"-m64"}
 	}
 	return nil
 }
diff --git a/src/cmd/cgo/main.go b/src/cmd/cgo/main.go
index 72ac19a..935f479 100644
--- a/src/cmd/cgo/main.go
+++ b/src/cmd/cgo/main.go
@@ -153,6 +153,7 @@ var ptrSizeMap = map[string]int64{
 	"ppc64le":  8,
 	"s390":     4,
 	"s390x":    8,
+	"sparc64":  8,
 }
 
 var intSizeMap = map[string]int64{
@@ -166,6 +167,7 @@ var intSizeMap = map[string]int64{
 	"ppc64le":  8,
 	"s390":     4,
 	"s390x":    8,
+	"sparc64":  8,
 }
 
 var cPrefix string
diff --git a/src/cmd/cgo/out.go b/src/cmd/cgo/out.go
index 842b1c5..de0f008 100644
--- a/src/cmd/cgo/out.go
+++ b/src/cmd/cgo/out.go
@@ -261,6 +261,11 @@ func dynimport(obj string) {
 		}
 		for _, s := range sym {
 			targ := s.Name
+			if targ == "" {
+				// sometimes we get entries with no name in the symbol table
+				// ignore them, and hope for the best.
+				continue
+			}
 			if s.Version != "" {
 				targ += "#" + s.Version
 			}
diff --git a/src/cmd/compile/internal/gc/cgen.go b/src/cmd/compile/internal/gc/cgen.go
index 74fe463..2c31a5c 100644
--- a/src/cmd/compile/internal/gc/cgen.go
+++ b/src/cmd/compile/internal/gc/cgen.go
@@ -252,7 +252,7 @@ func cgen_wb(n, res *Node, wb bool) {
 		return
 	}
 
-	if Ctxt.Arch.InFamily(sys.ARM64, sys.MIPS64, sys.PPC64) {
+	if Ctxt.Arch.InFamily(sys.ARM64, sys.MIPS64, sys.PPC64, sys.SPARC64) {
 		// if both are addressable, move
 		if n.Addable {
 			if n.Op == OREGISTER || res.Op == OREGISTER {
@@ -402,11 +402,11 @@ func cgen_wb(n, res *Node, wb bool) {
 		Regalloc(&n1, nl.Type, res)
 
 		Cgen(nl, &n1)
-		if Ctxt.Arch.Family == sys.ARM {
+		if Ctxt.Arch.InFamily(sys.ARM) {
 			var n2 Node
 			Nodconst(&n2, nl.Type, 0)
 			Thearch.Gins(a, &n2, &n1)
-		} else if Ctxt.Arch.Family == sys.ARM64 {
+		} else if Ctxt.Arch.InFamily(sys.ARM64, sys.SPARC64) {
 			Thearch.Gins(a, &n1, &n1)
 		} else {
 			Thearch.Gins(a, nil, &n1)
@@ -751,14 +751,14 @@ abop: // asymmetric binary
 		Regalloc(&n1, nl.Type, res)
 		Cgen(nl, &n1)
 
-		if Smallintconst(nr) && Ctxt.Arch.Family != sys.MIPS64 && Ctxt.Arch.Family != sys.ARM && Ctxt.Arch.Family != sys.ARM64 && Ctxt.Arch.Family != sys.PPC64 { // TODO(rsc): Check opcode for arm
+		if Smallintconst(nr) && !Ctxt.Arch.InFamily(sys.MIPS64, sys.ARM, sys.ARM64, sys.PPC64, sys.SPARC64) { // TODO(rsc): Check opcode for arm
 			n2 = *nr
 		} else {
 			Regalloc(&n2, nr.Type, nil)
 			Cgen(nr, &n2)
 		}
 	} else {
-		if Smallintconst(nr) && Ctxt.Arch.Family != sys.MIPS64 && Ctxt.Arch.Family != sys.ARM && Ctxt.Arch.Family != sys.ARM64 && Ctxt.Arch.Family != sys.PPC64 { // TODO(rsc): Check opcode for arm
+		if Smallintconst(nr) && !Ctxt.Arch.InFamily(sys.MIPS64, sys.ARM, sys.ARM64, sys.PPC64, sys.SPARC64) { // TODO(rsc): Check opcode for arm
 			n2 = *nr
 		} else {
 			Regalloc(&n2, nr.Type, res)
@@ -1828,7 +1828,7 @@ func bgenx(n, res *Node, wantTrue bool, likely int, to *obj.Prog) {
 		// Some architectures might need a temporary or other help here,
 		// but they don't support direct generation of a bool value yet.
 		// We can fix that as we go.
-		mayNeedTemp := Ctxt.Arch.InFamily(sys.ARM, sys.ARM64, sys.MIPS64, sys.PPC64, sys.S390X)
+		mayNeedTemp := Ctxt.Arch.InFamily(sys.ARM, sys.ARM64, sys.MIPS64, sys.PPC64, sys.S390X, sys.SPARC64)
 
 		if genval {
 			if mayNeedTemp {
@@ -2112,9 +2112,9 @@ func bgenx(n, res *Node, wantTrue bool, likely int, to *obj.Prog) {
 				}
 				return
 			}
-		case sys.ARM64, sys.PPC64:
+		case sys.ARM64, sys.PPC64, sys.SPARC64:
 			if genval {
-				Fatalf("genval 7g, 9g Isfloat special cases not implemented")
+				Fatalf("genval 7g, 9g, ug Isfloat special cases not implemented")
 			}
 			switch n.Op {
 			// On arm64 and ppc64, <= and >= mishandle NaN. Must decompose into < or > and =.
@@ -2627,7 +2627,7 @@ func hasHMUL64() bool {
 	switch Ctxt.Arch.Family {
 	case sys.AMD64, sys.S390X, sys.ARM64:
 		return true
-	case sys.ARM, sys.I386, sys.MIPS64, sys.PPC64:
+	case sys.ARM, sys.I386, sys.MIPS64, sys.PPC64, sys.SPARC64:
 		return false
 	}
 	Fatalf("unknown architecture")
@@ -2640,7 +2640,7 @@ func hasRROTC64() bool {
 	switch Ctxt.Arch.Family {
 	case sys.AMD64:
 		return true
-	case sys.ARM, sys.ARM64, sys.I386, sys.MIPS64, sys.PPC64, sys.S390X:
+	case sys.ARM, sys.ARM64, sys.I386, sys.MIPS64, sys.PPC64, sys.S390X, sys.SPARC64:
 		return false
 	}
 	Fatalf("unknown architecture")
@@ -2651,7 +2651,7 @@ func hasRightShiftWithCarry() bool {
 	switch Ctxt.Arch.Family {
 	case sys.ARM64:
 		return true
-	case sys.AMD64, sys.ARM, sys.I386, sys.MIPS64, sys.PPC64, sys.S390X:
+	case sys.AMD64, sys.ARM, sys.I386, sys.MIPS64, sys.PPC64, sys.S390X, sys.SPARC64:
 		return false
 	}
 	Fatalf("unknown architecture")
@@ -2662,7 +2662,7 @@ func hasAddSetCarry() bool {
 	switch Ctxt.Arch.Family {
 	case sys.ARM64:
 		return true
-	case sys.AMD64, sys.ARM, sys.I386, sys.MIPS64, sys.PPC64, sys.S390X:
+	case sys.AMD64, sys.ARM, sys.I386, sys.MIPS64, sys.PPC64, sys.S390X, sys.SPARC64:
 		return false
 	}
 	Fatalf("unknown architecture")
diff --git a/src/cmd/compile/internal/gc/gsubr.go b/src/cmd/compile/internal/gc/gsubr.go
index 4943d9d..67222b3 100644
--- a/src/cmd/compile/internal/gc/gsubr.go
+++ b/src/cmd/compile/internal/gc/gsubr.go
@@ -85,7 +85,7 @@ func Gbranch(as obj.As, t *Type, likely int) *obj.Prog {
 	p := Prog(as)
 	p.To.Type = obj.TYPE_BRANCH
 	p.To.Val = nil
-	if as != obj.AJMP && likely != 0 && !Thearch.LinkArch.InFamily(sys.PPC64, sys.ARM64, sys.MIPS64, sys.S390X) {
+	if as != obj.AJMP && likely != 0 && !Thearch.LinkArch.InFamily(sys.PPC64, sys.ARM64, sys.MIPS64, sys.S390X, sys.SPARC64) {
 		p.From.Type = obj.TYPE_CONST
 		if likely > 0 {
 			p.From.Offset = 1
@@ -447,7 +447,7 @@ func Naddr(a *obj.Addr, n *Node) {
 	case OADDR:
 		Naddr(a, n.Left)
 		a.Etype = uint8(Tptr)
-		if !Thearch.LinkArch.InFamily(sys.MIPS64, sys.ARM, sys.ARM64, sys.PPC64, sys.S390X) { // TODO(rsc): Do this even for these architectures.
+		if !Thearch.LinkArch.InFamily(sys.MIPS64, sys.ARM, sys.ARM64, sys.PPC64, sys.S390X, sys.SPARC64) { // TODO(rsc): Do this even for these architectures.
 			a.Width = int64(Widthptr)
 		}
 		if a.Type != obj.TYPE_MEM {
@@ -651,8 +651,8 @@ func unpatch(p *obj.Prog) *obj.Prog {
 	return q
 }
 
-var reg [100]int       // count of references to reg
-var regstk [100][]byte // allocation sites, when -v is given
+var reg [128]int       // count of references to reg
+var regstk [128][]byte // allocation sites, when -v is given
 
 func GetReg(r int) int {
 	return reg[r-Thearch.REGMIN]
diff --git a/src/cmd/compile/internal/gc/obj.go b/src/cmd/compile/internal/gc/obj.go
index b5c06d1..a0cd139 100644
--- a/src/cmd/compile/internal/gc/obj.go
+++ b/src/cmd/compile/internal/gc/obj.go
@@ -242,6 +242,19 @@ func duintptr(s *Sym, off int, v uint64) int {
 	return duintxx(s, off, v, Widthptr)
 }
 
+func dbvec(s *Sym, off int, bv bvec) int {
+	for j := 0; int32(j) < bv.n; j += 32 {
+		word := bv.b[j/32]
+
+		// Runtime reads the bitmaps as byte arrays. Oblige.
+		off = duint8(s, off, uint8(word))
+		off = duint8(s, off, uint8(word>>8))
+		off = duint8(s, off, uint8(word>>16))
+		off = duint8(s, off, uint8(word>>24))
+	}
+	return off
+}
+
 // stringConstantSyms holds the pair of symbols we create for a
 // constant string.
 type stringConstantSyms struct {
diff --git a/src/cmd/compile/internal/gc/pgen.go b/src/cmd/compile/internal/gc/pgen.go
index da2e675..18094bc 100644
--- a/src/cmd/compile/internal/gc/pgen.go
+++ b/src/cmd/compile/internal/gc/pgen.go
@@ -154,15 +154,11 @@ func emitptrargsmap() {
 		onebitwalktype1(Curfn.Type.Params(), &xoffset, bv)
 	}
 
-	for j := 0; int32(j) < bv.n; j += 32 {
-		off = duint32(sym, off, bv.b[j/32])
-	}
+	off = dbvec(sym, off, bv)
 	if Curfn.Type.Results().NumFields() > 0 {
 		xoffset = 0
 		onebitwalktype1(Curfn.Type.Results(), &xoffset, bv)
-		for j := 0; int32(j) < bv.n; j += 32 {
-			off = duint32(sym, off, bv.b[j/32])
-		}
+		off = dbvec(sym, off, bv)
 	}
 
 	ggloblsym(sym, int32(off), obj.RODATA|obj.LOCAL)
@@ -277,7 +273,7 @@ func allocauto(ptxt *obj.Prog) {
 		if haspointers(n.Type) {
 			stkptrsize = Stksize
 		}
-		if Thearch.LinkArch.InFamily(sys.MIPS64, sys.ARM, sys.ARM64, sys.PPC64, sys.S390X) {
+		if Thearch.LinkArch.InFamily(sys.MIPS64, sys.ARM, sys.ARM64, sys.PPC64, sys.S390X, sys.SPARC64) {
 			Stksize = Rnd(Stksize, int64(Widthptr))
 		}
 		if Stksize >= 1<<31 {
diff --git a/src/cmd/compile/internal/gc/plive.go b/src/cmd/compile/internal/gc/plive.go
index ca0421d..c4ae2ce 100644
--- a/src/cmd/compile/internal/gc/plive.go
+++ b/src/cmd/compile/internal/gc/plive.go
@@ -1662,7 +1662,7 @@ func livenessprintdebug(lv *Liveness) {
 // Dumps a slice of bitmaps to a symbol as a sequence of uint32 values. The
 // first word dumped is the total number of bitmaps. The second word is the
 // length of the bitmaps. All bitmaps are assumed to be of equal length. The
-// words that are followed are the raw bitmap words.
+// remaining bytes are the raw bitmaps.
 func onebitwritesymbol(arr []bvec, sym *Sym) {
 	off := 4                                  // number of bitmaps, to fill in later
 	off = duint32(sym, off, uint32(arr[0].n)) // number of bits in each bitmap
@@ -1674,16 +1674,7 @@ func onebitwritesymbol(arr []bvec, sym *Sym) {
 		if bv.b == nil {
 			break
 		}
-		for j := 0; int32(j) < bv.n; j += 32 {
-			word := bv.b[j/32]
-
-			// Runtime reads the bitmaps as byte arrays. Oblige.
-			off = duint8(sym, off, uint8(word))
-
-			off = duint8(sym, off, uint8(word>>8))
-			off = duint8(sym, off, uint8(word>>16))
-			off = duint8(sym, off, uint8(word>>24))
-		}
+		off = dbvec(sym, off, bv)
 	}
 
 	duint32(sym, 0, uint32(i)) // number of bitmaps
diff --git a/src/cmd/compile/internal/gc/reg.go b/src/cmd/compile/internal/gc/reg.go
index a80d72b..765da4d 100644
--- a/src/cmd/compile/internal/gc/reg.go
+++ b/src/cmd/compile/internal/gc/reg.go
@@ -250,7 +250,7 @@ func addmove(r *Flow, bn int, rn int, f int) {
 	p1.As = Thearch.Optoas(OAS, Types[uint8(v.etype)])
 
 	// TODO(rsc): Remove special case here.
-	if Thearch.LinkArch.InFamily(sys.MIPS64, sys.ARM, sys.ARM64, sys.PPC64) && v.etype == TBOOL {
+	if Thearch.LinkArch.InFamily(sys.MIPS64, sys.ARM, sys.ARM64, sys.PPC64, sys.SPARC64) && v.etype == TBOOL {
 		p1.As = Thearch.Optoas(OAS, Types[TUINT8])
 	}
 	p1.From.Type = obj.TYPE_REG
@@ -303,7 +303,7 @@ func mkvar(f *Flow, a *obj.Addr) Bits {
 		// TODO(rsc): Remove special case here.
 	case obj.TYPE_ADDR:
 		var bit Bits
-		if Thearch.LinkArch.InFamily(sys.MIPS64, sys.ARM, sys.ARM64, sys.PPC64) {
+		if Thearch.LinkArch.InFamily(sys.MIPS64, sys.ARM, sys.ARM64, sys.PPC64, sys.SPARC64) {
 			goto memcase
 		}
 		a.Type = obj.TYPE_MEM
diff --git a/src/cmd/compile/internal/gc/walk.go b/src/cmd/compile/internal/gc/walk.go
index 66eb7e9..16dd1fc 100644
--- a/src/cmd/compile/internal/gc/walk.go
+++ b/src/cmd/compile/internal/gc/walk.go
@@ -691,7 +691,7 @@ opswitch:
 		walkexprlist(n.List.Slice(), init)
 
 		if n.Left.Op == ONAME && n.Left.Sym.Name == "Sqrt" && n.Left.Sym.Pkg.Path == "math" {
-			if Thearch.LinkArch.InFamily(sys.AMD64, sys.ARM, sys.ARM64, sys.PPC64, sys.S390X) {
+			if Thearch.LinkArch.InFamily(sys.AMD64, sys.ARM, sys.ARM64, sys.PPC64, sys.S390X, sys.SPARC64) {
 				n.Op = OSQRT
 				n.Left = n.List.First()
 				n.List.Set(nil)
@@ -3303,7 +3303,7 @@ func samecheap(a *Node, b *Node) bool {
 // The result of walkrotate MUST be assigned back to n, e.g.
 // 	n.Left = walkrotate(n.Left)
 func walkrotate(n *Node) *Node {
-	if Thearch.LinkArch.InFamily(sys.MIPS64, sys.ARM64, sys.PPC64) {
+	if Thearch.LinkArch.InFamily(sys.MIPS64, sys.ARM64, sys.PPC64, sys.SPARC64) {
 		return n
 	}
 
@@ -3435,7 +3435,7 @@ func walkdiv(n *Node, init *Nodes) *Node {
 	// if >= 0, nr is 1<<pow // 1 if nr is negative.
 
 	// TODO(minux)
-	if Thearch.LinkArch.InFamily(sys.MIPS64, sys.PPC64) {
+	if Thearch.LinkArch.InFamily(sys.MIPS64, sys.PPC64, sys.SPARC64) {
 		return n
 	}
 
diff --git a/src/cmd/compile/internal/sparc64/cgen.go b/src/cmd/compile/internal/sparc64/cgen.go
new file mode 100644
index 0000000..e5013e3
--- /dev/null
+++ b/src/cmd/compile/internal/sparc64/cgen.go
@@ -0,0 +1,163 @@
+// Copyright 2009 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package sparc64
+
+import (
+	"cmd/compile/internal/gc"
+	"cmd/internal/obj"
+	"cmd/internal/obj/sparc64"
+)
+
+func blockcopy(n, res *gc.Node, osrc, odst, w int64) {
+	// determine alignment.
+	// want to avoid unaligned access, so have to use
+	// smaller operations for less aligned types.
+	// for example moving [4]byte must use 4 MOVB not 1 MOVW.
+	align := int(n.Type.Align)
+
+	var op obj.As
+	switch align {
+	default:
+		gc.Fatalf("sgen: invalid alignment %d for %v", align, n.Type)
+
+	case 1:
+		op = sparc64.AMOVB
+
+	case 2:
+		op = sparc64.AMOVH
+
+	case 4:
+		op = sparc64.AMOVW
+
+	case 8:
+		op = sparc64.AMOVD
+	}
+
+	if w%int64(align) != 0 {
+		gc.Fatalf("sgen: unaligned size %d (align=%d) for %v", w, align, n.Type)
+	}
+	c := int32(w / int64(align))
+
+	if osrc%int64(align) != 0 || odst%int64(align) != 0 {
+		gc.Fatalf("sgen: unaligned offset src %d or dst %d (align %d)", osrc, odst, align)
+	}
+
+	// if we are copying forward on the stack and
+	// the src and dst overlap, then reverse direction
+	dir := align
+
+	if osrc < odst && odst < osrc+w {
+		dir = -dir
+	}
+
+	var dst gc.Node
+	var src gc.Node
+	if n.Ullman >= res.Ullman {
+		gc.Agenr(n, &dst, res) // temporarily use dst
+		gc.Regalloc(&src, gc.Types[gc.Tptr], nil)
+		gins(sparc64.AMOVD, &dst, &src)
+		if res.Op == gc.ONAME {
+			gc.Gvardef(res)
+		}
+		gc.Agen(res, &dst)
+	} else {
+		if res.Op == gc.ONAME {
+			gc.Gvardef(res)
+		}
+		gc.Agenr(res, &dst, res)
+		gc.Agenr(n, &src, nil)
+	}
+
+	var tmp, tmp1 gc.Node
+	gc.Regalloc(&tmp, gc.Types[gc.Tptr], nil)
+	gc.Regalloc(&tmp1, gc.Types[gc.Tptr], nil)
+
+	// set up end marker
+	var nend gc.Node
+
+	// move src and dest to the end of block if necessary
+	if dir < 0 {
+		if c >= 4 {
+			gc.Regalloc(&nend, gc.Types[gc.Tptr], nil)
+			gins(sparc64.AMOVD, &src, &nend)
+		}
+
+		p := gins(sparc64.AADD, nil, &src)
+		p.From.Type = obj.TYPE_CONST
+		p.From.Offset = w
+
+		p = gins(sparc64.AADD, nil, &dst)
+		p.From.Type = obj.TYPE_CONST
+		p.From.Offset = w
+	} else {
+		p := gins(sparc64.AADD, nil, &src)
+		p.From.Type = obj.TYPE_CONST
+		p.From.Offset = int64(-dir)
+
+		p = gins(sparc64.AADD, nil, &dst)
+		p.From.Type = obj.TYPE_CONST
+		p.From.Offset = int64(-dir)
+
+		if c >= 4 {
+			gc.Regalloc(&nend, gc.Types[gc.Tptr], nil)
+			p := gins(sparc64.AMOVD, &src, &nend)
+			p.From.Type = obj.TYPE_ADDR
+			p.From.Offset = w
+		}
+	}
+
+	// move
+	// TODO: enable duffcopy for larger copies.
+	if c >= 4 {
+		// TODO(aram): instead of manually updating both src and dst, update
+		// only the index register and change the comparison.
+		ginscon(sparc64.AMOVD, int64(dir), &tmp1)
+
+		p := gins(op, &src, &tmp)
+		p.From.Type = obj.TYPE_MEM
+		p.From.Index = tmp1.Reg
+		p.From.Scale = 1
+		ploop := p
+
+		p = gins(sparc64.AADD, &tmp1, &src)
+
+		p = gins(op, &tmp, &dst)
+		p.To.Type = obj.TYPE_MEM
+		p.To.Index = tmp1.Reg
+		p.To.Scale = 1
+
+		p = gins(sparc64.AADD, &tmp1, &dst)
+
+		p = gcmp(sparc64.ACMP, &src, &nend)
+
+		gc.Patch(gc.Gbranch(sparc64.ABNED, nil, 0), ploop)
+		gc.Regfree(&nend)
+	} else {
+		// TODO(aram): instead of manually updating both src and dst, update
+		// only the index register.
+		ginscon(sparc64.AMOVD, int64(dir), &tmp1)
+		var p *obj.Prog
+		for ; c > 0; c-- {
+			p = gins(op, &src, &tmp)
+			p.From.Type = obj.TYPE_MEM
+			p.From.Index = tmp1.Reg
+			p.From.Scale = 1
+
+			p = gins(sparc64.AADD, &tmp1, &src)
+
+			p = gins(op, &tmp, &dst)
+			p.To.Type = obj.TYPE_MEM
+			p.To.Index = tmp1.Reg
+			p.To.Scale = 1
+
+			p = gins(sparc64.AADD, &tmp1, &dst)
+		}
+	}
+
+	gc.Regfree(&dst)
+	gc.Regfree(&src)
+	gc.Regfree(&tmp)
+	gc.Regfree(&tmp1)
+}
diff --git a/src/cmd/compile/internal/sparc64/galign.go b/src/cmd/compile/internal/sparc64/galign.go
new file mode 100644
index 0000000..5b66b4d
--- /dev/null
+++ b/src/cmd/compile/internal/sparc64/galign.go
@@ -0,0 +1,67 @@
+// Copyright 2009 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package sparc64
+
+import (
+	"cmd/compile/internal/gc"
+	"cmd/internal/obj/sparc64"
+)
+
+var MAXWIDTH int64 = 1 << 50
+
+func betypeinit() {
+}
+
+func Main() {
+	gc.Thearch.LinkArch = &sparc64.Linksparc64
+
+	gc.Thearch.REGSP = sparc64.REG_BSP
+	gc.Thearch.REGCTXT = sparc64.REG_CTXT
+	gc.Thearch.REGCALLX = sparc64.REG_RT1
+	gc.Thearch.REGCALLX2 = sparc64.REG_RT2
+	gc.Thearch.REGRETURN = sparc64.REG_RT1
+	gc.Thearch.REGMIN = sparc64.REG_MIN
+	gc.Thearch.REGMAX = sparc64.REG_MAX
+	gc.Thearch.REGZERO = sparc64.REG_ZR
+	gc.Thearch.FREGMIN = sparc64.REG_Y0
+	gc.Thearch.FREGMAX = sparc64.REG_Y15
+	gc.Thearch.MAXWIDTH = MAXWIDTH
+	gc.Thearch.ReservedRegs = resvd
+
+	gc.Thearch.Betypeinit = betypeinit
+	gc.Thearch.Cgen_hmul = cgen_hmul
+	gc.Thearch.Cgen_shift = cgen_shift
+	gc.Thearch.Clearfat = clearfat
+	gc.Thearch.Defframe = defframe
+	gc.Thearch.Dodiv = dodiv
+	gc.Thearch.Excise = excise
+	gc.Thearch.Expandchecks = expandchecks
+	gc.Thearch.Getg = getg
+	gc.Thearch.Gins = gins
+	gc.Thearch.Ginscmp = ginscmp
+	gc.Thearch.Ginscon = ginscon
+	gc.Thearch.Ginsnop = ginsnop
+	gc.Thearch.Gmove = gmove
+	gc.Thearch.Peep = peep
+	gc.Thearch.Proginfo = proginfo
+	gc.Thearch.Regtyp = regtyp
+	gc.Thearch.Sameaddr = sameaddr
+	gc.Thearch.Smallindir = smallindir
+	gc.Thearch.Stackaddr = stackaddr
+	gc.Thearch.Blockcopy = blockcopy
+	gc.Thearch.Sudoaddable = sudoaddable
+	gc.Thearch.Sudoclean = sudoclean
+	gc.Thearch.Excludedregs = excludedregs
+	gc.Thearch.RtoB = RtoB
+	gc.Thearch.FtoB = RtoB
+	gc.Thearch.BtoR = BtoR
+	gc.Thearch.BtoF = BtoF
+	gc.Thearch.Optoas = optoas
+	gc.Thearch.Doregbits = doregbits
+	gc.Thearch.Regnames = regnames
+
+	gc.Main()
+	gc.Exit(0)
+}
diff --git a/src/cmd/compile/internal/sparc64/ggen.go b/src/cmd/compile/internal/sparc64/ggen.go
new file mode 100644
index 0000000..24c55e3
--- /dev/null
+++ b/src/cmd/compile/internal/sparc64/ggen.go
@@ -0,0 +1,546 @@
+// Copyright 2009 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package sparc64
+
+import (
+	"cmd/compile/internal/gc"
+	"cmd/internal/obj"
+	"cmd/internal/obj/sparc64"
+	"fmt"
+)
+
+func defframe(ptxt *obj.Prog) {
+	// fill in argument size, stack size
+	ptxt.To.Type = obj.TYPE_TEXTSIZE
+
+	ptxt.To.Val = int32(gc.Rnd(gc.Curfn.Type.ArgWidth(), int64(gc.Widthptr)))
+	frame := uint32(gc.Rnd(gc.Stksize+gc.Maxarg, int64(gc.Widthreg)))
+
+	// sparc64 requires that the frame size (not counting saved LR)
+	// be empty or be 0 mod 16. If not, pad it.
+	if frame != 0 && frame%16 == 8 {
+		frame += 8
+	}
+
+	ptxt.To.Offset = int64(frame)
+
+	// insert code to zero ambiguously live variables
+	// so that the garbage collector only sees initialized values
+	// when it looks for pointers.
+	p := ptxt
+
+	hi := int64(0)
+	lo := hi
+
+	// iterate through declarations - they are sorted in decreasing xoffset order.
+	for _, n := range gc.Curfn.Func.Dcl {
+		if !n.Name.Needzero {
+			continue
+		}
+		if n.Class != gc.PAUTO {
+			gc.Fatalf("needzero class %d", n.Class)
+		}
+		if n.Type.Width%int64(gc.Widthptr) != 0 || n.Xoffset%int64(gc.Widthptr) != 0 || n.Type.Width == 0 {
+			gc.Fatalf("var %v has size %d offset %d", gc.Nconv(n, gc.FmtLong), int(n.Type.Width), int(n.Xoffset))
+		}
+
+		if lo != hi && n.Xoffset+n.Type.Width >= lo-int64(2*gc.Widthreg) {
+			// merge with range we already have
+			lo = n.Xoffset
+
+			continue
+		}
+
+		// zero old range
+		p = zerorange(p, int64(frame), lo, hi)
+
+		// set new range
+		hi = n.Xoffset + n.Type.Width
+
+		lo = n.Xoffset
+	}
+
+	// zero final range
+	zerorange(p, int64(frame), lo, hi)
+}
+
+var darwin = obj.Getgoos() == "darwin"
+
+func zerorange(p *obj.Prog, frame int64, lo int64, hi int64) *obj.Prog {
+	cnt := hi - lo
+	if cnt == 0 {
+		return p
+	}
+	if cnt < int64(4*gc.Widthptr) {
+		for i := int64(0); i < cnt; i += int64(gc.Widthptr) {
+			p = appendpp(p, sparc64.AMOVD, obj.TYPE_REG, sparc64.REG_ZR, 0, obj.TYPE_MEM, sparc64.REG_BFP, lo+i)
+		}
+	} else if false && cnt <= int64(128*gc.Widthptr) {
+		// TODO(shawn):
+		// Disabled for now since it's likely that the call to duffzero
+		// will stomp on the link register in a tail call case; see
+		// mips issue https://golang.org/issue/12108
+		p = appendpp(p, sparc64.AMOVD, obj.TYPE_REG, sparc64.REG_BFP, 0, obj.TYPE_REG, sparc64.REG_RT1, 0)
+		p = appendpp(p, sparc64.AADD, obj.TYPE_CONST, 0, lo, obj.TYPE_REG, sparc64.REG_RT1, 0)
+		p.Reg = sparc64.REG_RT1
+		p = appendpp(p, obj.ADUFFZERO, obj.TYPE_NONE, 0, 0, obj.TYPE_MEM, 0, 0)
+		f := gc.Sysfunc("duffzero")
+		gc.Naddr(&p.To, f)
+		gc.Afunclit(&p.To, f)
+		// the extra +8 is to account for the prologue padding
+		// added by preprocess()
+		p.To.Offset = 8 * (128 - cnt/int64(gc.Widthptr)) + 8
+	} else {
+		//	ADD	$lo, BFP, RT1
+		//	ADD	$(cnt), RT1, RT2
+		// loop:
+		//	MOVD	ZR, (RT1)
+		//	ADD	$8, RT1
+		//	CMP	RT1, RT2
+		//	BNED	loop
+		p = appendpp(p, sparc64.AADD, obj.TYPE_CONST, 0, int64(lo+sparc64.StackBias), obj.TYPE_REG, sparc64.REG_RT1, 0)
+		p.Reg = sparc64.REG_RFP
+		p = appendpp(p, sparc64.AADD, obj.TYPE_CONST, 0, cnt, obj.TYPE_REG, sparc64.REG_RT2, 0)
+		p.Reg = sparc64.REG_RT1
+		p = appendpp(p, sparc64.AMOVD, obj.TYPE_REG, sparc64.REG_ZR, 0, obj.TYPE_MEM, sparc64.REG_RT1, 0)
+		p1 := p
+		p = appendpp(p, sparc64.AADD, obj.TYPE_CONST, 0, 8, obj.TYPE_REG, sparc64.REG_RT1, 0)
+		p = appendpp(p, sparc64.ACMP, obj.TYPE_REG, sparc64.REG_RT1, 0, obj.TYPE_NONE, 0, 0)
+		p.Reg = sparc64.REG_RT2
+		p = appendpp(p, sparc64.ABNED, obj.TYPE_NONE, 0, 0, obj.TYPE_BRANCH, 0, 0)
+		gc.Patch(p, p1)
+	}
+
+	return p
+}
+
+func appendpp(p *obj.Prog, as obj.As, ftype obj.AddrType, freg int, foffset int64, ttype obj.AddrType, treg int, toffset int64) *obj.Prog {
+	q := gc.Ctxt.NewProg()
+	gc.Clearp(q)
+	q.As = as
+	q.Lineno = p.Lineno
+	q.From.Type = ftype
+	q.From.Reg = int16(freg)
+	q.From.Offset = foffset
+	q.To.Type = ttype
+	q.To.Reg = int16(treg)
+	q.To.Offset = toffset
+	q.Link = p.Link
+	p.Link = q
+	return q
+}
+
+func ginsnop() {
+	var con gc.Node
+	gc.Nodconst(&con, gc.Types[gc.TINT], 0)
+	gins(sparc64.ARNOP, nil, nil)
+}
+
+var panicdiv *gc.Node
+
+/*
+ * generate division.
+ * generates one of:
+ *	res = nl / nr
+ *	res = nl % nr
+ * according to op.
+ */
+func dodiv(op gc.Op, nl *gc.Node, nr *gc.Node, res *gc.Node) {
+	// Have to be careful about handling
+	// most negative int divided by -1 correctly.
+	// The hardware will generate undefined result.
+	// Also need to explicitly trap on division on zero,
+	// the hardware will silently generate undefined result.
+	// DIVW will leave unpredictable result in higher 32-bit,
+	// so always use DIVD/DIVDU.
+	t := nl.Type
+
+	t0 := t
+	check := false
+	if t.IsSigned() {
+		check = true
+		if gc.Isconst(nl, gc.CTINT) && nl.Int64() != -(1<<uint64(t.Width*8-1)) {
+			check = false
+		} else if gc.Isconst(nr, gc.CTINT) && nr.Int64() != -1 {
+			check = false
+		}
+	}
+
+	if t.Width < 8 {
+		if t.IsSigned() {
+			t = gc.Types[gc.TINT64]
+		} else {
+			t = gc.Types[gc.TUINT64]
+		}
+		check = false
+	}
+
+	a := optoas(gc.ODIV, t)
+
+	var tl gc.Node
+	gc.Regalloc(&tl, t0, nil)
+	var tr gc.Node
+	gc.Regalloc(&tr, t0, nil)
+	if nl.Ullman >= nr.Ullman {
+		gc.Cgen(nl, &tl)
+		gc.Cgen(nr, &tr)
+	} else {
+		gc.Cgen(nr, &tr)
+		gc.Cgen(nl, &tl)
+	}
+
+	if t != t0 {
+		// Convert
+		tl2 := tl
+
+		tr2 := tr
+		tl.Type = t
+		tr.Type = t
+		gmove(&tl2, &tl)
+		gmove(&tr2, &tr)
+	}
+
+	// Handle divide-by-zero panic.
+	p1 := gins(optoas(gc.OCMP, t), &tr, nil)
+	p1.Reg = sparc64.REG_ZR
+	p1 = gc.Gbranch(optoas(gc.ONE, t), nil, +1)
+	if panicdiv == nil {
+		panicdiv = gc.Sysfunc("panicdivide")
+	}
+	gc.Ginscall(panicdiv, -1)
+	gc.Patch(p1, gc.Pc)
+
+	var p2 *obj.Prog
+	if check {
+		var nm1 gc.Node
+		gc.Nodconst(&nm1, t, -1)
+		gcmp(optoas(gc.OCMP, t), &tr, &nm1)
+		p1 := gc.Gbranch(optoas(gc.ONE, t), nil, +1)
+		if op == gc.ODIV {
+			// a / (-1) is -a.
+			gins(optoas(gc.OMINUS, t), &tl, &tl)
+
+			gmove(&tl, res)
+		} else {
+			// a % (-1) is 0.
+			var nz gc.Node
+			gc.Nodconst(&nz, t, 0)
+
+			gmove(&nz, res)
+		}
+
+		p2 = gc.Gbranch(obj.AJMP, nil, 0)
+		gc.Patch(p1, gc.Pc)
+	}
+
+	p1 = gins(a, &tr, &tl)
+	if op == gc.ODIV {
+		gc.Regfree(&tr)
+		gmove(&tl, res)
+	} else {
+		// A%B = A-(A/B*B)
+		var tm gc.Node
+		gc.Regalloc(&tm, t, nil)
+
+		// patch div to use the 3 register form
+		// TODO(minux): add gins3?
+		p1.Reg = p1.To.Reg
+
+		p1.To.Reg = tm.Reg
+		gins(optoas(gc.OMUL, t), &tr, &tm)
+		gc.Regfree(&tr)
+		gins(optoas(gc.OSUB, t), &tm, &tl)
+		gc.Regfree(&tm)
+		gmove(&tl, res)
+	}
+
+	gc.Regfree(&tl)
+	if check {
+		gc.Patch(p2, gc.Pc)
+	}
+}
+
+/*
+ * generate high multiply:
+ *   res = (nl*nr) >> width
+ */
+func cgen_hmul(nl *gc.Node, nr *gc.Node, res *gc.Node) {
+	// largest ullman on left.
+	if nl.Ullman < nr.Ullman {
+		nl, nr = nr, nl
+	}
+
+	t := nl.Type
+	w := t.Width * 8
+	var n1 gc.Node
+	gc.Cgenr(nl, &n1, res)
+	var n2 gc.Node
+	gc.Cgenr(nr, &n2, nil)
+	switch gc.Simtype[t.Etype] {
+	case gc.TINT8,
+		gc.TINT16,
+		gc.TINT32:
+		gins(optoas(gc.OMUL, t), &n2, &n1)
+		p := gins(sparc64.ASRAW, nil, &n1)
+		p.From.Type = obj.TYPE_CONST
+		p.From.Offset = w
+
+	case gc.TUINT8,
+		gc.TUINT16,
+		gc.TUINT32:
+		gins(optoas(gc.OMUL, t), &n2, &n1)
+		p := gins(sparc64.ASRLW, nil, &n1)
+		p.From.Type = obj.TYPE_CONST
+		p.From.Offset = w
+
+	// TODO(aram):
+	//case gc.TINT64,
+	//	gc.TUINT64:
+	//	if t.IsSigned() {
+	//		gins(sparc64.ASMULH, &n2, &n1)
+	//	} else {
+	//		gins(sparc64.AUMULH, &n2, &n1)
+	//	}
+
+	default:
+		gc.Fatalf("cgen_hmul %v", t)
+	}
+
+	gc.Cgen(&n1, res)
+	gc.Regfree(&n1)
+	gc.Regfree(&n2)
+}
+
+/*
+ * generate shift according to op, one of:
+ *	res = nl << nr
+ *	res = nl >> nr
+ */
+func cgen_shift(op gc.Op, bounded bool, nl *gc.Node, nr *gc.Node, res *gc.Node) {
+	a := optoas(op, nl.Type)
+
+	if nr.Op == gc.OLITERAL {
+		var n1 gc.Node
+		gc.Regalloc(&n1, nl.Type, res)
+		gc.Cgen(nl, &n1)
+		sc := uint64(nr.Int64())
+		if sc >= uint64(nl.Type.Width)*8 {
+			// large shift gets 2 shifts by width-1
+			var n3 gc.Node
+			gc.Nodconst(&n3, gc.Types[gc.TUINT32], nl.Type.Width*8-1)
+
+			gins(a, &n3, &n1)
+			gins(a, &n3, &n1)
+		} else {
+			gins(a, nr, &n1)
+		}
+		gmove(&n1, res)
+		gc.Regfree(&n1)
+		return
+	}
+
+	if nl.Ullman >= gc.UINF {
+		var n4 gc.Node
+		gc.Tempname(&n4, nl.Type)
+		gc.Cgen(nl, &n4)
+		nl = &n4
+	}
+
+	if nr.Ullman >= gc.UINF {
+		var n5 gc.Node
+		gc.Tempname(&n5, nr.Type)
+		gc.Cgen(nr, &n5)
+		nr = &n5
+	}
+
+	// Allow either uint32 or uint64 as shift type,
+	// to avoid unnecessary conversion from uint32 to uint64
+	// just to do the comparison.
+	tcount := gc.Types[gc.Simtype[nr.Type.Etype]]
+
+	if tcount.Etype < gc.TUINT32 {
+		tcount = gc.Types[gc.TUINT32]
+	}
+
+	var n1 gc.Node
+	gc.Regalloc(&n1, nr.Type, nil) // to hold the shift type in CX
+	var n3 gc.Node
+	gc.Regalloc(&n3, tcount, &n1) // to clear high bits of CX
+
+	var n2 gc.Node
+	gc.Regalloc(&n2, nl.Type, res)
+
+	if nl.Ullman >= nr.Ullman {
+		gc.Cgen(nl, &n2)
+		gc.Cgen(nr, &n1)
+		gmove(&n1, &n3)
+	} else {
+		gc.Cgen(nr, &n1)
+		gmove(&n1, &n3)
+		gc.Cgen(nl, &n2)
+	}
+
+	gc.Regfree(&n3)
+
+	// test and fix up large shifts
+	if !bounded {
+		gc.Nodconst(&n3, tcount, nl.Type.Width*8)
+		gcmp(optoas(gc.OCMP, tcount), &n1, &n3)
+		p1 := gc.Gbranch(optoas(gc.OLT, tcount), nil, +1)
+		if op == gc.ORSH && nl.Type.IsSigned() {
+			gc.Nodconst(&n3, gc.Types[gc.TUINT32], nl.Type.Width*8-1)
+			gins(a, &n3, &n2)
+		} else {
+			gc.Nodconst(&n3, nl.Type, 0)
+			gmove(&n3, &n2)
+		}
+
+		gc.Patch(p1, gc.Pc)
+	}
+
+	gins(a, &n1, &n2)
+
+	gmove(&n2, res)
+
+	gc.Regfree(&n1)
+	gc.Regfree(&n2)
+}
+
+func clearfat(nl *gc.Node) {
+	/* clear a fat object */
+	if gc.Debug['g'] != 0 {
+		fmt.Printf("clearfat %v (%v, size: %d)\n", nl, nl.Type, nl.Type.Width)
+	}
+
+	w := uint64(nl.Type.Width)
+
+	// Avoid taking the address for simple enough types.
+	if gc.Componentgen(nil, nl) {
+		return
+	}
+
+	c := w % 8 // bytes
+	q := w / 8 // dwords
+
+	var r0 gc.Node
+	gc.Nodreg(&r0, gc.Types[gc.TUINT64], sparc64.REG_ZR)
+	var dst gc.Node
+
+	// REGRT1 is reserved on sparc64, see sparc64/gsubr.go.
+	gc.Nodreg(&dst, gc.Types[gc.Tptr], sparc64.REG_RT1)
+	gc.Agen(nl, &dst)
+
+	var boff uint64
+	if q > 128 {
+		p := gins(sparc64.ASUB, nil, &dst)
+		p.From.Type = obj.TYPE_CONST
+		p.From.Offset = 8
+
+		var end gc.Node
+		gc.Regalloc(&end, gc.Types[gc.Tptr], nil)
+		p = gins(sparc64.AMOVD, &dst, &end)
+		p.From.Type = obj.TYPE_ADDR
+		p.From.Offset = int64(q * 8)
+
+		p = gins(sparc64.AMOVD, &r0, &dst)
+		p.To.Type = obj.TYPE_MEM
+		p.To.Offset = 8
+		pl := p
+
+		p = gins(sparc64.AADD, nil, &dst)
+		p.From.Type = obj.TYPE_CONST
+		p.From.Offset = 8
+
+		p = gcmp(sparc64.ACMP, &dst, &end)
+		gc.Patch(gc.Gbranch(sparc64.ABNED, nil, 0), pl)
+
+		gc.Regfree(&end)
+
+		// The loop leaves G1 (RT1) on the last zeroed dword
+		boff = 8
+	} else if false && q >= 4 {
+		// TODO(shawn):
+		// Disabled for now since it's likely that the call to duffzero
+		// will stomp on the link register in a tail call case; see
+		// mips issue https://golang.org/issue/12108
+		f := gc.Sysfunc("duffzero")
+		p := gins(obj.ADUFFZERO, nil, f)
+		gc.Afunclit(&p.To, f)
+
+		// 8 and 128 = magic constants: see ../../../../runtime/mkduff.go
+		// the extra +8 is to account for the prologue padding
+		// added by preprocess()
+		p.To.Offset = int64(8 * (128 - q)) + 8
+
+		// duffzero leaves G1 (RT1) on the last zeroed dword
+		boff = 8
+	} else {
+		var p *obj.Prog
+		for t := uint64(0); t < q; t++ {
+			p = gins(sparc64.AMOVD, &r0, &dst)
+			p.To.Type = obj.TYPE_MEM
+			p.To.Offset = int64(8 * t)
+		}
+
+		boff = 8 * q
+	}
+
+	var p *obj.Prog
+	for t := uint64(0); t < c; t++ {
+		p = gins(sparc64.AMOVB, &r0, &dst)
+		p.To.Type = obj.TYPE_MEM
+		p.To.Offset = int64(t + boff)
+	}
+}
+
+// Called after regopt and peep have run.
+// Expand CHECKNIL pseudo-op into actual nil pointer check.
+func expandchecks(firstp *obj.Prog) {
+	var p1 *obj.Prog
+
+	for p := firstp; p != nil; p = p.Link {
+		if gc.Debug_checknil != 0 && gc.Ctxt.Debugvlog != 0 {
+			fmt.Printf("expandchecks: %v\n", p)
+		}
+		if p.As != obj.ACHECKNIL {
+			continue
+		}
+		if gc.Debug_checknil != 0 && p.Lineno > 1 { // p->lineno==1 in generated wrappers
+			gc.Warnl(p.Lineno, "generated nil check")
+		}
+		if p.From.Type != obj.TYPE_REG {
+			gc.Fatalf("invalid nil check %v\n", p)
+		}
+
+		// check is
+		//	ABRNZ arg, 2(PC)
+		//	MOVD ZR, 0(arg)
+		p1 = gc.Ctxt.NewProg()
+		gc.Clearp(p1)
+		p1.Link = p.Link
+		p.Link = p1
+		p1.Lineno = p.Lineno
+		p1.Pc = 9999
+
+		p.As = sparc64.ABRNZ
+		p.To.Type = obj.TYPE_BRANCH
+		p.To.Val = p1.Link
+
+		// crash by write to memory address 0.
+		p1.As = sparc64.AMOVD
+		p1.From.Type = obj.TYPE_REG
+		p1.From.Reg = sparc64.REG_ZR
+		p1.To.Type = obj.TYPE_MEM
+		p1.To.Reg = p.From.Reg
+		p1.To.Offset = 0
+	}
+}
+
+// res = runtime.getg()
+func getg(res *gc.Node) {
+	var n1 gc.Node
+	gc.Nodreg(&n1, res.Type, sparc64.REG_G)
+	gmove(&n1, res)
+}
diff --git a/src/cmd/compile/internal/sparc64/gsubr.go b/src/cmd/compile/internal/sparc64/gsubr.go
new file mode 100644
index 0000000..39e3e8f
--- /dev/null
+++ b/src/cmd/compile/internal/sparc64/gsubr.go
@@ -0,0 +1,1119 @@
+// Derived from Inferno utils/6c/txt.c
+// http://code.google.com/p/inferno-os/source/browse/utils/6c/txt.c
+//
+//	Copyright © 1994-1999 Lucent Technologies Inc.  All rights reserved.
+//	Portions Copyright © 1995-1997 C H Forsyth (forsyth@terzarima.net)
+//	Portions Copyright © 1997-1999 Vita Nuova Limited
+//	Portions Copyright © 2000-2007 Vita Nuova Holdings Limited (www.vitanuova.com)
+//	Portions Copyright © 2004,2006 Bruce Ellis
+//	Portions Copyright © 2005-2007 C H Forsyth (forsyth@terzarima.net)
+//	Revisions Copyright © 2000-2007 Lucent Technologies Inc. and others
+//	Portions Copyright © 2009 The Go Authors.  All rights reserved.
+//
+// Permission is hereby granted, free of charge, to any person obtaining a copy
+// of this software and associated documentation files (the "Software"), to deal
+// in the Software without restriction, including without limitation the rights
+// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+// copies of the Software, and to permit persons to whom the Software is
+// furnished to do so, subject to the following conditions:
+//
+// The above copyright notice and this permission notice shall be included in
+// all copies or substantial portions of the Software.
+//
+// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL THE
+// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
+// THE SOFTWARE.
+
+package sparc64
+
+import (
+	"cmd/compile/internal/big"
+	"cmd/compile/internal/gc"
+	"cmd/internal/obj"
+	"cmd/internal/obj/sparc64"
+	"fmt"
+)
+
+var resvd = []int{
+	sparc64.REG_ZR,
+	sparc64.REG_RT1,
+	sparc64.REG_CTXT,
+	sparc64.REG_G,
+	sparc64.REG_RT2,
+	sparc64.REG_TMP,
+	sparc64.REG_G6,
+	sparc64.REG_TLS,
+	sparc64.REG_RSP,
+	sparc64.REG_OLR,
+	sparc64.REG_TMP2,
+	sparc64.REG_L7,
+	sparc64.REG_I0, // TODO(aram): revisit this.
+	sparc64.REG_I1, // TODO(aram): revisit this.
+	sparc64.REG_I2, // TODO(aram): revisit this.
+	sparc64.REG_I3, // TODO(aram): revisit this.
+	sparc64.REG_I4, // TODO(aram): revisit this.
+	sparc64.REG_I5, // TODO(aram): revisit this.
+	sparc64.REG_RFP,
+	sparc64.REG_ILR,
+	sparc64.REG_YTMP,
+	sparc64.REG_YTWO,
+}
+
+/*
+ * generate
+ *	as $c, n
+ */
+func ginscon(as obj.As, c int64, n2 *gc.Node) {
+	var n1 gc.Node
+
+	gc.Nodconst(&n1, gc.Types[gc.TINT64], c)
+
+	if as != sparc64.AMOVD && (c < -sparc64.BIG || c > sparc64.BIG) || as == sparc64.AMULD || n2 != nil && n2.Op != gc.OREGISTER {
+		// cannot have more than 13-bit of immediate in ADD, etc.
+		// instead, MOV into register first.
+		var ntmp gc.Node
+		gc.Regalloc(&ntmp, gc.Types[gc.TINT64], nil)
+
+		gins(sparc64.AMOVD, &n1, &ntmp)
+		gins(as, &ntmp, n2)
+		gc.Regfree(&ntmp)
+		return
+	}
+
+	rawgins(as, &n1, n2)
+}
+
+/*
+ * generate
+ *	as n, $c (CMP)
+ */
+func ginscon2(as obj.As, n2 *gc.Node, c int64) {
+	var n1 gc.Node
+
+	gc.Nodconst(&n1, gc.Types[gc.TINT64], c)
+
+	switch as {
+	default:
+		gc.Fatalf("ginscon2")
+
+	case sparc64.ACMP:
+		if -sparc64.BIG <= c && c <= sparc64.BIG {
+			gcmp(as, n2, &n1)
+			return
+		}
+	}
+
+	// MOV n1 into register first
+	var ntmp gc.Node
+	gc.Regalloc(&ntmp, gc.Types[gc.TINT64], nil)
+
+	rawgins(sparc64.AMOVD, &n1, &ntmp)
+	gcmp(as, n2, &ntmp)
+	gc.Regfree(&ntmp)
+}
+
+func ginscmp(op gc.Op, t *gc.Type, n1, n2 *gc.Node, likely int) *obj.Prog {
+	if gc.Isint[t.Etype] && n1.Op == gc.OLITERAL && n2.Op != gc.OLITERAL {
+		// Reverse comparison to place constant last.
+		op = gc.Brrev(op)
+		n1, n2 = n2, n1
+	}
+
+	var r1, r2, g1, g2 gc.Node
+	gc.Regalloc(&r1, t, n1)
+	gc.Regalloc(&g1, n1.Type, &r1)
+	gc.Cgen(n1, &g1)
+	gmove(&g1, &r1)
+	if gc.Isint[t.Etype] && gc.Isconst(n2, gc.CTINT) {
+		ginscon2(optoas(gc.OCMP, t), &r1, n2.Int64())
+	} else {
+		gc.Regalloc(&r2, t, n2)
+		gc.Regalloc(&g2, n1.Type, &r2)
+		gc.Cgen(n2, &g2)
+		gmove(&g2, &r2)
+		gcmp(optoas(gc.OCMP, t), &r1, &r2)
+		gc.Regfree(&g2)
+		gc.Regfree(&r2)
+	}
+	gc.Regfree(&g1)
+	gc.Regfree(&r1)
+	return gc.Gbranch(optoas(op, t), nil, likely)
+}
+
+// set up nodes representing 2^63
+var (
+	bigi         gc.Node
+	bigf         gc.Node
+	bignodes_did bool
+)
+
+func bignodes() {
+	if bignodes_did {
+		return
+	}
+	bignodes_did = true
+
+	var i big.Int
+	i.SetInt64(1)
+	i.Lsh(&i, 63)
+
+	gc.Nodconst(&bigi, gc.Types[gc.TUINT64], 0)
+	bigi.SetBigInt(&i)
+
+	bigi.Convconst(&bigf, gc.Types[gc.TFLOAT64])
+}
+
+/*
+ * generate move:
+ *	t = f
+ * hard part is conversions.
+ */
+func gmove(f *gc.Node, t *gc.Node) {
+	if gc.Debug['M'] != 0 {
+		fmt.Printf("gmove %v -> %v\n", gc.Nconv(f, gc.FmtLong), gc.Nconv(t, gc.FmtLong))
+	}
+
+	ft := int(gc.Simsimtype(f.Type))
+	tt := int(gc.Simsimtype(t.Type))
+	cvt := t.Type
+
+	if gc.Iscomplex[ft] || gc.Iscomplex[tt] {
+		gc.Complexmove(f, t)
+		return
+	}
+
+	// cannot have two memory operands
+	var r1 gc.Node
+	var r2 gc.Node
+	var a obj.As
+	if gc.Ismem(f) && gc.Ismem(t) {
+		goto hard
+	}
+
+	// convert constant to desired type
+	if f.Op == gc.OLITERAL {
+		var con gc.Node
+		switch tt {
+		default:
+			f.Convconst(&con, t.Type)
+
+		case gc.TINT32,
+			gc.TINT16,
+			gc.TINT8:
+			var con gc.Node
+			f.Convconst(&con, gc.Types[gc.TINT64])
+			var r1 gc.Node
+			gc.Regalloc(&r1, con.Type, t)
+			gins(sparc64.AMOVD, &con, &r1)
+			gmove(&r1, t)
+			gc.Regfree(&r1)
+			return
+
+		case gc.TUINT32,
+			gc.TUINT16,
+			gc.TUINT8:
+			var con gc.Node
+			f.Convconst(&con, gc.Types[gc.TUINT64])
+			var r1 gc.Node
+			gc.Regalloc(&r1, con.Type, t)
+			gins(sparc64.AMOVD, &con, &r1)
+			gmove(&r1, t)
+			gc.Regfree(&r1)
+			return
+		}
+
+		f = &con
+		ft = tt // so big switch will choose a simple mov
+
+		// constants can't move directly to memory.
+		if gc.Ismem(t) {
+			goto hard
+		}
+	}
+
+	// value -> value copy, first operand in memory.
+	// any floating point operand requires register
+	// src, so goto hard to copy to register first.
+	if gc.Ismem(f) && ft != tt && (gc.Isfloat[ft] || gc.Isfloat[tt]) {
+		cvt = gc.Types[ft]
+		goto hard
+	}
+
+	// value -> value copy, only one memory operand.
+	// figure out the instruction to use.
+	// break out of switch for one-instruction gins.
+	// goto fsrccpy for "float operation, and source is an integer register".
+	// goto fdstcpy for "float operation, and destination is an integer register".
+	// goto rdst for "destination must be register".
+	// goto hard for "convert to cvt type first".
+	// otherwise handle and return.
+
+	switch uint32(ft)<<16 | uint32(tt) {
+	default:
+		gc.Fatalf("gmove %v -> %v", gc.Tconv(f.Type, gc.FmtLong), gc.Tconv(t.Type, gc.FmtLong))
+
+		/*
+		 * integer copy and truncate
+		 */
+	case gc.TINT8<<16 | gc.TINT8, // same size
+		gc.TUINT8<<16 | gc.TINT8,
+		gc.TINT16<<16 | gc.TINT8,
+		// truncate
+		gc.TUINT16<<16 | gc.TINT8,
+		gc.TINT32<<16 | gc.TINT8,
+		gc.TUINT32<<16 | gc.TINT8,
+		gc.TINT64<<16 | gc.TINT8,
+		gc.TUINT64<<16 | gc.TINT8:
+		a = sparc64.AMOVB
+
+	case gc.TINT8<<16 | gc.TUINT8, // same size
+		gc.TUINT8<<16 | gc.TUINT8,
+		gc.TINT16<<16 | gc.TUINT8,
+		// truncate
+		gc.TUINT16<<16 | gc.TUINT8,
+		gc.TINT32<<16 | gc.TUINT8,
+		gc.TUINT32<<16 | gc.TUINT8,
+		gc.TINT64<<16 | gc.TUINT8,
+		gc.TUINT64<<16 | gc.TUINT8:
+		a = sparc64.AMOVUB
+
+	case gc.TINT16<<16 | gc.TINT16, // same size
+		gc.TUINT16<<16 | gc.TINT16,
+		gc.TINT32<<16 | gc.TINT16,
+		// truncate
+		gc.TUINT32<<16 | gc.TINT16,
+		gc.TINT64<<16 | gc.TINT16,
+		gc.TUINT64<<16 | gc.TINT16:
+		a = sparc64.AMOVH
+
+	case gc.TINT16<<16 | gc.TUINT16, // same size
+		gc.TUINT16<<16 | gc.TUINT16,
+		gc.TINT32<<16 | gc.TUINT16,
+		// truncate
+		gc.TUINT32<<16 | gc.TUINT16,
+		gc.TINT64<<16 | gc.TUINT16,
+		gc.TUINT64<<16 | gc.TUINT16:
+		a = sparc64.AMOVUH
+
+	case gc.TINT32<<16 | gc.TINT32, // same size
+		gc.TUINT32<<16 | gc.TINT32,
+		gc.TINT64<<16 | gc.TINT32,
+		// truncate
+		gc.TUINT64<<16 | gc.TINT32:
+		a = sparc64.AMOVW
+
+	case gc.TINT32<<16 | gc.TUINT32, // same size
+		gc.TUINT32<<16 | gc.TUINT32,
+		gc.TINT64<<16 | gc.TUINT32,
+		gc.TUINT64<<16 | gc.TUINT32:
+		a = sparc64.AMOVUW
+
+	case gc.TINT64<<16 | gc.TINT64, // same size
+		gc.TINT64<<16 | gc.TUINT64,
+		gc.TUINT64<<16 | gc.TINT64,
+		gc.TUINT64<<16 | gc.TUINT64:
+		a = sparc64.AMOVD
+
+		/*
+		 * integer up-conversions
+		 */
+	case gc.TINT8<<16 | gc.TINT16, // sign extend int8
+		gc.TINT8<<16 | gc.TUINT16,
+		gc.TINT8<<16 | gc.TINT32,
+		gc.TINT8<<16 | gc.TUINT32,
+		gc.TINT8<<16 | gc.TINT64,
+		gc.TINT8<<16 | gc.TUINT64:
+		a = sparc64.AMOVB
+
+		goto rdst
+
+	case gc.TUINT8<<16 | gc.TINT16, // zero extend uint8
+		gc.TUINT8<<16 | gc.TUINT16,
+		gc.TUINT8<<16 | gc.TINT32,
+		gc.TUINT8<<16 | gc.TUINT32,
+		gc.TUINT8<<16 | gc.TINT64,
+		gc.TUINT8<<16 | gc.TUINT64:
+		a = sparc64.AMOVUB
+
+		goto rdst
+
+	case gc.TINT16<<16 | gc.TINT32, // sign extend int16
+		gc.TINT16<<16 | gc.TUINT32,
+		gc.TINT16<<16 | gc.TINT64,
+		gc.TINT16<<16 | gc.TUINT64:
+		a = sparc64.AMOVH
+
+		goto rdst
+
+	case gc.TUINT16<<16 | gc.TINT32, // zero extend uint16
+		gc.TUINT16<<16 | gc.TUINT32,
+		gc.TUINT16<<16 | gc.TINT64,
+		gc.TUINT16<<16 | gc.TUINT64:
+		a = sparc64.AMOVUH
+
+		goto rdst
+
+	case gc.TINT32<<16 | gc.TINT64, // sign extend int32
+		gc.TINT32<<16 | gc.TUINT64:
+		a = sparc64.AMOVW
+
+		goto rdst
+
+	case gc.TUINT32<<16 | gc.TINT64, // zero extend uint32
+		gc.TUINT32<<16 | gc.TUINT64:
+		a = sparc64.AMOVUW
+
+		goto rdst
+
+	//return;
+	// algorithm is:
+	//	if small enough, use native float64 -> int64 conversion.
+	//	otherwise, subtract 2^63, convert, and add it back.
+	/*
+	* float to integer
+	 */
+	case gc.TFLOAT32<<16 | gc.TINT32,
+		gc.TFLOAT32<<16 | gc.TINT64,
+		gc.TFLOAT32<<16 | gc.TINT16,
+		gc.TFLOAT32<<16 | gc.TINT8,
+		gc.TFLOAT32<<16 | gc.TUINT16,
+		gc.TFLOAT32<<16 | gc.TUINT8,
+		gc.TFLOAT32<<16 | gc.TUINT32,
+		gc.TFLOAT32<<16 | gc.TUINT64:
+		cvt = gc.Types[gc.TFLOAT64]
+
+		goto hard
+
+	case gc.TFLOAT64<<16 | gc.TINT32,
+		gc.TFLOAT64<<16 | gc.TINT64,
+		gc.TFLOAT64<<16 | gc.TINT16,
+		gc.TFLOAT64<<16 | gc.TINT8,
+		gc.TFLOAT64<<16 | gc.TUINT16,
+		gc.TFLOAT64<<16 | gc.TUINT8,
+		gc.TFLOAT64<<16 | gc.TUINT32,
+		gc.TFLOAT64<<16 | gc.TUINT64:
+		bignodes()
+
+		var r1 gc.Node
+		gc.Regalloc(&r1, gc.Types[ft], f)
+		gmove(f, &r1)
+		if tt == gc.TUINT64 {
+			gc.Regalloc(&r2, gc.Types[gc.TFLOAT64], nil)
+			gmove(&bigf, &r2)
+			gins(sparc64.AFCMPD, &r2, &r1)
+			p1 := gc.Gbranch(optoas(gc.OGT, gc.Types[gc.TFLOAT64]), nil, +1)
+			gins(sparc64.AFSUBD, &r2, &r1)
+			gc.Patch(p1, gc.Pc)
+			gc.Regfree(&r2)
+		}
+
+		gc.Regalloc(&r2, gc.Types[gc.TFLOAT64], nil)
+		var r3 gc.Node
+		gc.Regalloc(&r3, gc.Types[gc.TINT64], t)
+		gins(sparc64.AFDTOX, &r1, &r2)
+		p1 := gins(sparc64.AFMOVD, &r2, nil)
+		p1.To.Type = obj.TYPE_MEM
+		p1.To.Reg = sparc64.REG_RSP
+		p1.To.Offset = -8 + sparc64.StackBias
+		p1 = gins(sparc64.AMOVD, nil, &r3)
+		p1.From.Type = obj.TYPE_MEM
+		p1.From.Reg = sparc64.REG_RSP
+		p1.From.Offset = -8 + sparc64.StackBias
+		gc.Regfree(&r2)
+		gc.Regfree(&r1)
+		if tt == gc.TUINT64 {
+			p1 := gc.Gbranch(optoas(gc.OGT, gc.Types[gc.TFLOAT64]), nil, +1)
+			gc.Nodreg(&r1, gc.Types[gc.TINT64], sparc64.REG_RT1)
+			gins(sparc64.AMOVD, &bigi, &r1)
+			gins(sparc64.AADD, &r1, &r3)
+			gc.Patch(p1, gc.Pc)
+		}
+
+		gmove(&r3, t)
+		gc.Regfree(&r3)
+		return
+
+		//warn("gmove: convert int to float not implemented: %N -> %N\n", f, t);
+	//return;
+	// algorithm is:
+	//	if small enough, use native int64 -> uint64 conversion.
+	//	otherwise, halve (rounding to odd?), convert, and double.
+	/*
+	 * integer to float
+	 */
+	case gc.TINT32<<16 | gc.TFLOAT32,
+		gc.TINT32<<16 | gc.TFLOAT64,
+		gc.TINT16<<16 | gc.TFLOAT32,
+		gc.TINT16<<16 | gc.TFLOAT64,
+		gc.TINT8<<16 | gc.TFLOAT32,
+		gc.TINT8<<16 | gc.TFLOAT64:
+		cvt = gc.Types[gc.TINT64]
+
+		goto hard
+
+	case gc.TUINT16<<16 | gc.TFLOAT32,
+		gc.TUINT16<<16 | gc.TFLOAT64,
+		gc.TUINT8<<16 | gc.TFLOAT32,
+		gc.TUINT8<<16 | gc.TFLOAT64,
+		gc.TUINT32<<16 | gc.TFLOAT32,
+		gc.TUINT32<<16 | gc.TFLOAT64:
+		cvt = gc.Types[gc.TUINT64]
+
+		goto hard
+
+	case gc.TINT64<<16 | gc.TFLOAT32,
+		gc.TINT64<<16 | gc.TFLOAT64,
+		gc.TUINT64<<16 | gc.TFLOAT32,
+		gc.TUINT64<<16 | gc.TFLOAT64:
+		bignodes()
+
+		// The algorithm is:
+		//	if small enough, use native int64 -> float64 conversion,
+		//	otherwise halve (x -> (x>>1)|(x&1)), convert, and double.
+		var r1 gc.Node
+		gc.Regalloc(&r1, gc.Types[gc.TINT64], nil)
+		gmove(f, &r1)
+		if ft == gc.TUINT64 {
+			gc.Nodreg(&r2, gc.Types[gc.TUINT64], sparc64.REG_RT1)
+			gmove(&bigi, &r2)
+			gins(sparc64.ACMP, &r1, &r2)
+			p1 := gc.Gbranch(sparc64.ABLEUD, nil, +1)
+			var r3 gc.Node
+			gc.Regalloc(&r3, gc.Types[gc.TUINT64], nil)
+			p2 := gins(sparc64.AAND, nil, &r3) // andi.
+			p2.Reg = r1.Reg
+			p2.From.Type = obj.TYPE_CONST
+			p2.From.Offset = 1
+			p3 := gins(sparc64.ASRLD, nil, &r1)
+			p3.From.Type = obj.TYPE_CONST
+			p3.From.Offset = 1
+			gins(sparc64.AOR, &r3, &r1)
+			gc.Regfree(&r3)
+			gc.Patch(p1, gc.Pc)
+		}
+
+		gc.Regalloc(&r2, gc.Types[gc.TFLOAT64], t)
+		p1 := gins(sparc64.AMOVD, &r1, nil)
+		p1.To.Type = obj.TYPE_MEM
+		p1.To.Reg = sparc64.REG_RSP
+		p1.To.Offset = -8 + sparc64.StackBias
+		p1 = gins(sparc64.AFMOVD, nil, &r2)
+		p1.From.Type = obj.TYPE_MEM
+		p1.From.Reg = sparc64.REG_RSP
+		p1.From.Offset = -8 + sparc64.StackBias
+		gins(sparc64.AFXTOD, &r2, &r2)
+		gc.Regfree(&r1)
+		if ft == gc.TUINT64 {
+			p1 := gc.Gbranch(sparc64.ABLEUD, nil, +1)
+			gc.Nodreg(&r1, gc.Types[gc.TFLOAT64], sparc64.REG_YTWO)
+			gins(sparc64.AFMULD, &r1, &r2)
+			gc.Patch(p1, gc.Pc)
+		}
+
+		gmove(&r2, t)
+		gc.Regfree(&r2)
+		return
+
+	/*
+	 * float to float
+	 */
+	case gc.TFLOAT32<<16 | gc.TFLOAT32:
+		a = sparc64.AFMOVS
+
+	case gc.TFLOAT64<<16 | gc.TFLOAT64:
+		a = sparc64.AFMOVD
+
+	case gc.TFLOAT32<<16 | gc.TFLOAT64:
+		a = sparc64.AFSTOD
+		goto rdst
+
+	case gc.TFLOAT64<<16 | gc.TFLOAT32:
+		a = sparc64.AFDTOS
+		goto rdst
+	}
+
+	gins(a, f, t)
+	return
+
+	// requires register destination
+rdst:
+	gc.Regalloc(&r1, t.Type, t)
+
+	gins(a, f, &r1)
+	gmove(&r1, t)
+	gc.Regfree(&r1)
+	return
+
+	// requires register intermediate
+hard:
+	gc.Regalloc(&r1, cvt, t)
+
+	gmove(f, &r1)
+	gmove(&r1, t)
+	gc.Regfree(&r1)
+	return
+}
+
+// gins is called by the front end.
+// It synthesizes some multiple-instruction sequences
+// so the front end can stay simpler.
+func gins(as obj.As, f, t *gc.Node) *obj.Prog {
+	if as >= obj.A_ARCHSPECIFIC {
+		if x, ok := f.IntLiteral(); ok {
+			ginscon(as, x, t)
+			return nil // caller must not use
+		}
+	}
+	if as == sparc64.ACMP {
+		if x, ok := t.IntLiteral(); ok {
+			ginscon2(as, f, x)
+			return nil // caller must not use
+		}
+	}
+	return rawgins(as, f, t)
+}
+
+/*
+ * generate one instruction:
+ *	as f, t
+ */
+func rawgins(as obj.As, f *gc.Node, t *gc.Node) *obj.Prog {
+	// TODO(austin): Add self-move test like in 6g (but be careful
+	// of truncation moves)
+
+	p := gc.Prog(as)
+	gc.Naddr(&p.From, f)
+	gc.Naddr(&p.To, t)
+
+	switch as {
+	case sparc64.ACMP, sparc64.AFCMPS, sparc64.AFCMPD:
+		if t != nil {
+			if f.Op != gc.OREGISTER {
+				gc.Fatalf("bad operands to gcmp")
+			}
+			p.From = p.To
+			p.To = obj.Addr{}
+			raddr(f, p)
+		}
+	}
+
+	// Bad things the front end has done to us. Crash to find call stack.
+	switch as {
+	case sparc64.AAND, sparc64.AMULD:
+		if p.From.Type == obj.TYPE_CONST {
+			gc.Debug['h'] = 1
+			gc.Fatalf("bad inst: %v", p)
+		}
+	case sparc64.ACMP:
+		if p.From.Type == obj.TYPE_MEM || p.To.Type == obj.TYPE_MEM {
+			gc.Debug['h'] = 1
+			gc.Fatalf("bad inst: %v", p)
+		}
+	}
+
+	if gc.Debug['g'] != 0 {
+		fmt.Printf("%v\n", p)
+	}
+
+	w := int32(0)
+	switch as {
+	case sparc64.AMOVB,
+		sparc64.AMOVUB:
+		w = 1
+
+	case sparc64.AMOVH,
+		sparc64.AMOVUH:
+		w = 2
+
+	case sparc64.AMOVW,
+		sparc64.AMOVUW:
+		w = 4
+
+	case sparc64.AMOVD:
+		if p.From.Type == obj.TYPE_CONST || p.From.Type == obj.TYPE_ADDR {
+			break
+		}
+		w = 8
+	}
+
+	if w != 0 && ((f != nil && p.From.Width < int64(w)) || (t != nil && p.To.Type != obj.TYPE_REG && p.To.Width > int64(w))) {
+		gc.Dump("f", f)
+		gc.Dump("t", t)
+		gc.Fatalf("bad width: %v (%d, %d)\n", p, p.From.Width, p.To.Width)
+	}
+
+	return p
+}
+
+/*
+ * insert n into reg slot of p
+ */
+func raddr(n *gc.Node, p *obj.Prog) {
+	var a obj.Addr
+
+	gc.Naddr(&a, n)
+	if a.Type != obj.TYPE_REG {
+		if n != nil {
+			gc.Fatalf("bad in raddr: %v", n.Op)
+		} else {
+			gc.Fatalf("bad in raddr: <null>")
+		}
+		p.Reg = 0
+	} else {
+		p.Reg = a.Reg
+	}
+}
+
+func gcmp(as obj.As, lhs *gc.Node, rhs *gc.Node) *obj.Prog {
+	if lhs.Op != gc.OREGISTER {
+		gc.Fatalf("bad operands to gcmp: %v %v", lhs.Op, rhs.Op)
+	}
+
+	p := rawgins(as, rhs, nil)
+	raddr(lhs, p)
+	return p
+}
+
+/*
+ * return Axxx for Oxxx on type t.
+ */
+func optoas(op gc.Op, t *gc.Type) obj.As {
+	if t == nil {
+		gc.Fatalf("optoas: t is nil")
+	}
+
+	// avoid constant conversions in switches below
+	const (
+		OMINUS_ = uint32(gc.OMINUS) << 16
+		OLSH_   = uint32(gc.OLSH) << 16
+		ORSH_   = uint32(gc.ORSH) << 16
+		OADD_   = uint32(gc.OADD) << 16
+		OSUB_   = uint32(gc.OSUB) << 16
+		OMUL_   = uint32(gc.OMUL) << 16
+		ODIV_   = uint32(gc.ODIV) << 16
+		OOR_    = uint32(gc.OOR) << 16
+		OAND_   = uint32(gc.OAND) << 16
+		OXOR_   = uint32(gc.OXOR) << 16
+		OEQ_    = uint32(gc.OEQ) << 16
+		ONE_    = uint32(gc.ONE) << 16
+		OLT_    = uint32(gc.OLT) << 16
+		OLE_    = uint32(gc.OLE) << 16
+		OGE_    = uint32(gc.OGE) << 16
+		OGT_    = uint32(gc.OGT) << 16
+		OCMP_   = uint32(gc.OCMP) << 16
+		OAS_    = uint32(gc.OAS) << 16
+		OHMUL_  = uint32(gc.OHMUL) << 16
+		OSQRT_  = uint32(gc.OSQRT) << 16
+	)
+
+	a := obj.AXXX
+	switch uint32(op)<<16 | uint32(gc.Simtype[t.Etype]) {
+	default:
+		gc.Fatalf("optoas: no entry for op=%v type=%v", op, t)
+
+	case OEQ_ | gc.TBOOL,
+		OEQ_ | gc.TINT8,
+		OEQ_ | gc.TUINT8,
+		OEQ_ | gc.TINT16,
+		OEQ_ | gc.TUINT16,
+		OEQ_ | gc.TINT32,
+		OEQ_ | gc.TUINT32,
+		OEQ_ | gc.TPTR32:
+		a = sparc64.ABEW
+
+	case OEQ_ | gc.TINT64,
+		OEQ_ | gc.TUINT64,
+		OEQ_ | gc.TPTR64:
+		a = sparc64.ABED
+
+	case OEQ_ | gc.TFLOAT32,
+		OEQ_ | gc.TFLOAT64:
+		a = sparc64.AFBE
+
+	case ONE_ | gc.TBOOL,
+		ONE_ | gc.TINT8,
+		ONE_ | gc.TUINT8,
+		ONE_ | gc.TINT16,
+		ONE_ | gc.TUINT16,
+		ONE_ | gc.TINT32,
+		ONE_ | gc.TUINT32,
+		ONE_ | gc.TPTR32:
+		a = sparc64.ABNEW
+
+	case ONE_ | gc.TINT64,
+		ONE_ | gc.TUINT64,
+		ONE_ | gc.TPTR64:
+		a = sparc64.ABNED
+
+	case ONE_ | gc.TFLOAT32,
+		ONE_ | gc.TFLOAT64:
+		a = sparc64.AFBNE
+
+	case OLT_ | gc.TINT8,
+		OLT_ | gc.TINT16,
+		OLT_ | gc.TINT32:
+		a = sparc64.ABLW
+
+	case OLT_ | gc.TINT64:
+		a = sparc64.ABLD
+
+	case OLT_ | gc.TUINT8,
+		OLT_ | gc.TUINT16,
+		OLT_ | gc.TUINT32:
+		a = sparc64.ABCSW
+
+	case OLT_ | gc.TUINT64:
+		a = sparc64.ABCSD
+
+	case OLT_ | gc.TFLOAT32,
+		OLT_ | gc.TFLOAT64:
+		a = sparc64.AFBL
+
+	case OLE_ | gc.TINT8,
+		OLE_ | gc.TINT16,
+		OLE_ | gc.TINT32:
+		a = sparc64.ABLEW
+
+	case OLE_ | gc.TINT64:
+		a = sparc64.ABLED
+
+	case OLE_ | gc.TUINT8,
+		OLE_ | gc.TUINT16,
+		OLE_ | gc.TUINT32:
+		a = sparc64.ABLEUW
+
+	case OLE_ | gc.TUINT64:
+		a = sparc64.ABLEUD
+
+	case OLE_ | gc.TFLOAT32,
+		OLE_ | gc.TFLOAT64:
+		a = sparc64.AFBLE
+
+	case OGT_ | gc.TINT8,
+		OGT_ | gc.TINT16,
+		OGT_ | gc.TINT32:
+		a = sparc64.ABGW
+
+	case OGT_ | gc.TINT64:
+		a = sparc64.ABGD
+
+	case OGT_ | gc.TFLOAT32,
+		OGT_ | gc.TFLOAT64:
+		a = sparc64.AFBG
+
+	case OGT_ | gc.TUINT8,
+		OGT_ | gc.TUINT16,
+		OGT_ | gc.TUINT32:
+		a = sparc64.ABGUW
+
+	case OGT_ | gc.TUINT64:
+		a = sparc64.ABGUD
+
+	case OGE_ | gc.TINT8,
+		OGE_ | gc.TINT16,
+		OGE_ | gc.TINT32:
+		a = sparc64.ABGEW
+
+	case OGE_ | gc.TINT64:
+		a = sparc64.ABGED
+
+	case OGE_ | gc.TFLOAT32,
+		OGE_ | gc.TFLOAT64:
+		a = sparc64.AFBGE
+
+	case OGE_ | gc.TUINT8,
+		OGE_ | gc.TUINT16,
+		OGE_ | gc.TUINT32:
+		a = sparc64.ABCCW
+
+	case OGE_ | gc.TUINT64:
+		a = sparc64.ABCCD
+
+	case OCMP_ | gc.TBOOL,
+		OCMP_ | gc.TINT8,
+		OCMP_ | gc.TINT16,
+		OCMP_ | gc.TINT32,
+		OCMP_ | gc.TPTR32,
+		OCMP_ | gc.TINT64,
+		OCMP_ | gc.TUINT8,
+		OCMP_ | gc.TUINT16,
+		OCMP_ | gc.TUINT32,
+		OCMP_ | gc.TUINT64,
+		OCMP_ | gc.TPTR64:
+		a = sparc64.ACMP
+
+	case OCMP_ | gc.TFLOAT32:
+		a = sparc64.AFCMPS
+
+	case OCMP_ | gc.TFLOAT64:
+		a = sparc64.AFCMPD
+
+	case OAS_ | gc.TBOOL,
+		OAS_ | gc.TINT8:
+		a = sparc64.AMOVB
+
+	case OAS_ | gc.TUINT8:
+		a = sparc64.AMOVUB
+
+	case OAS_ | gc.TINT16:
+		a = sparc64.AMOVH
+
+	case OAS_ | gc.TUINT16:
+		a = sparc64.AMOVUH
+
+	case OAS_ | gc.TINT32:
+		a = sparc64.AMOVW
+
+	case OAS_ | gc.TUINT32,
+		OAS_ | gc.TPTR32:
+		a = sparc64.AMOVUW
+
+	case OAS_ | gc.TINT64,
+		OAS_ | gc.TUINT64,
+		OAS_ | gc.TPTR64:
+		a = sparc64.AMOVD
+
+	case OAS_ | gc.TFLOAT32:
+		a = sparc64.AFMOVS
+
+	case OAS_ | gc.TFLOAT64:
+		a = sparc64.AFMOVD
+
+	case OADD_ | gc.TINT8,
+		OADD_ | gc.TUINT8,
+		OADD_ | gc.TINT16,
+		OADD_ | gc.TUINT16,
+		OADD_ | gc.TINT32,
+		OADD_ | gc.TUINT32,
+		OADD_ | gc.TPTR32,
+		OADD_ | gc.TINT64,
+		OADD_ | gc.TUINT64,
+		OADD_ | gc.TPTR64:
+		a = sparc64.AADD
+
+	case OADD_ | gc.TFLOAT32:
+		a = sparc64.AFADDS
+
+	case OADD_ | gc.TFLOAT64:
+		a = sparc64.AFADDD
+
+	case OSUB_ | gc.TINT8,
+		OSUB_ | gc.TUINT8,
+		OSUB_ | gc.TINT16,
+		OSUB_ | gc.TUINT16,
+		OSUB_ | gc.TINT32,
+		OSUB_ | gc.TUINT32,
+		OSUB_ | gc.TPTR32,
+		OSUB_ | gc.TINT64,
+		OSUB_ | gc.TUINT64,
+		OSUB_ | gc.TPTR64:
+		a = sparc64.ASUB
+
+	case OSUB_ | gc.TFLOAT32:
+		a = sparc64.AFSUBS
+
+	case OSUB_ | gc.TFLOAT64:
+		a = sparc64.AFSUBD
+
+	case OMINUS_ | gc.TINT8,
+		OMINUS_ | gc.TUINT8,
+		OMINUS_ | gc.TINT16,
+		OMINUS_ | gc.TUINT16,
+		OMINUS_ | gc.TINT32,
+		OMINUS_ | gc.TUINT32,
+		OMINUS_ | gc.TPTR32,
+		OMINUS_ | gc.TINT64,
+		OMINUS_ | gc.TUINT64,
+		OMINUS_ | gc.TPTR64:
+		a = sparc64.ANEG
+
+	case OMINUS_ | gc.TFLOAT32:
+		a = sparc64.AFNEGS
+
+	case OMINUS_ | gc.TFLOAT64:
+		a = sparc64.AFNEGD
+
+	case OAND_ | gc.TINT8,
+		OAND_ | gc.TUINT8,
+		OAND_ | gc.TINT16,
+		OAND_ | gc.TUINT16,
+		OAND_ | gc.TINT32,
+		OAND_ | gc.TUINT32,
+		OAND_ | gc.TPTR32,
+		OAND_ | gc.TINT64,
+		OAND_ | gc.TUINT64,
+		OAND_ | gc.TPTR64:
+		a = sparc64.AAND
+
+	case OOR_ | gc.TINT8,
+		OOR_ | gc.TUINT8,
+		OOR_ | gc.TINT16,
+		OOR_ | gc.TUINT16,
+		OOR_ | gc.TINT32,
+		OOR_ | gc.TUINT32,
+		OOR_ | gc.TPTR32,
+		OOR_ | gc.TINT64,
+		OOR_ | gc.TUINT64,
+		OOR_ | gc.TPTR64:
+		a = sparc64.AOR
+
+	case OXOR_ | gc.TINT8,
+		OXOR_ | gc.TUINT8,
+		OXOR_ | gc.TINT16,
+		OXOR_ | gc.TUINT16,
+		OXOR_ | gc.TINT32,
+		OXOR_ | gc.TUINT32,
+		OXOR_ | gc.TPTR32,
+		OXOR_ | gc.TINT64,
+		OXOR_ | gc.TUINT64,
+		OXOR_ | gc.TPTR64:
+		a = sparc64.AXOR
+
+		// TODO(minux): handle rotates
+	//case CASE(OLROT, TINT8):
+	//case CASE(OLROT, TUINT8):
+	//case CASE(OLROT, TINT16):
+	//case CASE(OLROT, TUINT16):
+	//case CASE(OLROT, TINT32):
+	//case CASE(OLROT, TUINT32):
+	//case CASE(OLROT, TPTR32):
+	//case CASE(OLROT, TINT64):
+	//case CASE(OLROT, TUINT64):
+	//case CASE(OLROT, TPTR64):
+	//	a = 0//???; RLDC?
+	//	break;
+
+	case OLSH_ | gc.TINT8,
+		OLSH_ | gc.TUINT8,
+		OLSH_ | gc.TINT16,
+		OLSH_ | gc.TUINT16,
+		OLSH_ | gc.TINT32,
+		OLSH_ | gc.TUINT32,
+		OLSH_ | gc.TPTR32:
+		a = sparc64.ASLLW
+
+	case OLSH_ | gc.TINT64,
+		OLSH_ | gc.TUINT64,
+		OLSH_ | gc.TPTR64:
+		a = sparc64.ASLLD
+
+	case ORSH_ | gc.TUINT8,
+		ORSH_ | gc.TUINT16,
+		ORSH_ | gc.TUINT32,
+		ORSH_ | gc.TPTR32:
+		a = sparc64.ASRLW
+
+	case ORSH_ | gc.TUINT64,
+		ORSH_ | gc.TPTR64:
+		a = sparc64.ASRLD
+
+	case ORSH_ | gc.TINT8,
+		ORSH_ | gc.TINT16,
+		ORSH_ | gc.TINT32:
+		a = sparc64.ASRAW
+
+	case ORSH_ | gc.TINT64:
+		a = sparc64.ASRAD
+
+	// TODO(shawn): handle rotates, likely via addcc/addxc and PSR icc
+	// overflow bit
+	//case CASE(ORROTC, TINT8):
+	//case CASE(ORROTC, TUINT8):
+	//case CASE(ORROTC, TINT16):
+	//case CASE(ORROTC, TUINT16):
+	//case CASE(ORROTC, TINT32):
+	//case CASE(ORROTC, TUINT32):
+	//case CASE(ORROTC, TINT64):
+	//case CASE(ORROTC, TUINT64):
+	//	a = 0//??? RLDC??
+	//	break;
+
+	// TODO(aram): handle high-multiply via mulx?
+	//case OHMUL_ | gc.TINT64:
+	//	a = sparc64.ASMULH
+	//
+	//case OHMUL_ | gc.TUINT64,
+	//	OHMUL_ | gc.TPTR64:
+	//	a = sparc64.AUMULH
+
+	case OMUL_ | gc.TINT8,
+		OMUL_ | gc.TINT16,
+		OMUL_ | gc.TINT32,
+		OMUL_ | gc.TINT64,
+		OMUL_ | gc.TUINT8,
+		OMUL_ | gc.TUINT16,
+		OMUL_ | gc.TUINT32,
+		OMUL_ | gc.TPTR32,
+		OMUL_ | gc.TUINT64,
+		OMUL_ | gc.TPTR64:
+		a = sparc64.AMULD
+
+	case OMUL_ | gc.TFLOAT32:
+		a = sparc64.AFMULS
+
+	case OMUL_ | gc.TFLOAT64:
+		a = sparc64.AFMULD
+
+	case ODIV_ | gc.TINT8,
+		ODIV_ | gc.TINT16,
+		ODIV_ | gc.TINT32,
+		ODIV_ | gc.TINT64:
+		a = sparc64.ASDIVD
+
+	case ODIV_ | gc.TUINT8,
+		ODIV_ | gc.TUINT16,
+		ODIV_ | gc.TUINT32,
+		ODIV_ | gc.TPTR32,
+		ODIV_ | gc.TUINT64,
+		ODIV_ | gc.TPTR64:
+		a = sparc64.AUDIVD
+
+	case ODIV_ | gc.TFLOAT32:
+		a = sparc64.AFDIVS
+
+	case ODIV_ | gc.TFLOAT64:
+		a = sparc64.AFDIVD
+
+	case OSQRT_ | gc.TFLOAT64:
+		a = sparc64.AFSQRTD
+	}
+
+	return a
+}
+
+const (
+	ODynam   = 1 << 0
+	OAddable = 1 << 1
+)
+
+func xgen(n *gc.Node, a *gc.Node, o int) bool {
+	// TODO(minux)
+
+	return -1 != 0 /*TypeKind(100016)*/
+}
+
+func sudoclean() {
+	return
+}
+
+/*
+ * generate code to compute address of n,
+ * a reference to a (perhaps nested) field inside
+ * an array or struct.
+ * return 0 on failure, 1 on success.
+ * on success, leaves usable address in a.
+ *
+ * caller is responsible for calling sudoclean
+ * after successful sudoaddable,
+ * to release the register used for a.
+ */
+func sudoaddable(as obj.As, n *gc.Node, a *obj.Addr) bool {
+	// TODO(minux)
+
+	*a = obj.Addr{}
+	return false
+}
diff --git a/src/cmd/compile/internal/sparc64/peep.go b/src/cmd/compile/internal/sparc64/peep.go
new file mode 100644
index 0000000..ab9d136
--- /dev/null
+++ b/src/cmd/compile/internal/sparc64/peep.go
@@ -0,0 +1,81 @@
+// Derived from Inferno utils/6c/peep.c
+// http://code.google.com/p/inferno-os/source/browse/utils/6c/peep.c
+//
+//	Copyright © 1994-1999 Lucent Technologies Inc.  All rights reserved.
+//	Portions Copyright © 1995-1997 C H Forsyth (forsyth@terzarima.net)
+//	Portions Copyright © 1997-1999 Vita Nuova Limited
+//	Portions Copyright © 2000-2007 Vita Nuova Holdings Limited (www.vitanuova.com)
+//	Portions Copyright © 2004,2006 Bruce Ellis
+//	Portions Copyright © 2005-2007 C H Forsyth (forsyth@terzarima.net)
+//	Revisions Copyright © 2000-2007 Lucent Technologies Inc. and others
+//	Portions Copyright © 2009 The Go Authors.  All rights reserved.
+//
+// Permission is hereby granted, free of charge, to any person obtaining a copy
+// of this software and associated documentation files (the "Software"), to deal
+// in the Software without restriction, including without limitation the rights
+// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+// copies of the Software, and to permit persons to whom the Software is
+// furnished to do so, subject to the following conditions:
+//
+// The above copyright notice and this permission notice shall be included in
+// all copies or substantial portions of the Software.
+//
+// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL THE
+// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
+// THE SOFTWARE.
+
+package sparc64
+
+import (
+	"cmd/compile/internal/gc"
+	"cmd/internal/obj"
+	"cmd/internal/obj/sparc64"
+	"fmt"
+)
+
+var gactive uint32
+
+func peep(firstp *obj.Prog) {
+	// TODO(aram):
+}
+
+func excise(r *gc.Flow) {
+	p := r.Prog
+	if gc.Debug['P'] != 0 && gc.Debug['v'] != 0 {
+		fmt.Printf("%v ===delete===\n", p)
+	}
+	obj.Nopout(p)
+	gc.Ostats.Ndelmov++
+}
+
+func regtyp(a *obj.Addr) bool {
+	// TODO(rsc): Floating point register exclusions?
+	return a.Type == obj.TYPE_REG && sparc64.REG_G0 <= a.Reg && a.Reg <= sparc64.REG_Y15 && a.Reg != sparc64.REG_ZR
+}
+
+func sameaddr(a *obj.Addr, v *obj.Addr) bool {
+	if a.Type != v.Type {
+		return false
+	}
+	if regtyp(v) && a.Reg == v.Reg {
+		return true
+	}
+	if v.Type == obj.NAME_AUTO || v.Type == obj.NAME_PARAM {
+		if v.Offset == a.Offset {
+			return true
+		}
+	}
+	return false
+}
+
+func smallindir(a *obj.Addr, reg *obj.Addr) bool {
+	return reg.Type == obj.TYPE_REG && a.Type == obj.TYPE_MEM && a.Reg == reg.Reg && 0 <= a.Offset && a.Offset < 4096
+}
+
+func stackaddr(a *obj.Addr) bool {
+	return a.Type == obj.TYPE_REG && a.Reg == sparc64.REG_RSP
+}
diff --git a/src/cmd/compile/internal/sparc64/prog.go b/src/cmd/compile/internal/sparc64/prog.go
new file mode 100644
index 0000000..996b41b
--- /dev/null
+++ b/src/cmd/compile/internal/sparc64/prog.go
@@ -0,0 +1,183 @@
+// Copyright 2014 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package sparc64
+
+import (
+	"cmd/compile/internal/gc"
+	"cmd/internal/obj"
+	"cmd/internal/obj/sparc64"
+)
+
+const (
+	LeftRdwr  uint32 = gc.LeftRead | gc.LeftWrite
+	RightRdwr uint32 = gc.RightRead | gc.RightWrite
+)
+
+// This table gives the basic information about instruction
+// generated by the compiler and processed in the optimizer.
+// See opt.h for bit definitions.
+//
+// Instructions not generated need not be listed.
+// As an exception to that rule, we typically write down all the
+// size variants of an operation even if we just use a subset.
+//
+// The table is formatted for 8-space tabs.
+var progtable = [sparc64.ALAST]obj.ProgInfo{
+	obj.ATYPE:     {Flags: gc.Pseudo | gc.Skip},
+	obj.ATEXT:     {Flags: gc.Pseudo},
+	obj.AFUNCDATA: {Flags: gc.Pseudo},
+	obj.APCDATA:   {Flags: gc.Pseudo},
+	obj.AUNDEF:    {Flags: gc.Break},
+	obj.AUSEFIELD: {Flags: gc.OK},
+	obj.ACHECKNIL: {Flags: gc.LeftRead},
+	obj.AVARDEF:   {Flags: gc.Pseudo | gc.RightWrite},
+	obj.AVARKILL:  {Flags: gc.Pseudo | gc.RightWrite},
+	obj.AVARLIVE:  {Flags: gc.Pseudo | gc.LeftRead},
+
+	// NOP is an internal no-op that also stands
+	// for USED and SET annotations, not the Power opcode.
+	obj.ANOP:      {Flags: gc.LeftRead | gc.RightWrite},
+	sparc64.ARNOP: {Flags: gc.OK},
+
+	// Integer
+	sparc64.AADD:   {Flags: gc.SizeQ | gc.LeftRead | gc.RegRead | gc.RightWrite},
+	sparc64.ASUB:   {Flags: gc.SizeQ | gc.LeftRead | gc.RegRead | gc.RightWrite},
+	sparc64.ANEG:   {Flags: gc.SizeQ | gc.LeftRead | gc.RegRead | gc.RightWrite},
+	sparc64.AAND:   {Flags: gc.SizeQ | gc.LeftRead | gc.RegRead | gc.RightWrite},
+	sparc64.AOR:    {Flags: gc.SizeQ | gc.LeftRead | gc.RegRead | gc.RightWrite},
+	sparc64.AXOR:   {Flags: gc.SizeQ | gc.LeftRead | gc.RegRead | gc.RightWrite},
+	sparc64.AMULD:  {Flags: gc.SizeQ | gc.LeftRead | gc.RegRead | gc.RightWrite},
+	sparc64.ASDIVD: {Flags: gc.SizeQ | gc.LeftRead | gc.RegRead | gc.RightWrite},
+	sparc64.AUDIVD: {Flags: gc.SizeQ | gc.LeftRead | gc.RegRead | gc.RightWrite},
+	sparc64.ASLLD:  {Flags: gc.SizeQ | gc.LeftRead | gc.RegRead | gc.RightWrite},
+	sparc64.ASRLD:  {Flags: gc.SizeQ | gc.LeftRead | gc.RegRead | gc.RightWrite},
+	sparc64.ASRAD:  {Flags: gc.SizeQ | gc.LeftRead | gc.RegRead | gc.RightWrite},
+	sparc64.ASLLW:  {Flags: gc.SizeL | gc.LeftRead | gc.RegRead | gc.RightWrite},
+	sparc64.ASRLW:  {Flags: gc.SizeL | gc.LeftRead | gc.RegRead | gc.RightWrite},
+	sparc64.ASRAW:  {Flags: gc.SizeL | gc.LeftRead | gc.RegRead | gc.RightWrite},
+	sparc64.ACMP:   {Flags: gc.SizeQ | gc.LeftRead | gc.RegRead},
+
+	// Floating point.
+	sparc64.AFADDD:  {Flags: gc.SizeD | gc.LeftRead | gc.RegRead | gc.RightWrite},
+	sparc64.AFADDS:  {Flags: gc.SizeF | gc.LeftRead | gc.RegRead | gc.RightWrite},
+	sparc64.AFSUBD:  {Flags: gc.SizeD | gc.LeftRead | gc.RegRead | gc.RightWrite},
+	sparc64.AFSUBS:  {Flags: gc.SizeF | gc.LeftRead | gc.RegRead | gc.RightWrite},
+	sparc64.AFNEGD:  {Flags: gc.SizeD | gc.LeftRead | gc.RightWrite},
+	sparc64.AFNEGS:  {Flags: gc.SizeF | gc.LeftRead | gc.RightWrite},
+	sparc64.AFSQRTD: {Flags: gc.SizeD | gc.LeftRead | gc.RightWrite},
+	sparc64.AFMULD:  {Flags: gc.SizeD | gc.LeftRead | gc.RegRead | gc.RightWrite},
+	sparc64.AFMULS:  {Flags: gc.SizeF | gc.LeftRead | gc.RegRead | gc.RightWrite},
+	sparc64.AFDIVD:  {Flags: gc.SizeD | gc.LeftRead | gc.RegRead | gc.RightWrite},
+	sparc64.AFDIVS:  {Flags: gc.SizeF | gc.LeftRead | gc.RegRead | gc.RightWrite},
+	sparc64.AFCMPD:  {Flags: gc.SizeD | gc.LeftRead | gc.RegRead},
+	sparc64.AFCMPS:  {Flags: gc.SizeF | gc.LeftRead | gc.RegRead},
+
+	// float -> integer
+	sparc64.AFDTOX: {Flags: gc.SizeD | gc.LeftRead | gc.RightWrite | gc.Conv},
+	sparc64.AFSTOX: {Flags: gc.SizeF | gc.LeftRead | gc.RightWrite | gc.Conv},
+	sparc64.AFDTOI: {Flags: gc.SizeD | gc.LeftRead | gc.RightWrite | gc.Conv},
+	sparc64.AFSTOI: {Flags: gc.SizeF | gc.LeftRead | gc.RightWrite | gc.Conv},
+
+	// float -> float
+	sparc64.AFSTOD: {Flags: gc.SizeD | gc.LeftRead | gc.RightWrite | gc.Conv},
+	sparc64.AFDTOS: {Flags: gc.SizeD | gc.LeftRead | gc.RightWrite | gc.Conv},
+
+	// integer -> float
+	sparc64.AFXTOD: {Flags: gc.SizeQ | gc.LeftRead | gc.RightWrite | gc.Conv},
+	sparc64.AFXTOS: {Flags: gc.SizeQ | gc.LeftRead | gc.RightWrite | gc.Conv},
+	sparc64.AFITOD: {Flags: gc.SizeL | gc.LeftRead | gc.RightWrite | gc.Conv},
+	sparc64.AFITOS: {Flags: gc.SizeL | gc.LeftRead | gc.RightWrite | gc.Conv},
+
+	// Moves
+	sparc64.AMOVB:  {Flags: gc.SizeB | gc.LeftRead | gc.RightWrite | gc.Move | gc.Conv},
+	sparc64.AMOVUB: {Flags: gc.SizeB | gc.LeftRead | gc.RightWrite | gc.Move | gc.Conv},
+	sparc64.AMOVH:  {Flags: gc.SizeW | gc.LeftRead | gc.RightWrite | gc.Move | gc.Conv},
+	sparc64.AMOVUH: {Flags: gc.SizeW | gc.LeftRead | gc.RightWrite | gc.Move | gc.Conv},
+	sparc64.AMOVW:  {Flags: gc.SizeL | gc.LeftRead | gc.RightWrite | gc.Move | gc.Conv},
+	sparc64.AMOVUW: {Flags: gc.SizeL | gc.LeftRead | gc.RightWrite | gc.Move | gc.Conv},
+	sparc64.ASTW:   {Flags: gc.SizeL | gc.LeftRead | gc.RightWrite | gc.Move | gc.Conv},
+	sparc64.AMOVD:  {Flags: gc.SizeQ | gc.LeftRead | gc.RightWrite | gc.Move},
+	sparc64.ASTD:   {Flags: gc.SizeQ | gc.LeftRead | gc.RightWrite | gc.Move},
+	sparc64.AFMOVS: {Flags: gc.SizeF | gc.LeftRead | gc.RightWrite | gc.Move | gc.Conv},
+	sparc64.AFMOVD: {Flags: gc.SizeD | gc.LeftRead | gc.RightWrite | gc.Move},
+
+	// Jumps
+	obj.AJMP:       {Flags: gc.Jump | gc.Break},
+	obj.ACALL:      {Flags: gc.Call},
+	sparc64.ABRNZ:  {Flags: gc.Cjmp},
+	sparc64.ABCCD:  {Flags: gc.Cjmp},
+	sparc64.ABCCW:  {Flags: gc.Cjmp},
+	sparc64.ABCSD:  {Flags: gc.Cjmp},
+	sparc64.ABCSW:  {Flags: gc.Cjmp},
+	sparc64.ABED:   {Flags: gc.Cjmp},
+	sparc64.ABEW:   {Flags: gc.Cjmp},
+	sparc64.ABGD:   {Flags: gc.Cjmp},
+	sparc64.ABGED:  {Flags: gc.Cjmp},
+	sparc64.ABGEW:  {Flags: gc.Cjmp},
+	sparc64.ABGUD:  {Flags: gc.Cjmp},
+	sparc64.ABGUW:  {Flags: gc.Cjmp},
+	sparc64.ABGW:   {Flags: gc.Cjmp},
+	sparc64.ABLD:   {Flags: gc.Cjmp},
+	sparc64.ABLED:  {Flags: gc.Cjmp},
+	sparc64.ABLEUD: {Flags: gc.Cjmp},
+	sparc64.ABLEUW: {Flags: gc.Cjmp},
+	sparc64.ABLEW:  {Flags: gc.Cjmp},
+	sparc64.ABLW:   {Flags: gc.Cjmp},
+	sparc64.ABNED:  {Flags: gc.Cjmp},
+	sparc64.ABNEW:  {Flags: gc.Cjmp},
+	sparc64.AFBE:   {Flags: gc.Cjmp},
+	sparc64.AFBG:   {Flags: gc.Cjmp},
+	sparc64.AFBGE:  {Flags: gc.Cjmp},
+	sparc64.AFBL:   {Flags: gc.Cjmp},
+	sparc64.AFBLE:  {Flags: gc.Cjmp},
+	sparc64.AFBNE:  {Flags: gc.Cjmp},
+	obj.ARET:       {Flags: gc.Break},
+	obj.ADUFFZERO:  {Flags: gc.Call},
+	obj.ADUFFCOPY:  {Flags: gc.Call},
+}
+
+func proginfo(p *obj.Prog) {
+	info := &p.Info
+	*info = progtable[p.As]
+	if info.Flags == 0 {
+		gc.Fatalf("proginfo: unknown instruction %v", p)
+	}
+
+	if (info.Flags&gc.RegRead != 0) && p.Reg == 0 {
+		info.Flags &^= gc.RegRead
+		info.Flags |= gc.RightRead /*CanRegRead |*/
+	}
+
+	if (p.From.Type == obj.TYPE_MEM || p.From.Type == obj.TYPE_ADDR) && p.From.Reg != 0 {
+		info.Regindex |= RtoB(int(p.From.Reg))
+		if p.Scond != 0 {
+			info.Regset |= RtoB(int(p.From.Reg))
+		}
+	}
+
+	if (p.To.Type == obj.TYPE_MEM || p.To.Type == obj.TYPE_ADDR) && p.To.Reg != 0 {
+		info.Regindex |= RtoB(int(p.To.Reg))
+		if p.Scond != 0 {
+			info.Regset |= RtoB(int(p.To.Reg))
+		}
+	}
+
+	if p.From.Type == obj.TYPE_ADDR && p.From.Sym != nil && (info.Flags&gc.LeftRead != 0) {
+		info.Flags &^= gc.LeftRead
+		info.Flags |= gc.LeftAddr
+	}
+
+	if p.As == obj.ADUFFZERO {
+		info.Reguse |= RtoB(sparc64.REG_RT1)
+		info.Regset |= RtoB(sparc64.REG_RT1)
+	}
+
+	if p.As == obj.ADUFFCOPY {
+		// TODO(austin) Revisit when duffcopy is implemented
+		info.Reguse |= RtoB(sparc64.REG_RT1) | RtoB(sparc64.REG_RT2) | RtoB(sparc64.REG_G5)
+
+		info.Regset |= RtoB(sparc64.REG_RT1) | RtoB(sparc64.REG_RT2)
+	}
+}
diff --git a/src/cmd/compile/internal/sparc64/reg.go b/src/cmd/compile/internal/sparc64/reg.go
new file mode 100644
index 0000000..2c3b454
--- /dev/null
+++ b/src/cmd/compile/internal/sparc64/reg.go
@@ -0,0 +1,185 @@
+// Derived from Inferno utils/6c/reg.c
+// http://code.google.com/p/inferno-os/source/browse/utils/6c/reg.c
+//
+//	Copyright © 1994-1999 Lucent Technologies Inc.  All rights reserved.
+//	Portions Copyright © 1995-1997 C H Forsyth (forsyth@terzarima.net)
+//	Portions Copyright © 1997-1999 Vita Nuova Limited
+//	Portions Copyright © 2000-2007 Vita Nuova Holdings Limited (www.vitanuova.com)
+//	Portions Copyright © 2004,2006 Bruce Ellis
+//	Portions Copyright © 2005-2007 C H Forsyth (forsyth@terzarima.net)
+//	Revisions Copyright © 2000-2007 Lucent Technologies Inc. and others
+//	Portions Copyright © 2009 The Go Authors.  All rights reserved.
+//
+// Permission is hereby granted, free of charge, to any person obtaining a copy
+// of this software and associated documentation files (the "Software"), to deal
+// in the Software without restriction, including without limitation the rights
+// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+// copies of the Software, and to permit persons to whom the Software is
+// furnished to do so, subject to the following conditions:
+//
+// The above copyright notice and this permission notice shall be included in
+// all copies or substantial portions of the Software.
+//
+// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL THE
+// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
+// THE SOFTWARE.
+
+package sparc64
+
+import (
+	"cmd/compile/internal/gc"
+	"cmd/internal/obj/sparc64"
+)
+
+const (
+	NREGVAR = 64 /* 32 general + 32 floating */
+)
+
+var regname = []string{
+	".R0",
+	".R1",
+	".R2",
+	".R3",
+	".R4",
+	".R5",
+	".R6",
+	".R7",
+	".R8",
+	".R9",
+	".R10",
+	".R11",
+	".R12",
+	".R13",
+	".R14",
+	".R15",
+	".R16",
+	".R17",
+	".R18",
+	".R19",
+	".R20",
+	".R21",
+	".R22",
+	".R23",
+	".R24",
+	".R25",
+	".R26",
+	".R27",
+	".R28",
+	".R29",
+	".R30",
+	".R31",
+	".F0",
+	".F1",
+	".F2",
+	".F3",
+	".F4",
+	".F5",
+	".F6",
+	".F7",
+	".F8",
+	".F9",
+	".F10",
+	".F11",
+	".F12",
+	".F13",
+	".F14",
+	".F15",
+	".F16",
+	".F17",
+	".F18",
+	".F19",
+	".F20",
+	".F21",
+	".F22",
+	".F23",
+	".F24",
+	".F25",
+	".F26",
+	".F27",
+	".F28",
+	".F29",
+	".F30",
+	".F31",
+}
+
+func regnames(n *int) []string {
+	*n = NREGVAR
+	return regname
+}
+
+func excludedregs() uint64 {
+	// Exclude registers with fixed functions in [R8, REG_MAX] range.
+	regbits := RtoB(sparc64.REG_RSP) | RtoB(sparc64.REG_OLR) | RtoB(sparc64.REG_TMP2)
+
+	// Exclude G0 - G7.
+	for r := sparc64.REG_G0; r <= sparc64.REG_G7; r++ {
+		regbits |= RtoB(r)
+	}
+	// Exclude I6 - I7.
+	for r := sparc64.REG_MAX + 1; r <= sparc64.REG_I7; r++ {
+		regbits |= RtoB(r)
+	}
+
+	// Exclude I0 - I7, for debugging.
+	// TODO(aram): revisit this.
+	for r := sparc64.REG_I0; r <= sparc64.REG_I7; r++ {
+		regbits |= RtoB(r)
+	}
+
+	// Exclude floating point registers with fixed functions
+	regbits |= RtoB(sparc64.REG_YTMP) | RtoB(sparc64.REG_YTWO)
+
+	// Exclude Y16-Y31, since they don't exist.
+	for r := sparc64.REG_Y15 + 1; r <= (sparc64.REG_Y0 + 31); r++ {
+		regbits |= RtoB(r)
+	}
+
+	return regbits
+}
+
+func doregbits(r int) uint64 {
+	return 0
+}
+
+/*
+ * track register variables including external registers:
+ *	bit	reg
+ *	0	R0 (G0)
+ *	1	R1 (G1)
+ *	...	...
+ *	31	R31 (I7)
+ *	32+0	Y0
+ *	32+1	Y1
+ *	...	...
+ *	32+15	Y15
+ *	...	unused
+ */
+func RtoB(r int) uint64 {
+	if r >= sparc64.REG_G0 && r <= sparc64.REG_I7 {
+		return 1 << uint(r-sparc64.REG_G0)
+	}
+	if r >= sparc64.REG_Y0 && r <= sparc64.REG_Y0+31 {
+		return 1 << uint(32+r-sparc64.REG_Y0)
+	}
+	return 0
+}
+
+func BtoR(b uint64) int {
+	b &= 0xffffffff
+	if b == 0 {
+		return 0
+	}
+	return gc.Bitno(b) + sparc64.REG_G0
+}
+
+func BtoF(b uint64) int {
+	b >>= 32
+	if b == 0 {
+		return 0
+	}
+	return gc.Bitno(b) + sparc64.REG_Y0
+}
diff --git a/src/cmd/compile/main.go b/src/cmd/compile/main.go
index 8b8161e..8df5b98 100644
--- a/src/cmd/compile/main.go
+++ b/src/cmd/compile/main.go
@@ -11,6 +11,7 @@ import (
 	"cmd/compile/internal/mips64"
 	"cmd/compile/internal/ppc64"
 	"cmd/compile/internal/s390x"
+	"cmd/compile/internal/sparc64"
 	"cmd/compile/internal/x86"
 	"cmd/internal/obj"
 	"fmt"
@@ -41,5 +42,7 @@ func main() {
 		ppc64.Main()
 	case "s390x":
 		s390x.Main()
+	case "sparc64":
+		sparc64.Main()
 	}
 }
diff --git a/src/cmd/dist/build.go b/src/cmd/dist/build.go
index 9eb9caf..ba23bca 100644
--- a/src/cmd/dist/build.go
+++ b/src/cmd/dist/build.go
@@ -61,6 +61,7 @@ var okgoarch = []string{
 	"ppc64",
 	"ppc64le",
 	"s390x",
+	"sparc64",
 }
 
 // The known operating systems.
@@ -637,6 +638,8 @@ func install(dir string) {
 			pathf("%s/src/runtime/funcdata.h", goroot), 0)
 		copyfile(pathf("%s/pkg/include/asm_ppc64x.h", goroot),
 			pathf("%s/src/runtime/asm_ppc64x.h", goroot), 0)
+		copyfile(pathf("%s/pkg/include/asm_sparc64.h", goroot),
+			pathf("%s/src/runtime/asm_sparc64.h", goroot), 0)
 	}
 
 	// Generate any missing files; regenerate existing ones.
@@ -1118,6 +1121,7 @@ var cgoEnabled = map[string]bool{
 	"plan9/amd64":     false,
 	"plan9/arm":       false,
 	"solaris/amd64":   true,
+	"solaris/sparc64": true,
 	"windows/386":     true,
 	"windows/amd64":   true,
 }
diff --git a/src/cmd/dist/buildtool.go b/src/cmd/dist/buildtool.go
index a535316..0765d0e 100644
--- a/src/cmd/dist/buildtool.go
+++ b/src/cmd/dist/buildtool.go
@@ -36,6 +36,7 @@ var bootstrapDirs = []string{
 	"compile/internal/gc",
 	"compile/internal/mips64",
 	"compile/internal/ppc64",
+	"compile/internal/sparc64",
 	"compile/internal/ssa",
 	"compile/internal/x86",
 	"compile/internal/s390x",
@@ -47,6 +48,7 @@ var bootstrapDirs = []string{
 	"internal/obj/mips",
 	"internal/obj/ppc64",
 	"internal/obj/s390x",
+	"internal/obj/sparc64",
 	"internal/obj/x86",
 	"internal/sys",
 	"link",
@@ -57,6 +59,7 @@ var bootstrapDirs = []string{
 	"link/internal/mips64",
 	"link/internal/ppc64",
 	"link/internal/s390x",
+	"link/internal/sparc64",
 	"link/internal/x86",
 }
 
diff --git a/src/cmd/dist/test.go b/src/cmd/dist/test.go
index e56d108..394587d 100644
--- a/src/cmd/dist/test.go
+++ b/src/cmd/dist/test.go
@@ -151,7 +151,7 @@ func (t *tester) run() {
 	}
 
 	t.timeoutScale = 1
-	if t.goarch == "arm" || t.goos == "windows" {
+	if t.goarch == "arm" || t.goarch == "sparc64" || t.goos == "windows" {
 		t.timeoutScale = 2
 	}
 	if s := os.Getenv("GO_TEST_TIMEOUT_SCALE"); s != "" {
diff --git a/src/cmd/dist/util.go b/src/cmd/dist/util.go
index bbf3b75..23a762f 100644
--- a/src/cmd/dist/util.go
+++ b/src/cmd/dist/util.go
@@ -413,6 +413,9 @@ func main() {
 		if strings.Contains(out, "i386") {
 			gohostarch = "386"
 		}
+		if strings.Contains(out, "sparcv9") {
+			gohostarch = "sparc64"
+		}
 	case "plan9":
 		gohostarch = os.Getenv("objtype")
 		if gohostarch == "" {
diff --git a/src/cmd/dist/util_gccgo_nocgo.go b/src/cmd/dist/util_gccgo_nocgo.go
new file mode 100644
index 0000000..8b23374
--- /dev/null
+++ b/src/cmd/dist/util_gccgo_nocgo.go
@@ -0,0 +1,13 @@
+// Copyright 2015 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// +build gccgo,!cgo
+
+package main
+
+func cansse2() bool { return true }
+
+func useVFPv1() {}
+
+func useVFPv3() {}
diff --git a/src/cmd/go/build.go b/src/cmd/go/build.go
index 3c0b994..fe832c9 100644
--- a/src/cmd/go/build.go
+++ b/src/cmd/go/build.go
@@ -3110,6 +3110,8 @@ func (b *builder) gccArchArgs() []string {
 		return []string{"-m64", "-march=z196"}
 	case "mips64", "mips64le":
 		return []string{"-mabi=64"}
+	case "sparc64":
+		return []string{"-m64"}
 	}
 	return nil
 }
diff --git a/src/cmd/internal/obj/link.go b/src/cmd/internal/obj/link.go
index b6861f4..84813ac 100644
--- a/src/cmd/internal/obj/link.go
+++ b/src/cmd/internal/obj/link.go
@@ -262,7 +262,7 @@ type ProgInfo struct {
 // that are common to all architectures.
 // However, the majority of opcodes are arch-specific
 // and are declared in their respective architecture's subpackage.
-type As int16
+type As int32
 
 // These are the portable opcodes.
 const (
@@ -303,6 +303,7 @@ const (
 	ABaseARM64
 	ABaseMIPS64
 	ABaseS390X
+	ABaseSPARC64
 
 	AMask = 1<<12 - 1 // AND with this to use the opcode as an array index.
 )
@@ -460,6 +461,12 @@ const (
 	// R_ADDROFF resolves to a 32-bit offset from the beginning of the section
 	// holding the data being relocated to the referenced symbol.
 	R_ADDROFF
+	// R_ADDRSPARC64LO (only used on sparc64) resolves to low 32bits of a
+	// 64-bit address, by loading the address into a register with two instructions.
+	R_ADDRSPARC64LO
+	// R_ADDRSPARC64HI (only used on sparc64) resolves to high 32bits of a
+	// 64-bit address, by loading the address into a register with two instructions.
+	R_ADDRSPARC64HI
 	R_SIZE
 	R_CALL
 	R_CALLARM
@@ -469,6 +476,7 @@ const (
 	// R_CALLMIPS (only used on mips64) resolves to non-PC-relative target address
 	// of a CALL (JAL) instruction, by encoding the address into the instruction.
 	R_CALLMIPS
+	R_CALLSPARC64
 	R_CONST
 	R_PCREL
 	// R_TLS_LE, used on 386, amd64, and ARM, resolves to the offset of the
@@ -482,6 +490,10 @@ const (
 	// is not set on intel platforms but is set to a TLS symbol -- runtime.tlsg -- in
 	// the linker when externally linking).
 	R_TLS_IE
+	// R_SPARC64_TLS_LE is used to implement the "local exec" model for tls
+	// access. It resolves to the offset of the thread-local symbol from the
+	// thread pointer (R7) and inserts this value into a pair of instruction words.
+	R_SPARC64_TLS_LE
 	R_GOTOFF
 	R_PLT0
 	R_PLT1
@@ -692,6 +704,9 @@ func (ctxt *Link) FixedFrameSize() int64 {
 		// PIC code on ppc64le requires 32 bytes of stack, and it's easier to
 		// just use that much stack always on ppc64x.
 		return int64(4 * ctxt.Arch.PtrSize)
+	case sys.SPARC64:
+		// requires a minimum of 176 bytes of stack.
+		return int64(16*8 + 6*8)
 	default:
 		return int64(ctxt.Arch.PtrSize)
 	}
diff --git a/src/cmd/internal/obj/sizeof_test.go b/src/cmd/internal/obj/sizeof_test.go
index f7173d3..bb09923 100644
--- a/src/cmd/internal/obj/sizeof_test.go
+++ b/src/cmd/internal/obj/sizeof_test.go
@@ -24,7 +24,7 @@ func TestSizeof(t *testing.T) {
 	}{
 		{Addr{}, 52, 80},
 		{LSym{}, 80, 136},
-		{Prog{}, 196, 288},
+		{Prog{}, 196, 296},
 	}
 
 	for _, tt := range tests {
diff --git a/src/cmd/internal/obj/sparc64/a.out.go b/src/cmd/internal/obj/sparc64/a.out.go
new file mode 100644
index 0000000..a3817a7
--- /dev/null
+++ b/src/cmd/internal/obj/sparc64/a.out.go
@@ -0,0 +1,499 @@
+// Copyright 2015 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package sparc64
+
+import "cmd/internal/obj"
+
+// General purpose registers, kept in the low bits of Prog.Reg.
+const (
+	// integer
+	REG_G0 = obj.RBaseSPARC64 + iota
+	REG_G1
+	REG_G2
+	REG_G3
+	REG_G4
+	REG_G5
+	REG_G6
+	REG_G7
+	REG_O0
+	REG_O1
+	REG_O2
+	REG_O3
+	REG_O4
+	REG_O5
+	REG_O6
+	REG_O7
+	REG_L0
+	REG_L1
+	REG_L2
+	REG_L3
+	REG_L4
+	REG_L5
+	REG_L6
+	REG_L7
+	REG_I0
+	REG_I1
+	REG_I2
+	REG_I3
+	REG_I4
+	REG_I5
+	REG_I6
+	REG_I7
+
+	// single-precision floating point
+	REG_F0
+	REG_F1
+	REG_F2
+	REG_F3
+	REG_F4
+	REG_F5
+	REG_F6
+	REG_F7
+	REG_F8
+	REG_F9
+	REG_F10
+	REG_F11
+	REG_F12
+	REG_F13
+	REG_F14
+	REG_F15
+	REG_F16
+	REG_F17
+	REG_F18
+	REG_F19
+	REG_F20
+	REG_F21
+	REG_F22
+	REG_F23
+	REG_F24
+	REG_F25
+	REG_F26
+	REG_F27
+	REG_F28
+	REG_F29
+	REG_F30
+	REG_F31
+
+	// double-precision floating point; the first half is aliased to
+	// single-precision registers, that is: Dn is aliased to Fn, Fn+1,
+	// where n ≤ 30.
+	REG_D0
+	REG_D32
+	REG_D2
+	REG_D34
+	REG_D4
+	REG_D36
+	REG_D6
+	REG_D38
+	REG_D8
+	REG_D40
+	REG_D10
+	REG_D42
+	REG_D12
+	REG_D44
+	REG_D14
+	REG_D46
+	REG_D16
+	REG_D48
+	REG_D18
+	REG_D50
+	REG_D20
+	REG_D52
+	REG_D22
+	REG_D54
+	REG_D24
+	REG_D56
+	REG_D26
+	REG_D58
+	REG_D28
+	REG_D60
+	REG_D30
+	REG_D62
+
+	// common single/double-precision virtualized registers.
+	// Yn is aliased to F2n, F2n+1, D2n.
+	REG_Y0
+	REG_Y1
+	REG_Y2
+	REG_Y3
+	REG_Y4
+	REG_Y5
+	REG_Y6
+	REG_Y7
+	REG_Y8
+	REG_Y9
+	REG_Y10
+	REG_Y11
+	REG_Y12
+	REG_Y13
+	REG_Y14
+	REG_Y15
+)
+
+const (
+	// floating-point condition-code registers
+	REG_FCC0 = REG_G0 + 256 + iota
+	REG_FCC1
+	REG_FCC2
+	REG_FCC3
+)
+
+const (
+	// integer condition-code flags
+	REG_ICC = REG_G0 + 384
+	REG_XCC = REG_G0 + 384 + 2
+)
+
+const (
+	REG_SPECIAL = REG_G0 + 512
+
+	REG_CCR  = REG_SPECIAL + 2
+	REG_TICK = REG_SPECIAL + 4
+	REG_RPC  = REG_SPECIAL + 5
+
+	REG_BSP = REG_RSP + 256
+	REG_BFP = REG_RFP + 256
+
+	REG_LAST = REG_G0 + 1024
+)
+
+// Register assignments:
+const (
+	REG_ZR   = REG_G0
+	REG_RT1  = REG_G1
+	REG_CTXT = REG_G2
+	REG_G    = REG_G3
+	REG_RT2  = REG_G4
+	REG_TMP  = REG_G5
+	REG_TLS  = REG_G7
+	REG_RSP  = REG_O6
+	REG_OLR  = REG_O7
+	REG_TMP2 = REG_L0
+	REG_RFP  = REG_I6
+	REG_ILR  = REG_I7
+	REG_FTMP = REG_F30
+	REG_DTMP = REG_D30
+	REG_YTMP = REG_Y15
+	REG_YTWO = REG_Y14
+)
+
+const (
+	REG_MIN = REG_G0
+	REG_MAX = REG_I5
+)
+
+const (
+	StackBias             = 0x7ff  // craziness
+	WindowSaveAreaSize    = 16 * 8 // only slots for RFP and PLR used
+	ArgumentsSaveAreaSize = 6 * 8  // unused
+	MinStackFrameSize     = WindowSaveAreaSize + ArgumentsSaveAreaSize
+)
+
+const (
+	BIG = 1<<12 - 1 // magnitude of smallest negative immediate
+)
+
+// Prog.mark
+const (
+	FOLL = 1 << iota
+	LABEL
+	LEAF
+)
+
+const (
+	ClassUnknown = iota
+
+	ClassReg    // R1..R31
+	ClassFReg   // F0..F31
+	ClassDReg   // D0..D62
+	ClassCond   // ICC, XCC
+	ClassFCond  // FCC0..FCC3
+	ClassSpcReg // TICK, CCR, etc
+
+	ClassZero     // $0 or ZR
+	ClassConst5   // unsigned 5-bit constant
+	ClassConst6   // unsigned 6-bit constant
+	ClassConst10  // signed 10-bit constant
+	ClassConst11  // signed 11-bit constant
+	ClassConst13  // signed 13-bit constant
+	ClassConst31_ // signed 32-bit constant, negative
+	ClassConst31  // signed 32-bit constant, positive or zero
+	ClassConst32  // 32-bit constant
+	ClassConst    // 64-bit constant
+	ClassFConst   // floating-point constant
+
+	ClassRegReg     // $(Rn+Rm) or $(Rn)(Rm*1)
+	ClassRegConst13 // $n(R), n is 13-bit signed
+	ClassRegConst   // $n(R), n large
+
+	ClassIndirRegReg // (Rn+Rm) or (Rn)(Rm*1)
+	ClassIndir0      // (R)
+	ClassIndir13     // n(R), n is 13-bit signed
+	ClassIndir       // n(R), n large
+
+	ClassBranch      // n(PC) branch target, n is 21-bit signed, mod 4
+	ClassLargeBranch // n(PC) branch target, n is 32-bit signed, mod 4
+
+	ClassAddr    // $sym(SB)
+	ClassMem     // sym(SB)
+	ClassTLSAddr // $tlssym(SB)
+	ClassTLSMem  // tlssym(SB)
+
+	ClassTextSize
+	ClassNone
+
+	ClassBias = 64 // BFP or BSP present in Addr, bitwise OR with classes above
+)
+
+var cnames = []string{
+	ClassUnknown:     "ClassUnknown",
+	ClassReg:         "ClassReg",
+	ClassFReg:        "ClassFReg",
+	ClassDReg:        "ClassDReg",
+	ClassCond:        "ClassCond",
+	ClassFCond:       "ClassFCond",
+	ClassSpcReg:      "ClassSpcReg",
+	ClassZero:        "ClassZero",
+	ClassConst5:      "ClassConst5",
+	ClassConst6:      "ClassConst6",
+	ClassConst10:     "ClassConst10",
+	ClassConst11:     "ClassConst11",
+	ClassConst13:     "ClassConst13",
+	ClassConst31_:    "ClassConst31-",
+	ClassConst31:     "ClassConst31+",
+	ClassConst32:     "ClassConst32",
+	ClassConst:       "ClassConst",
+	ClassFConst:      "ClassFConst",
+	ClassRegReg:      "ClassRegReg",
+	ClassRegConst13:  "ClassRegConst13",
+	ClassRegConst:    "ClassRegConst",
+	ClassIndirRegReg: "ClassIndirRegReg",
+	ClassIndir0:      "ClassIndir0",
+	ClassIndir13:     "ClassIndir13",
+	ClassIndir:       "ClassIndir",
+	ClassBranch:      "ClassBranch",
+	ClassLargeBranch: "ClassLargeBranch",
+	ClassAddr:        "ClassAddr",
+	ClassMem:         "ClassMem",
+	ClassTLSAddr:     "ClassTLSAddr",
+	ClassTLSMem:      "ClassTLSMem",
+	ClassTextSize:    "ClassTextSize",
+	ClassNone:        "ClassNone",
+	ClassBias:        "ClassBias",
+}
+
+//go:generate go run ../stringer.go -i $GOFILE -o anames.go -p sparc64
+
+const (
+	AADD = obj.ABaseSPARC64 + obj.A_ARCHSPECIFIC + iota
+	AADDCC
+	AADDC
+	AADDCCC
+	AAND
+	AANDCC
+	AANDN
+	AANDNCC
+
+	// These are the two-operand SPARCv9 32-, and 64-bit, branch
+	// on integer condition codes with prediction (BPcc), not the
+	// single-operand SPARCv8 32-bit branch on integer condition
+	// codes (Bicc).
+	ABN
+	ABNE
+	ABE
+	ABG
+	ABLE
+	ABGE
+	ABL
+	ABGU
+	ABLEU
+	ABCC
+	ABCS
+	ABPOS
+	ABNEG
+	ABVC
+	ABVS
+
+	ABRZ
+	ABRLEZ
+	ABRLZ
+	ABRNZ
+	ABRGZ
+	ABRGEZ
+	ACASW
+	ACASD
+	AFABSS
+	AFABSD
+	AFADDS
+	AFADDD
+	AFBA
+	AFBN
+	AFBU
+	AFBG
+	AFBUG
+	AFBL
+	AFBUL
+	AFBLG
+	AFBNE
+	AFBE
+	AFBUE
+	AFBGE
+	AFBUGE
+	AFBLE
+	AFBULE
+	AFBO
+	AFCMPS
+	AFCMPD
+	AFDIVS
+	AFDIVD
+	AFITOS
+	AFITOD
+	AFLUSH
+	AFLUSHW
+	AFMOVS // the SPARC64 instruction, and alias for loads and stores
+	AFMOVD // the SPARC64 instruction, and alias for loads and stores
+	AFMULS
+	AFMULD
+	AFSMULD
+	AFNEGS
+	AFNEGD
+	AFSQRTS
+	AFSQRTD
+	AFSTOX
+	AFDTOX
+	AFSTOI
+	AFDTOI
+	AFSTOD
+	AFDTOS
+	AFSUBS
+	AFSUBD
+	AFXTOS
+	AFXTOD
+	AJMPL
+	ALDSB
+	ALDSH
+	ALDSW
+	ALDUB
+	ALDD
+	ALDDF
+	ALDSF
+	ALDUH
+	ALDUW
+	AMEMBAR
+	AMOVA
+	AMOVCC
+	AMOVCS
+	AMOVE
+	AMOVG
+	AMOVGE
+	AMOVGU
+	AMOVL
+	AMOVLE
+	AMOVLEU
+	AMOVN
+	AMOVNE
+	AMOVNEG
+	AMOVPOS
+	AMOVRGEZ
+	AMOVRGZ
+	AMOVRLEZ
+	AMOVRLZ
+	AMOVRNZ
+	AMOVRZ
+	AMOVVC
+	AMOVVS
+	AMULD
+	AOR
+	AORCC
+	AORN
+	AORNCC
+	ARD
+	ARESTORE // not used under normal circumstances
+	ASAVE    // not used under normal circumstances
+	ASDIVD
+	ASETHI
+	AUDIVD
+	ASLLW
+	ASRLW
+	ASRAW
+	ASLLD
+	ASRLD
+	ASRAD
+	ASTB
+	ASTH
+	ASTW
+	ASTD
+	ASTSF
+	ASTDF
+	ASUB
+	ASUBCC
+	ASUBC
+	ASUBCCC
+	ATA
+	AXOR
+	AXORCC
+	AXNOR
+	AXNORCC
+
+	// Pseudo-instructions, aliases to SPARC64 instructions and
+	// synthetic instructions.
+	ACMP // SUBCC R1, R2, ZR
+	ANEG
+	AMOVUB
+	AMOVB
+	AMOVUH
+	AMOVH
+	AMOVUW
+	AMOVW
+	AMOVD // also the SPARC64 synthetic instruction
+	ARNOP // SETHI $0, ZR
+
+	// These are aliases to two-operand SPARCv9 32-, and 64-bit,
+	// branch on integer condition codes with prediction (BPcc),
+	// with ICC implied.
+	ABNW
+	ABNEW
+	ABEW
+	ABGW
+	ABLEW
+	ABGEW
+	ABLW
+	ABGUW
+	ABLEUW
+	ABCCW
+	ABCSW
+	ABPOSW
+	ABNEGW
+	ABVCW
+	ABVSW
+
+	// These are aliases to two-operand SPARCv9 32-, and 64-bit,
+	// branch on integer condition codes with prediction (BPcc),
+	// with XCC implied.
+	ABND
+	ABNED
+	ABED
+	ABGD
+	ABLED
+	ABGED
+	ABLD
+	ABGUD
+	ABLEUD
+	ABCCD
+	ABCSD
+	ABPOSD
+	ABNEGD
+	ABVCD
+	ABVSD
+
+	AWORD
+	ADWORD
+
+	// JMPL $8(ILR), ZR
+	//      RESTORE $0, ZR, ZR
+	ARETRESTORE
+
+	ALAST
+)
diff --git a/src/cmd/internal/obj/sparc64/anames.go b/src/cmd/internal/obj/sparc64/anames.go
new file mode 100644
index 0000000..f10947b
--- /dev/null
+++ b/src/cmd/internal/obj/sparc64/anames.go
@@ -0,0 +1,196 @@
+// Generated by stringer -i a.out.go -o anames.go -p sparc64
+// Do not edit.
+
+package sparc64
+
+import "cmd/internal/obj"
+
+var Anames = []string{
+	obj.A_ARCHSPECIFIC: "ADD",
+	"ADDCC",
+	"ADDC",
+	"ADDCCC",
+	"AND",
+	"ANDCC",
+	"ANDN",
+	"ANDNCC",
+	"BN",
+	"BNE",
+	"BE",
+	"BG",
+	"BLE",
+	"BGE",
+	"BL",
+	"BGU",
+	"BLEU",
+	"BCC",
+	"BCS",
+	"BPOS",
+	"BNEG",
+	"BVC",
+	"BVS",
+	"BRZ",
+	"BRLEZ",
+	"BRLZ",
+	"BRNZ",
+	"BRGZ",
+	"BRGEZ",
+	"CASW",
+	"CASD",
+	"FABSS",
+	"FABSD",
+	"FADDS",
+	"FADDD",
+	"FBA",
+	"FBN",
+	"FBU",
+	"FBG",
+	"FBUG",
+	"FBL",
+	"FBUL",
+	"FBLG",
+	"FBNE",
+	"FBE",
+	"FBUE",
+	"FBGE",
+	"FBUGE",
+	"FBLE",
+	"FBULE",
+	"FBO",
+	"FCMPS",
+	"FCMPD",
+	"FDIVS",
+	"FDIVD",
+	"FITOS",
+	"FITOD",
+	"FLUSH",
+	"FLUSHW",
+	"FMOVS",
+	"FMOVD",
+	"FMULS",
+	"FMULD",
+	"FSMULD",
+	"FNEGS",
+	"FNEGD",
+	"FSQRTS",
+	"FSQRTD",
+	"FSTOX",
+	"FDTOX",
+	"FSTOI",
+	"FDTOI",
+	"FSTOD",
+	"FDTOS",
+	"FSUBS",
+	"FSUBD",
+	"FXTOS",
+	"FXTOD",
+	"JMPL",
+	"LDSB",
+	"LDSH",
+	"LDSW",
+	"LDUB",
+	"LDD",
+	"LDDF",
+	"LDSF",
+	"LDUH",
+	"LDUW",
+	"MEMBAR",
+	"MOVA",
+	"MOVCC",
+	"MOVCS",
+	"MOVE",
+	"MOVG",
+	"MOVGE",
+	"MOVGU",
+	"MOVL",
+	"MOVLE",
+	"MOVLEU",
+	"MOVN",
+	"MOVNE",
+	"MOVNEG",
+	"MOVPOS",
+	"MOVRGEZ",
+	"MOVRGZ",
+	"MOVRLEZ",
+	"MOVRLZ",
+	"MOVRNZ",
+	"MOVRZ",
+	"MOVVC",
+	"MOVVS",
+	"MULD",
+	"OR",
+	"ORCC",
+	"ORN",
+	"ORNCC",
+	"RD",
+	"RESTORE",
+	"SAVE",
+	"SDIVD",
+	"SETHI",
+	"UDIVD",
+	"SLLW",
+	"SRLW",
+	"SRAW",
+	"SLLD",
+	"SRLD",
+	"SRAD",
+	"STB",
+	"STH",
+	"STW",
+	"STD",
+	"STSF",
+	"STDF",
+	"SUB",
+	"SUBCC",
+	"SUBC",
+	"SUBCCC",
+	"TA",
+	"XOR",
+	"XORCC",
+	"XNOR",
+	"XNORCC",
+	"CMP",
+	"NEG",
+	"MOVUB",
+	"MOVB",
+	"MOVUH",
+	"MOVH",
+	"MOVUW",
+	"MOVW",
+	"MOVD",
+	"RNOP",
+	"BNW",
+	"BNEW",
+	"BEW",
+	"BGW",
+	"BLEW",
+	"BGEW",
+	"BLW",
+	"BGUW",
+	"BLEUW",
+	"BCCW",
+	"BCSW",
+	"BPOSW",
+	"BNEGW",
+	"BVCW",
+	"BVSW",
+	"BND",
+	"BNED",
+	"BED",
+	"BGD",
+	"BLED",
+	"BGED",
+	"BLD",
+	"BGUD",
+	"BLEUD",
+	"BCCD",
+	"BCSD",
+	"BPOSD",
+	"BNEGD",
+	"BVCD",
+	"BVSD",
+	"WORD",
+	"DWORD",
+	"RETRESTORE",
+	"LAST",
+}
diff --git a/src/cmd/internal/obj/sparc64/asm.go b/src/cmd/internal/obj/sparc64/asm.go
new file mode 100644
index 0000000..b76b87d
--- /dev/null
+++ b/src/cmd/internal/obj/sparc64/asm.go
@@ -0,0 +1,1842 @@
+// Copyright 2015 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package sparc64
+
+import (
+	"cmd/internal/obj"
+	"errors"
+	"fmt"
+	"sort"
+)
+
+type Optab struct {
+	as obj.As // instruction
+	a1 int8   // from
+	a2 int8   // reg
+	a3 int8   // from3
+	a4 int8   // to
+}
+
+type OptabSlice []Optab
+
+func (tab OptabSlice) Len() int { return len(tab) }
+
+func (tab OptabSlice) Swap(i, j int) { tab[i], tab[j] = tab[j], tab[i] }
+
+func (tab OptabSlice) Less(i, j int) bool {
+	return ocmp(tab[i], tab[j])
+}
+
+func ocmp(o1, o2 Optab) bool {
+	if o1.as != o2.as {
+		return o1.as < o2.as
+	}
+	if o1.a1 != o2.a1 {
+		return o1.a1 < o2.a1
+	}
+	if o1.a2 != o2.a2 {
+		return o1.a2 < o2.a2
+	}
+	if o1.a3 != o2.a3 {
+		return o1.a3 < o2.a3
+	}
+	return o1.a4 < o2.a4
+}
+
+type Opval struct {
+	op     int8 // selects case in asmout switch
+	size   int8 // *not* including delay-slot
+	OpInfo      // information about the instruction
+}
+
+type OpInfo int8
+
+const (
+	ClobberTMP OpInfo = 1 << iota
+)
+
+var optab = map[Optab]Opval{
+	Optab{obj.ATEXT, ClassAddr, ClassNone, ClassNone, ClassTextSize}: {0, 0, 0},
+	Optab{obj.AFUNCDATA, ClassConst, ClassNone, ClassNone, ClassMem}: {0, 0, 0},
+	Optab{obj.APCDATA, ClassConst, ClassNone, ClassNone, ClassConst}: {0, 0, 0},
+
+	Optab{AADD, ClassReg, ClassNone, ClassNone, ClassReg}:  {1, 4, 0},
+	Optab{AAND, ClassReg, ClassNone, ClassNone, ClassReg}:  {1, 4, 0},
+	Optab{AMULD, ClassReg, ClassNone, ClassNone, ClassReg}: {1, 4, 0},
+	Optab{ASLLD, ClassReg, ClassNone, ClassNone, ClassReg}: {1, 4, 0},
+	Optab{ASLLW, ClassReg, ClassNone, ClassNone, ClassReg}: {1, 4, 0},
+	Optab{AADD, ClassReg, ClassReg, ClassNone, ClassReg}:   {1, 4, 0},
+	Optab{AAND, ClassReg, ClassReg, ClassNone, ClassReg}:   {1, 4, 0},
+	Optab{AMULD, ClassReg, ClassReg, ClassNone, ClassReg}:  {1, 4, 0},
+	Optab{ASLLD, ClassReg, ClassReg, ClassNone, ClassReg}:  {1, 4, 0},
+	Optab{ASLLW, ClassReg, ClassReg, ClassNone, ClassReg}:  {1, 4, 0},
+
+	Optab{ASAVE, ClassReg, ClassReg, ClassNone, ClassReg}: {1, 4, 0},
+	Optab{ASAVE, ClassReg, ClassReg | ClassBias, ClassNone, ClassReg | ClassBias}: {1, 4, 0},
+
+	Optab{AFADDD, ClassDReg, ClassNone, ClassNone, ClassDReg}:  {1, 4, 0},
+	Optab{AFADDD, ClassDReg, ClassDReg, ClassNone, ClassDReg}:  {1, 4, 0},
+	Optab{AFSMULD, ClassFReg, ClassFReg, ClassNone, ClassDReg}: {1, 4, 0},
+
+	Optab{AMOVD, ClassReg, ClassNone, ClassNone, ClassReg}: {2, 4, 0},
+
+	Optab{AADD, ClassConst13, ClassNone, ClassNone, ClassReg}:  {3, 4, 0},
+	Optab{AAND, ClassConst13, ClassNone, ClassNone, ClassReg}:  {3, 4, 0},
+	Optab{AMULD, ClassConst13, ClassNone, ClassNone, ClassReg}: {3, 4, 0},
+	Optab{ASLLD, ClassConst6, ClassNone, ClassNone, ClassReg}:  {3, 4, 0},
+	Optab{ASLLW, ClassConst5, ClassNone, ClassNone, ClassReg}:  {3, 4, 0},
+	Optab{AADD, ClassConst13, ClassReg, ClassNone, ClassReg}:   {3, 4, 0},
+	Optab{AAND, ClassConst13, ClassReg, ClassNone, ClassReg}:   {3, 4, 0},
+	Optab{AMULD, ClassConst13, ClassReg, ClassNone, ClassReg}:  {3, 4, 0},
+	Optab{ASLLD, ClassConst6, ClassReg, ClassNone, ClassReg}:   {3, 4, 0},
+	Optab{ASLLW, ClassConst5, ClassReg, ClassNone, ClassReg}:   {3, 4, 0},
+
+	Optab{ASAVE, ClassConst13, ClassReg, ClassNone, ClassReg}: {3, 4, 0},
+	Optab{ASAVE, ClassConst13, ClassReg | ClassBias, ClassNone, ClassReg | ClassBias}: {3, 4, 0},
+
+	Optab{AMOVD, ClassConst13, ClassNone, ClassNone, ClassReg}: {4, 4, 0},
+	Optab{AMOVW, ClassConst13, ClassNone, ClassNone, ClassReg}: {4, 4, 0},
+
+	Optab{ALDD, ClassIndirRegReg, ClassNone, ClassNone, ClassReg}:   {5, 4, 0},
+	Optab{ASTD, ClassReg, ClassNone, ClassNone, ClassIndirRegReg}:   {6, 4, 0},
+	Optab{ALDDF, ClassIndirRegReg, ClassNone, ClassNone, ClassDReg}: {5, 4, 0},
+	Optab{ASTDF, ClassDReg, ClassNone, ClassNone, ClassIndirRegReg}: {6, 4, 0},
+
+	Optab{ALDD, ClassIndir13, ClassNone, ClassNone, ClassReg}:   {7, 4, 0},
+	Optab{ASTD, ClassReg, ClassNone, ClassNone, ClassIndir13}:   {8, 4, 0},
+	Optab{ALDDF, ClassIndir13, ClassNone, ClassNone, ClassDReg}: {7, 4, 0},
+	Optab{ASTDF, ClassDReg, ClassNone, ClassNone, ClassIndir13}: {8, 4, 0},
+
+	Optab{ARD, ClassSpcReg, ClassNone, ClassNone, ClassReg}: {9, 4, 0},
+
+	Optab{ACASD, ClassIndir0, ClassReg, ClassNone, ClassReg}: {10, 4, 0},
+
+	Optab{AFSTOD, ClassFReg, ClassNone, ClassNone, ClassDReg}: {11, 4, 0},
+	Optab{AFDTOS, ClassDReg, ClassNone, ClassNone, ClassFReg}: {11, 4, 0},
+
+	Optab{AFMOVD, ClassDReg, ClassNone, ClassNone, ClassDReg}: {11, 4, 0},
+
+	Optab{AFXTOD, ClassDReg, ClassNone, ClassNone, ClassDReg}: {11, 4, 0},
+	Optab{AFITOD, ClassFReg, ClassNone, ClassNone, ClassDReg}: {11, 4, 0},
+	Optab{AFXTOS, ClassDReg, ClassNone, ClassNone, ClassFReg}: {11, 4, 0},
+	Optab{AFITOS, ClassFReg, ClassNone, ClassNone, ClassFReg}: {11, 4, 0},
+
+	Optab{AFSTOX, ClassFReg, ClassNone, ClassNone, ClassDReg}: {11, 4, 0},
+	Optab{AFDTOX, ClassDReg, ClassNone, ClassNone, ClassDReg}: {11, 4, 0},
+	Optab{AFDTOI, ClassDReg, ClassNone, ClassNone, ClassFReg}: {11, 4, 0},
+	Optab{AFSTOI, ClassFReg, ClassNone, ClassNone, ClassFReg}: {11, 4, 0},
+
+	Optab{AFABSD, ClassDReg, ClassNone, ClassNone, ClassDReg}: {11, 4, 0},
+
+	Optab{ASETHI, ClassConst32, ClassNone, ClassNone, ClassReg}: {12, 4, 0},
+	Optab{ARNOP, ClassNone, ClassNone, ClassNone, ClassNone}:    {12, 4, 0},
+	Optab{AFLUSHW, ClassNone, ClassNone, ClassNone, ClassNone}:  {12, 4, 0},
+
+	Optab{AMEMBAR, ClassConst, ClassNone, ClassNone, ClassNone}: {13, 4, 0},
+
+	Optab{AFCMPD, ClassDReg, ClassDReg, ClassNone, ClassFCond}: {14, 4, 0},
+	Optab{AFCMPD, ClassDReg, ClassDReg, ClassNone, ClassNone}:  {14, 4, 0},
+
+	Optab{AMOVD, ClassConst32, ClassNone, ClassNone, ClassReg}:  {15, 8, 0},
+	Optab{AMOVD, ClassConst31_, ClassNone, ClassNone, ClassReg}: {16, 8, 0},
+
+	Optab{obj.AJMP, ClassNone, ClassNone, ClassNone, ClassBranch}: {17, 4, 0},
+	Optab{ABN, ClassCond, ClassNone, ClassNone, ClassBranch}:      {17, 4, 0},
+	Optab{ABNW, ClassNone, ClassNone, ClassNone, ClassBranch}:     {17, 4, 0},
+	Optab{ABRZ, ClassReg, ClassNone, ClassNone, ClassBranch}:      {18, 4, 0},
+	Optab{AFBA, ClassNone, ClassNone, ClassNone, ClassBranch}:     {19, 4, 0},
+
+	Optab{AJMPL, ClassReg, ClassNone, ClassNone, ClassReg}:        {20, 4, 0},
+	Optab{AJMPL, ClassRegConst13, ClassNone, ClassNone, ClassReg}: {20, 4, 0},
+	Optab{AJMPL, ClassRegReg, ClassNone, ClassNone, ClassReg}:     {21, 4, 0},
+
+	Optab{obj.ACALL, ClassNone, ClassNone, ClassNone, ClassMem}:     {22, 4, 0},
+	Optab{obj.ADUFFZERO, ClassNone, ClassNone, ClassNone, ClassMem}: {22, 4, 0},
+	Optab{obj.ADUFFCOPY, ClassNone, ClassNone, ClassNone, ClassMem}: {22, 4, 0},
+
+	Optab{AMOVD, ClassAddr, ClassNone, ClassNone, ClassReg}: {23, 24, ClobberTMP},
+
+	Optab{ALDD, ClassMem, ClassNone, ClassNone, ClassReg}:   {24, 28, ClobberTMP},
+	Optab{ALDDF, ClassMem, ClassNone, ClassNone, ClassDReg}: {24, 28, ClobberTMP},
+	Optab{ASTD, ClassReg, ClassNone, ClassNone, ClassMem}:   {25, 28, ClobberTMP},
+	Optab{ASTDF, ClassDReg, ClassNone, ClassNone, ClassMem}: {25, 28, ClobberTMP},
+
+	Optab{obj.ARET, ClassNone, ClassNone, ClassNone, ClassNone}: {26, 4, 0},
+
+	Optab{ATA, ClassConst13, ClassNone, ClassNone, ClassNone}: {27, 4, 0},
+
+	Optab{AMOVD, ClassRegConst13, ClassNone, ClassNone, ClassReg}: {28, 4, 0},
+
+	Optab{AMOVUB, ClassReg, ClassNone, ClassNone, ClassReg}: {29, 4, 0},
+	Optab{AMOVUH, ClassReg, ClassNone, ClassNone, ClassReg}: {30, 8, 0},
+	Optab{AMOVUW, ClassReg, ClassNone, ClassNone, ClassReg}: {31, 4, 0},
+
+	Optab{AMOVB, ClassReg, ClassNone, ClassNone, ClassReg}: {32, 8, 0},
+	Optab{AMOVH, ClassReg, ClassNone, ClassNone, ClassReg}: {33, 8, 0},
+	Optab{AMOVW, ClassReg, ClassNone, ClassNone, ClassReg}: {34, 4, 0},
+
+	Optab{ANEG, ClassReg, ClassNone, ClassNone, ClassReg}: {35, 4, 0},
+
+	Optab{ACMP, ClassReg, ClassReg, ClassNone, ClassNone}:     {36, 4, 0},
+	Optab{ACMP, ClassConst13, ClassReg, ClassNone, ClassNone}: {37, 4, 0},
+
+	Optab{ABND, ClassNone, ClassNone, ClassNone, ClassBranch}: {38, 4, 0},
+
+	Optab{obj.AUNDEF, ClassNone, ClassNone, ClassNone, ClassNone}: {39, 4, 0},
+
+	Optab{obj.ACALL, ClassNone, ClassNone, ClassNone, ClassReg}:    {40, 4, 0},
+	Optab{obj.ACALL, ClassReg, ClassNone, ClassNone, ClassReg}:     {40, 4, 0},
+	Optab{obj.ACALL, ClassNone, ClassNone, ClassNone, ClassIndir0}: {40, 4, 0},
+	Optab{obj.ACALL, ClassReg, ClassNone, ClassNone, ClassIndir0}:  {40, 4, 0},
+
+	Optab{AADD, ClassConst32, ClassNone, ClassNone, ClassReg}: {41, 12, ClobberTMP},
+	Optab{AAND, ClassConst32, ClassNone, ClassNone, ClassReg}: {41, 12, ClobberTMP},
+	Optab{AADD, ClassConst32, ClassReg, ClassNone, ClassReg}:  {41, 12, ClobberTMP},
+	Optab{AAND, ClassConst32, ClassReg, ClassNone, ClassReg}:  {41, 12, ClobberTMP},
+
+	Optab{AMOVD, ClassRegConst, ClassNone, ClassNone, ClassReg}: {42, 12, ClobberTMP},
+
+	Optab{ASTD, ClassReg, ClassNone, ClassNone, ClassIndir}:   {43, 12, ClobberTMP},
+	Optab{ASTDF, ClassDReg, ClassNone, ClassNone, ClassIndir}: {43, 12, ClobberTMP},
+	Optab{ALDD, ClassIndir, ClassNone, ClassNone, ClassReg}:   {44, 12, ClobberTMP},
+	Optab{ALDDF, ClassIndir, ClassNone, ClassNone, ClassDReg}: {44, 12, ClobberTMP},
+
+	Optab{obj.AJMP, ClassNone, ClassNone, ClassNone, ClassMem}: {45, 28, ClobberTMP},
+
+	Optab{AMOVA, ClassCond, ClassNone, ClassConst11, ClassReg}: {46, 4, 0},
+	Optab{AMOVA, ClassCond, ClassReg, ClassNone, ClassReg}:     {47, 4, 0},
+
+	Optab{AMOVRZ, ClassReg, ClassNone, ClassConst10, ClassReg}: {48, 4, 0},
+	Optab{AMOVRZ, ClassReg, ClassReg, ClassNone, ClassReg}:     {49, 4, 0},
+
+	Optab{AMOVD, ClassTLSAddr, ClassNone, ClassNone, ClassReg}: {50, 12, 0},
+
+	Optab{ARETRESTORE, ClassNone, ClassNone, ClassNone, ClassNone}: {51, 12, 0},
+
+	Optab{obj.AJMP, ClassNone, ClassNone, ClassNone, ClassLargeBranch}: {52, 28, ClobberTMP},
+	Optab{ABN, ClassCond, ClassNone, ClassNone, ClassLargeBranch}:  {53, 48, ClobberTMP},
+	Optab{ABNW, ClassNone, ClassNone, ClassNone, ClassLargeBranch}: {53, 48, ClobberTMP},
+	Optab{ABRZ, ClassReg, ClassNone, ClassNone, ClassLargeBranch}:  {54, 48, ClobberTMP},
+	Optab{AFBA, ClassNone, ClassNone, ClassNone, ClassLargeBranch}: {55, 48, ClobberTMP},
+	Optab{ABND, ClassNone, ClassNone, ClassNone, ClassLargeBranch}: {56, 48, ClobberTMP},
+
+	Optab{obj.ACALL, ClassNone, ClassNone, ClassNone, ClassBranch}:      {57, 4, 0},
+	Optab{obj.ACALL, ClassNone, ClassNone, ClassNone, ClassLargeBranch}: {57, 4, 0},
+}
+
+// Compatible classes, if something accepts a $hugeconst, it
+// can also accept $smallconst, $0 and ZR. Something that accepts a
+// register, can also accept $0, etc.
+var cc = map[int8][]int8{
+	ClassReg:         {ClassZero},
+	ClassConst6:      {ClassConst5, ClassZero},
+	ClassConst10:     {ClassConst6, ClassConst5, ClassZero},
+	ClassConst11:     {ClassConst10, ClassConst6, ClassConst5, ClassZero},
+	ClassConst13:     {ClassConst11, ClassConst10, ClassConst6, ClassConst5, ClassZero},
+	ClassConst31:     {ClassConst6, ClassConst5, ClassZero},
+	ClassConst32:     {ClassConst31_, ClassConst31, ClassConst13, ClassConst11, ClassConst10, ClassConst6, ClassConst5, ClassZero},
+	ClassConst:       {ClassConst32, ClassConst31_, ClassConst31, ClassConst13, ClassConst11, ClassConst10, ClassConst6, ClassConst5, ClassZero},
+	ClassRegConst:    {ClassRegConst13},
+	ClassIndir13:     {ClassIndir0},
+	ClassIndir:       {ClassIndir13, ClassIndir0},
+	ClassLargeBranch: {ClassBranch},
+}
+
+func isAddrCompatible(ctxt *obj.Link, a *obj.Addr, class int8) bool {
+	cls := aclass(ctxt, a)
+	cls &= ^ClassBias
+	if cls == class {
+		return true
+	}
+	for _, v := range cc[class] {
+		if cls == v {
+			return true
+		}
+	}
+	return false
+}
+
+var isInstDouble = map[obj.As]bool{
+	AFADDD:  true,
+	AFSUBD:  true,
+	AFABSD:  true,
+	AFCMPD:  true,
+	AFDIVD:  true,
+	AFMOVD:  true,
+	AFMULD:  true,
+	AFNEGD:  true,
+	AFSQRTD: true,
+	ALDDF:   true,
+	ASTDF:   true,
+}
+
+var isInstFloat = map[obj.As]bool{
+	AFADDS:  true,
+	AFSUBS:  true,
+	AFABSS:  true,
+	AFCMPS:  true,
+	AFDIVS:  true,
+	AFMOVS:  true,
+	AFMULS:  true,
+	AFNEGS:  true,
+	AFSQRTS: true,
+	ALDSF:   true,
+	ASTSF:   true,
+}
+
+var isSrcDouble = map[obj.As]bool{
+	AFXTOD: true,
+	AFXTOS: true,
+	AFDTOX: true,
+	AFDTOI: true,
+	AFDTOS: true,
+}
+
+var isSrcFloat = map[obj.As]bool{
+	AFITOD: true,
+	AFITOS: true,
+	AFSTOX: true,
+	AFSTOI: true,
+	AFSTOD: true,
+}
+
+var isDstDouble = map[obj.As]bool{
+	AFXTOD: true,
+	AFITOD: true,
+	AFSTOX: true,
+	AFDTOX: true,
+	AFSTOD: true,
+}
+
+var isDstFloat = map[obj.As]bool{
+	AFXTOS: true,
+	AFITOS: true,
+	AFDTOI: true,
+	AFSTOI: true,
+	AFDTOS: true,
+}
+
+// Compatible instructions, if an asm* function accepts AADD,
+// it accepts ASUBCCC too.
+var ci = map[obj.As][]obj.As{
+	AADD:   {AADDCC, AADDC, AADDCCC, ASUB, ASUBCC, ASUBC, ASUBCCC},
+	AAND:   {AANDCC, AANDN, AANDNCC, AOR, AORCC, AORN, AORNCC, AXOR, AXORCC, AXNOR, AXNORCC},
+	ABN:    {ABNE, ABE, ABG, ABLE, ABGE, ABL, ABGU, ABLEU, ABCC, ABCS, ABPOS, ABNEG, ABVC, ABVS},
+	ABNW:   {ABNEW, ABEW, ABGW, ABLEW, ABGEW, ABLW, ABGUW, ABLEUW, ABCCW, ABCSW, ABPOSW, ABNEGW, ABVCW, ABVSW},
+	ABND:   {ABNED, ABED, ABGD, ABLED, ABGED, ABLD, ABGUD, ABLEUD, ABCCD, ABCSD, ABPOSD, ABNEGD, ABVCD, ABVSD},
+	ABRZ:   {ABRLEZ, ABRLZ, ABRNZ, ABRGZ, ABRGEZ},
+	ACASD:  {ACASW},
+	AFABSD: {AFABSS, AFNEGD, AFNEGS, AFSQRTD, AFNEGS},
+	AFADDD: {AFADDS, AFSUBS, AFSUBD, AFMULD, AFMULS, AFSMULD, AFDIVD, AFDIVS},
+	AFBA:   {AFBN, AFBU, AFBG, AFBUG, AFBL, AFBUL, AFBLG, AFBNE, AFBE, AFBUE, AFBGE, AFBUGE, AFBLE, AFBULE, AFBO},
+	AFCMPD: {AFCMPS},
+	AFITOD: {AFITOS},
+	AFMOVD: {AFMOVS},
+	AFSTOD: {AFDTOS},
+	AFXTOD: {AFXTOS},
+	ALDD:   {ALDSB, ALDSH, ALDSW, ALDUB, ALDUH, ALDUW, AMOVB, AMOVH, AMOVW, AMOVUB, AMOVUH, AMOVUW, AMOVD},
+	ALDDF:  {ALDSF, AFMOVD, AFMOVS},
+	AMOVA:  {AMOVN, AMOVNE, AMOVE, AMOVG, AMOVLE, AMOVGE, AMOVL, AMOVGU, AMOVLEU, AMOVCC, AMOVCS, AMOVPOS, AMOVNEG, AMOVVC, AMOVVS},
+	AMOVRZ: {AMOVRLEZ, AMOVRLZ, AMOVRNZ, AMOVRGZ, AMOVRGEZ},
+	AMULD:  {ASDIVD, AUDIVD},
+	ARD:    {AMOVD},
+	ASLLD:  {ASRLD, ASRAD},
+	ASLLW:  {ASLLW, ASRLW, ASRAW},
+	ASTD:   {ASTB, ASTH, ASTW, AMOVB, AMOVH, AMOVW, AMOVUB, AMOVUH, AMOVUW, AMOVD},
+	ASTDF:  {ASTSF, AFMOVD, AFMOVS},
+	ASAVE:  {ARESTORE},
+}
+
+func opkeys() OptabSlice {
+	keys := make(OptabSlice, 0, len(optab))
+	// create sorted map index by keys
+	for k := range optab {
+		keys = append(keys, k)
+	}
+	sort.Sort(keys)
+	return keys
+}
+
+func init() {
+	// For each line in optab, duplicate it so that we'll also
+	// have a line that will accept compatible instructions, but
+	// only if there isn't an already existent line with the same
+	// key. Also change operand type, if the instruction is a double.
+	for _, o := range opkeys() {
+		for _, c := range ci[o.as] {
+			do := o
+			do.as = c
+			if isInstDouble[o.as] && isInstFloat[do.as] {
+				if do.a1 == ClassDReg {
+					do.a1 = ClassFReg
+				}
+				if do.a2 == ClassDReg {
+					do.a2 = ClassFReg
+				}
+				if do.a3 == ClassDReg {
+					do.a3 = ClassFReg
+				}
+				if do.a4 == ClassDReg {
+					do.a4 = ClassFReg
+				}
+			}
+			_, ok := optab[do]
+			if !ok {
+				optab[do] = optab[o]
+			}
+		}
+	}
+	// For each line in optab that accepts a large-class operand,
+	// duplicate it so that we'll also have a line that accepts a
+	// small-class operand, but do it only if there isn't an already
+	// existent line with the same key.
+	for _, o := range opkeys() {
+		for _, c := range cc[o.a1] {
+			do := o
+			do.a1 = c
+			_, ok := optab[do]
+			if !ok {
+				optab[do] = optab[o]
+			}
+		}
+	}
+	for _, o := range opkeys() {
+		for _, c := range cc[o.a2] {
+			do := o
+			do.a2 = c
+			_, ok := optab[do]
+			if !ok {
+				optab[do] = optab[o]
+			}
+		}
+	}
+	for _, o := range opkeys() {
+		for _, c := range cc[o.a3] {
+			do := o
+			do.a3 = c
+			_, ok := optab[do]
+			if !ok {
+				optab[do] = optab[o]
+			}
+		}
+	}
+	for _, o := range opkeys() {
+		for _, c := range cc[o.a4] {
+			do := o
+			do.a4 = c
+			_, ok := optab[do]
+			if !ok {
+				optab[do] = optab[o]
+			}
+		}
+	}
+}
+
+func oplook(p *obj.Prog) (Opval, error) {
+	var a2, a3 int8 = ClassNone, ClassNone
+	if p.Reg != 0 {
+		a2 = rclass(p.Reg)
+	}
+	var type3 obj.AddrType
+	if p.From3 != nil {
+		a3 = p.From3.Class
+		type3 = p.From3.Type
+	}
+	o := Optab{as: p.As, a1: p.From.Class, a2: a2, a3: a3, a4: p.To.Class}
+	v, ok := optab[o]
+	if !ok {
+		return Opval{}, fmt.Errorf("illegal combination %v %v %v %v %v, %d %d %d %d", p, DRconv(o.a1), DRconv(o.a2), DRconv(o.a3), DRconv(o.a4), p.From.Type, p.Reg, type3, p.To.Type)
+	}
+	return v, nil
+}
+
+func ir(imm22 uint32, rd int16) uint32 {
+	return uint32(rd)&31<<25 | uint32(imm22&(1<<23-1))
+}
+
+func d22(a, disp22 int) uint32 {
+	return uint32(a&1<<29 | disp22&(1<<23-1))
+}
+
+func d19(a, cc1, cc0, p, disp19 int) uint32 {
+	return uint32(a&1<<29 | cc1&1<<21 | cc0&1<<20 | p&1<<19 | disp19&(1<<20-1))
+}
+
+func d30(disp30 int) uint32 {
+	return uint32(disp30 & (1<<31 - 1))
+}
+
+func rrr(rs1, imm_asi, rs2, rd int16) uint32 {
+	return uint32(uint32(rd)&31<<25 | uint32(rs1)&31<<14 | uint32(imm_asi)&255<<5 | uint32(rs2)&31)
+}
+
+func rsr(rs1 int16, simm13 int64, rd int16) uint32 {
+	return uint32(int(rd)&31<<25 | int(rs1)&31<<14 | 1<<13 | int(simm13)&(1<<14-1))
+}
+
+func rd(r int16) uint32 {
+	return uint32(int(r) & 31 << 25)
+}
+
+func op(op int) uint32 {
+	return uint32(op << 30)
+}
+
+func op3(op, op3 int) uint32 {
+	return uint32(op<<30 | op3<<19)
+}
+
+func op2(op2 int) uint32 {
+	return uint32(op2 << 22)
+}
+
+func cond(cond int) uint32 {
+	return uint32(cond << 25)
+}
+
+func opf(opf int) uint32 {
+	return uint32(opf << 5)
+}
+
+func opload(a obj.As) uint32 {
+	switch a {
+	// Load integer.
+	case ALDSB, AMOVB:
+		return op3(3, 9)
+	case ALDSH, AMOVH:
+		return op3(3, 10)
+	case ALDSW, AMOVW:
+		return op3(3, 8)
+	case ALDUB, AMOVUB:
+		return op3(3, 1)
+	case ALDUH, AMOVUH:
+		return op3(3, 2)
+	case ALDUW, AMOVUW:
+		return op3(3, 0)
+	case ALDD, AMOVD:
+		return op3(3, 11)
+
+	// Load floating-point register.
+	case ALDSF, AFMOVS:
+		return op3(3, 0x20)
+	case ALDDF, AFMOVD:
+		return op3(3, 0x23)
+
+	default:
+		panic("unknown instruction: " + obj.Aconv(a))
+	}
+}
+
+func opstore(a obj.As) uint32 {
+	switch a {
+	// Store Integer.
+	case ASTB, AMOVB, AMOVUB:
+		return op3(3, 5)
+	case ASTH, AMOVH, AMOVUH:
+		return op3(3, 6)
+	case ASTW, AMOVW, AMOVUW:
+		return op3(3, 4)
+	case ASTD, AMOVD:
+		return op3(3, 14)
+
+	// Store floating-point.
+	case ASTSF, AFMOVS:
+		return op3(3, 0x24)
+	case ASTDF, AFMOVD:
+		return op3(3, 0x27)
+
+	default:
+		panic("unknown instruction: " + obj.Aconv(a))
+	}
+}
+
+func oprd(a obj.As) uint32 {
+	switch a {
+	// Read ancillary state register.
+	case ARD, AMOVD:
+		return op3(2, 0x28)
+
+	default:
+		panic("unknown instruction: " + obj.Aconv(a))
+	}
+}
+
+func opalu(a obj.As) uint32 {
+	switch a {
+	// Add.
+	case AADD:
+		return op3(2, 0)
+	case AADDCC:
+		return op3(2, 16)
+	case AADDC:
+		return op3(2, 8)
+	case AADDCCC:
+		return op3(2, 24)
+
+	// AND logical operation.
+	case AAND:
+		return op3(2, 1)
+	case AANDCC:
+		return op3(2, 17)
+	case AANDN:
+		return op3(2, 5)
+	case AANDNCC:
+		return op3(2, 21)
+
+	// Multiply and divide.
+	case AMULD:
+		return op3(2, 9)
+	case ASDIVD:
+		return op3(2, 0x2D)
+	case AUDIVD:
+		return op3(2, 0xD)
+
+	// OR logical operation.
+	case AOR, AMOVD, AMOVW:
+		return op3(2, 2)
+	case AORCC:
+		return op3(2, 18)
+	case AORN:
+		return op3(2, 6)
+	case AORNCC:
+		return op3(2, 22)
+
+	// Subtract.
+	case ASUB:
+		return op3(2, 4)
+	case ASUBCC:
+		return op3(2, 20)
+	case ASUBC:
+		return op3(2, 12)
+	case ASUBCCC:
+		return op3(2, 28)
+
+	// XOR logical operation.
+	case AXOR:
+		return op3(2, 3)
+	case AXORCC:
+		return op3(2, 19)
+	case AXNOR:
+		return op3(2, 7)
+	case AXNORCC:
+		return op3(2, 23)
+
+	// Floating-Point Add
+	case AFADDS:
+		return op3(2, 0x34) | opf(0x41)
+	case AFADDD:
+		return op3(2, 0x34) | opf(0x42)
+
+	// Floating-point subtract.
+	case AFSUBS:
+		return op3(2, 0x34) | opf(0x45)
+	case AFSUBD:
+		return op3(2, 0x34) | opf(0x46)
+
+	// Floating-point divide.
+	case AFDIVS:
+		return op3(2, 0x34) | opf(0x4D)
+	case AFDIVD:
+		return op3(2, 0x34) | opf(0x4E)
+
+	// Floating-point multiply.
+	case AFMULS:
+		return op3(2, 0x34) | opf(0x49)
+	case AFMULD:
+		return op3(2, 0x34) | opf(0x4A)
+	case AFSMULD:
+		return op3(2, 0x34) | opf(0x69)
+
+	// Shift.
+	case ASLLW:
+		return op3(2, 0x25)
+	case ASRLW:
+		return op3(2, 0x26)
+	case ASRAW:
+		return op3(2, 0x27)
+	case ASLLD:
+		return op3(2, 0x25) | 1<<12
+	case ASRLD:
+		return op3(2, 0x26) | 1<<12
+	case ASRAD:
+		return op3(2, 0x27) | 1<<12
+
+	case ASAVE:
+		return op3(2, 0x3C)
+	case ARESTORE:
+		return op3(2, 0x3D)
+
+	default:
+		panic("unknown instruction: " + obj.Aconv(a))
+	}
+}
+
+func opcode(a obj.As) uint32 {
+	switch a {
+	// Branch on integer condition codes with prediction (BPcc).
+	case obj.AJMP:
+		return cond(8) | op2(1)
+	case ABN, ABNW, ABND:
+		return cond(0) | op2(1)
+	case ABNE, ABNEW, ABNED:
+		return cond(9) | op2(1)
+	case ABE, ABEW, ABED:
+		return cond(1) | op2(1)
+	case ABG, ABGW, ABGD:
+		return cond(10) | op2(1)
+	case ABLE, ABLEW, ABLED:
+		return cond(2) | op2(1)
+	case ABGE, ABGEW, ABGED:
+		return cond(11) | op2(1)
+	case ABL, ABLW, ABLD:
+		return cond(3) | op2(1)
+	case ABGU, ABGUW, ABGUD:
+		return cond(12) | op2(1)
+	case ABLEU, ABLEUW, ABLEUD:
+		return cond(4) | op2(1)
+	case ABCC, ABCCW, ABCCD:
+		return cond(13) | op2(1)
+	case ABCS, ABCSW, ABCSD:
+		return cond(5) | op2(1)
+	case ABPOS, ABPOSW, ABPOSD:
+		return cond(14) | op2(1)
+	case ABNEG, ABNEGW, ABNEGD:
+		return cond(6) | op2(1)
+	case ABVC, ABVCW, ABVCD:
+		return cond(15) | op2(1)
+	case ABVS, ABVSW, ABVSD:
+		return cond(7) | op2(1)
+
+	// Branch on integer register with prediction (BPr).
+	case ABRZ:
+		return cond(1) | op2(3)
+	case ABRLEZ:
+		return cond(2) | op2(3)
+	case ABRLZ:
+		return cond(3) | op2(3)
+	case ABRNZ:
+		return cond(5) | op2(3)
+	case ABRGZ:
+		return cond(6) | op2(3)
+	case ABRGEZ:
+		return cond(7) | op2(3)
+
+	// Call and link
+	case obj.ACALL, obj.ADUFFCOPY, obj.ADUFFZERO:
+		return op(1)
+
+	case ACASW:
+		return op3(3, 0x3C)
+	case ACASD:
+		return op3(3, 0x3E)
+
+	case AFABSS:
+		return op3(2, 0x34) | opf(9)
+	case AFABSD:
+		return op3(2, 0x34) | opf(10)
+
+	// Branch on floating-point condition codes (FBfcc).
+	case AFBA:
+		return cond(8) | op2(6)
+	case AFBN:
+		return cond(0) | op2(6)
+	case AFBU:
+		return cond(7) | op2(6)
+	case AFBG:
+		return cond(6) | op2(6)
+	case AFBUG:
+		return cond(5) | op2(6)
+	case AFBL:
+		return cond(4) | op2(6)
+	case AFBUL:
+		return cond(3) | op2(6)
+	case AFBLG:
+		return cond(2) | op2(6)
+	case AFBNE:
+		return cond(1) | op2(6)
+	case AFBE:
+		return cond(9) | op2(6)
+	case AFBUE:
+		return cond(10) | op2(6)
+	case AFBGE:
+		return cond(11) | op2(6)
+	case AFBUGE:
+		return cond(12) | op2(6)
+	case AFBLE:
+		return cond(13) | op2(6)
+	case AFBULE:
+		return cond(14) | op2(6)
+	case AFBO:
+		return cond(15) | op2(6)
+
+	// Floating-point compare.
+	case AFCMPS:
+		return op3(2, 0x35) | opf(0x51)
+	case AFCMPD:
+		return op3(2, 0x35) | opf(0x52)
+
+	// Convert 32-bit integer to floating point.
+	case AFITOS:
+		return op3(2, 0x34) | opf(0xC4)
+	case AFITOD:
+		return op3(2, 0x34) | opf(0xC8)
+
+	case AFLUSH:
+		return op3(2, 0x3B)
+
+	case AFLUSHW:
+		return op3(2, 0x2B)
+
+	// Floating-point move.
+	case AFMOVS:
+		return op3(2, 0x34) | opf(1)
+	case AFMOVD:
+		return op3(2, 0x34) | opf(2)
+
+	// Floating-point negate.
+	case AFNEGS:
+		return op3(2, 0x34) | opf(5)
+	case AFNEGD:
+		return op3(2, 0x34) | opf(6)
+
+	// Floating-point square root.
+	case AFSQRTS:
+		return op3(2, 0x34) | opf(0x29)
+	case AFSQRTD:
+		return op3(2, 0x34) | opf(0x2A)
+
+	// Convert floating-point to integer.
+	case AFSTOX:
+		return op3(2, 0x34) | opf(0x81)
+	case AFDTOX:
+		return op3(2, 0x34) | opf(0x82)
+	case AFSTOI:
+		return op3(2, 0x34) | opf(0xD1)
+	case AFDTOI:
+		return op3(2, 0x34) | opf(0xD2)
+
+	// Convert between floating-point formats.
+	case AFSTOD:
+		return op3(2, 0x34) | opf(0xC9)
+	case AFDTOS:
+		return op3(2, 0x34) | opf(0xC6)
+
+	// Convert 64-bit integer to floating point.
+	case AFXTOS:
+		return op3(2, 0x34) | opf(0x84)
+	case AFXTOD:
+		return op3(2, 0x34) | opf(0x88)
+
+	// Jump and link.
+	case AJMPL:
+		return op3(2, 0x38)
+
+	// Move Integer Register on Condition (MOVcc).
+	case AMOVA:
+		return op3(2, 0x2C) | 8<<14 | 1<<18
+	case AMOVN:
+		return op3(2, 0x2C) | 0<<14 | 1<<18
+	case AMOVNE:
+		return op3(2, 0x2C) | 9<<14 | 1<<18
+	case AMOVE:
+		return op3(2, 0x2C) | 1<<14 | 1<<18
+	case AMOVG:
+		return op3(2, 0x2C) | 10<<14 | 1<<18
+	case AMOVLE:
+		return op3(2, 0x2C) | 2<<14 | 1<<18
+	case AMOVGE:
+		return op3(2, 0x2C) | 11<<14 | 1<<18
+	case AMOVL:
+		return op3(2, 0x2C) | 3<<14 | 1<<18
+	case AMOVGU:
+		return op3(2, 0x2C) | 12<<14 | 1<<18
+	case AMOVLEU:
+		return op3(2, 0x2C) | 4<<14 | 1<<18
+	case AMOVCC:
+		return op3(2, 0x2C) | 13<<14 | 1<<18
+	case AMOVCS:
+		return op3(2, 0x2C) | 5<<14 | 1<<18
+	case AMOVPOS:
+		return op3(2, 0x2C) | 14<<14 | 1<<18
+	case AMOVNEG:
+		return op3(2, 0x2C) | 6<<14 | 1<<18
+	case AMOVVC:
+		return op3(2, 0x2C) | 15<<14 | 1<<18
+	case AMOVVS:
+		return op3(2, 0x2C) | 7<<14 | 1<<18
+
+	// Move Integer Register on Register Condition (MOVr).
+	case AMOVRZ:
+		return op3(2, 0x2f) | 1<<10
+	case AMOVRLEZ:
+		return op3(2, 0x2f) | 2<<10
+	case AMOVRLZ:
+		return op3(2, 0x2f) | 3<<10
+	case AMOVRNZ:
+		return op3(2, 0x2f) | 5<<10
+	case AMOVRGZ:
+		return op3(2, 0x2f) | 6<<10
+	case AMOVRGEZ:
+		return op3(2, 0x2f) | 7<<10
+
+	// Memory Barrier.
+	case AMEMBAR:
+		return op3(2, 0x28) | 0xF<<14 | 1<<13
+
+	case ASETHI, ARNOP:
+		return op2(4)
+
+	// Trap on Integer Condition Codes (Tcc).
+	case ATA:
+		return op3(2, 0x3A)
+
+	default:
+		panic("unknown instruction: " + obj.Aconv(a))
+	}
+}
+
+func oregclass(offset int64) int8 {
+	if offset == 0 {
+		return ClassIndir0
+	}
+	if -4096 <= offset && offset <= 4095 {
+		return ClassIndir13
+	}
+	return ClassIndir
+}
+
+func addrclass(offset int64) int8 {
+	if -4096 <= offset && offset <= 4095 {
+		return ClassRegConst13
+	}
+	return ClassRegConst
+}
+
+func constclass(offset int64) int8 {
+	if 0 <= offset && offset <= 31 {
+		return ClassConst5
+	}
+	if 0 <= offset && offset <= 63 {
+		return ClassConst6
+	}
+	if -512 <= offset && offset <= 513 {
+		return ClassConst10
+	}
+	if -1024 <= offset && offset <= 1023 {
+		return ClassConst11
+	}
+	if -4096 <= offset && offset <= 4095 {
+		return ClassConst13
+	}
+	if -1<<31 <= offset && offset < 0 {
+		return ClassConst31_
+	}
+	if 0 <= offset && offset <= 1<<31-1 {
+		return ClassConst31
+	}
+	if 0 <= offset && offset <= 1<<32-1 {
+		return ClassConst32
+	}
+	return ClassConst
+}
+
+func rclass(r int16) int8 {
+	switch {
+	case r == REG_ZR:
+		return ClassZero
+	case REG_G1 <= r && r <= REG_I7:
+		return ClassReg
+	case REG_F0 <= r && r <= REG_F31:
+		return ClassFReg
+	case REG_D0 <= r && r <= REG_D62:
+		return ClassDReg
+	case r == REG_ICC || r == REG_XCC:
+		return ClassCond
+	case REG_FCC0 <= r && r <= REG_FCC3:
+		return ClassFCond
+	case r == REG_BSP || r == REG_BFP:
+		return ClassReg | ClassBias
+	case r >= REG_SPECIAL:
+		return ClassSpcReg
+	}
+	return ClassUnknown
+}
+
+func aclass(ctxt *obj.Link, a *obj.Addr) int8 {
+	switch a.Type {
+	case obj.TYPE_NONE:
+		return ClassNone
+
+	case obj.TYPE_REG:
+		return rclass(a.Reg)
+
+	case obj.TYPE_MEM:
+		switch a.Name {
+		case obj.NAME_EXTERN, obj.NAME_STATIC:
+			if a.Sym == nil {
+				return ClassUnknown
+			}
+			if a.Sym.Type == obj.STLSBSS {
+				return ClassTLSMem
+			}
+			return ClassMem
+
+		case obj.NAME_AUTO, obj.NAME_PARAM:
+			return aclass(ctxt, autoeditaddr(ctxt, a))
+
+		case obj.NAME_NONE:
+			if a.Scale == 1 {
+				return ClassIndirRegReg
+			}
+			return oregclass(a.Offset) | rclass(a.Reg)&ClassBias
+		}
+
+	case obj.TYPE_FCONST:
+		return ClassFConst
+
+	case obj.TYPE_TEXTSIZE:
+		return ClassTextSize
+
+	case obj.TYPE_CONST, obj.TYPE_ADDR:
+		switch a.Name {
+		case obj.NAME_NONE:
+			if a.Reg != 0 {
+				if a.Reg == REG_ZR && a.Offset == 0 {
+					return ClassZero
+				}
+				if a.Scale == 1 {
+					return ClassRegReg
+				}
+				return addrclass(a.Offset) | rclass(a.Reg)&ClassBias
+			}
+			return constclass(a.Offset)
+
+		case obj.NAME_EXTERN, obj.NAME_STATIC:
+			if a.Sym == nil {
+				return ClassUnknown
+			}
+			if a.Sym.Type == obj.STLSBSS {
+				return ClassTLSAddr
+			}
+			return ClassAddr
+
+		case obj.NAME_AUTO, obj.NAME_PARAM:
+			return aclass(ctxt, autoeditaddr(ctxt, a))
+		}
+	case obj.TYPE_BRANCH:
+		if a.Class == ClassLargeBranch {
+			// Set by span() after initial pcs have been calculated.
+			return ClassLargeBranch
+		}
+		return ClassBranch
+	}
+	return ClassUnknown
+}
+
+func span(ctxt *obj.Link, cursym *obj.LSym) {
+	if cursym.Text == nil || cursym.Text.Link == nil { // handle external functions and ELF section symbols
+		return
+	}
+
+	var pc int64 // relative to entry point
+	for p := cursym.Text.Link; p != nil; p = p.Link {
+		o, err := oplook(autoeditprog(ctxt, p))
+		if err != nil {
+			ctxt.Diag(err.Error())
+		}
+		p.Pc = pc
+		pc += int64(o.size)
+	}
+
+	// Now that initial Pcs have been determined, reclassify branches that
+	// exceed the standard 21-bit signed maximum offset and recalculate.
+	pc = 0
+	for p := cursym.Text.Link; p != nil; p = p.Link {
+		if p.To.Type == obj.TYPE_BRANCH && p.To.Class == ClassBranch {
+			var offset int64
+			if p.Pcond != nil {
+				offset = p.Pcond.Pc - p.Pc
+			} else {
+				// obj.brloop will set p.Pcond to nil for jumps
+				// to the same instruction.
+				offset = p.To.Val.(*obj.Prog).Pc - p.Pc
+			}
+			if offset < -1<<20 || offset > 1<<20-1 {
+				// Ideally, this would be done in aclass(), but
+				// we don't have access to p there or the pc
+				// (yet) in most cases. oplook will use this to
+				// transform the branch appropriately so that
+				// asmout will perform a "large" branch.
+				p.To.Class = ClassLargeBranch
+			}
+		}
+
+		o, err := oplook(autoeditprog(ctxt, p))
+		if err != nil {
+			ctxt.Diag(err.Error())
+		}
+
+		p.Pc = pc
+		pc += int64(o.size)
+	}
+
+	cursym.Size = pc
+	cursym.Grow(cursym.Size)
+
+	var text []uint32 // actual assembled bytes
+	for p := cursym.Text.Link; p != nil; p = p.Link {
+		p1 := autoeditprog(ctxt, p)
+		o, _ := oplook(p1)
+		out, err := asmout(p1, o, cursym)
+		if err != nil {
+			ctxt.Diag("span: can't assemble: %v\n\t%v", err, p)
+		}
+		text = append(text, out...)
+	}
+
+	bp := cursym.P
+	for _, v := range text {
+		ctxt.Arch.ByteOrder.PutUint32(bp, v)
+		bp = bp[4:]
+	}
+}
+
+// bigmove assembles a move of the constant part of addr into reg.
+func bigmove(ctxt *obj.Link, addr *obj.Addr, reg int16) (out []uint32) {
+	out = make([]uint32, 2)
+	class := aclass(ctxt, addr)
+	switch class {
+	case ClassRegConst, ClassIndir:
+		class = constclass(addr.Offset)
+	}
+	switch class {
+	// MOVD $imm32, R ->
+	// 	SETHI hi($imm32), R
+	// 	OR R, lo($imm32), R
+	case ClassConst31, ClassConst32:
+		out[0] = opcode(ASETHI) | ir(uint32(addr.Offset)>>10, reg)
+		out[1] = opalu(AOR) | rsr(reg, int64(addr.Offset&0x3FF), reg)
+
+	// MOVD -$imm31, R ->
+	// 	SETHI hi(^$imm32), R
+	// 	XOR R, lo($imm32)|0x1C00, R
+	case ClassConst31_:
+		out[0] = opcode(ASETHI) | ir(^(uint32(addr.Offset))>>10, reg)
+		out[1] = opalu(AXOR) | rsr(reg, int64(uint32(addr.Offset)&0x3ff|0x1C00), reg)
+	default:
+		panic("unexpected operand class: " + DRconv(class))
+	}
+	return out
+}
+
+func usesRegs(a *obj.Addr) bool {
+	if a == nil {
+		return false
+	}
+	switch a.Class {
+	case ClassReg, ClassFReg, ClassDReg, ClassCond, ClassFCond, ClassSpcReg, ClassZero, ClassRegReg, ClassRegConst13, ClassRegConst, ClassIndirRegReg, ClassIndir0, ClassIndir13, ClassIndir:
+		return true
+	}
+	return false
+}
+
+func isTMP(r int16) bool {
+	return r == REG_TMP || r == REG_TMP2
+}
+
+func usesTMP(a *obj.Addr) bool {
+	return usesRegs(a) && (isTMP(a.Reg) || isTMP(a.Index))
+}
+
+func srcCount(p *obj.Prog) (c int) {
+	if p.From.Type != obj.TYPE_NONE {
+		c++
+	}
+	if p.Reg != obj.REG_NONE {
+		c++
+	}
+	if p.From3Type() != obj.TYPE_NONE {
+		c++
+	}
+	return c
+}
+
+// largebranch assembles a branch to a pc that exceeds a 21-bit signed displacement
+func largebranch(offset int64) ([]uint32, error) {
+	if offset%4 != 0 {
+		return nil, errors.New("branch target not mod 4")
+	}
+
+	out := make([]uint32, 7)
+	// We don't know where we are, and we don't want to emit a
+	// reloc, so save %o7 since we may be in the function prologue,
+	// then do a pc-relative call to determine current address,
+	// then restore %o7 so that we can use the current address plus
+	// the calculated offset to perform a "large" jump to the
+	// desired location.
+	out[0] = opalu(AMOVD) | rrr(REG_ZR, 0, REG_OLR, REG_TMP2)
+	out[1] = opcode(obj.ACALL) | d30(1)
+	out[2] = opalu(AMOVD) | rrr(REG_ZR, 0, REG_OLR, REG_TMP)
+	out[3] = opalu(AMOVD) | rrr(REG_ZR, 0, REG_TMP2, REG_OLR)
+	offset -= 4 // make branch relative to call
+	class := constclass(offset)
+	switch class {
+	// 	SETHI hi($imm32), R
+	// 	OR R, lo($imm32), R
+	case ClassConst31, ClassConst32:
+		out[4] = opcode(ASETHI) | ir(uint32(offset)>>10, REG_TMP2)
+		out[5] = opalu(AOR) | rsr(REG_TMP2, int64(offset&0x3FF), REG_TMP2)
+
+	// 	SETHI hi(^$imm32), R
+	// 	XOR R, lo($imm32)|0x1C00, R
+	case ClassConst31_:
+		out[4] = opcode(ASETHI) | ir(^(uint32(offset))>>10, REG_TMP2)
+		out[5] = opalu(AXOR) | rsr(REG_TMP2, int64(uint32(offset)&0x3ff|0x1C00), REG_TMP2)
+	default:
+		panic("unexpected operand class: " + DRconv(class))
+	}
+	out[6] = opcode(AJMPL) | rrr(REG_TMP, 0, REG_TMP2, REG_ZR)
+	return out, nil
+}
+
+func asmout(p *obj.Prog, o Opval, cursym *obj.LSym) (out []uint32, err error) {
+	out = make([]uint32, 12)
+	o1 := &out[0]
+	o2 := &out[1]
+	o3 := &out[2]
+	o4 := &out[3]
+	o5 := &out[4]
+	o6 := &out[5]
+	o7 := &out[6]
+	o8 := &out[7]
+	o9 := &out[8]
+	o10 := &out[9]
+	o11 := &out[10]
+	o12 := &out[11]
+	if o.OpInfo == ClobberTMP {
+		if usesTMP(&p.From) {
+			return nil, fmt.Errorf("asmout: %q not allowed: synthetic instruction clobbers temporary registers", obj.Mconv(&p.From))
+		}
+		if isTMP(p.Reg) {
+			return nil, fmt.Errorf("asmout: %q not allowed: synthetic instruction clobbers temporary registers", Rconv(int(p.Reg)))
+		}
+		if usesTMP(p.From3) {
+			return nil, fmt.Errorf("asmout: %q not allowed: synthetic instruction clobbers temporary registers", obj.Mconv(p.From3))
+		}
+		if usesTMP(&p.To) {
+			if p.From.Type == obj.TYPE_NONE || srcCount(p) < 2 {
+				return nil, fmt.Errorf("asmout: illegal use of temporary register: synthetic instruction clobbers temporary registers")
+			}
+		}
+	}
+	switch o.op {
+	default:
+		return nil, fmt.Errorf("unknown asm %d in %v", o, p)
+
+	case 0: /* pseudo ops */
+		break
+
+	// op Rs,       Rd	-> Rd = Rs op Rd
+	// op Rs1, Rs2, Rd	-> Rd = Rs2 op Rs1
+	case 1:
+		reg := p.To.Reg
+		if p.Reg != 0 {
+			reg = p.Reg
+		}
+		*o1 = opalu(p.As) | rrr(reg, 0, p.From.Reg, p.To.Reg)
+
+	// MOVD Rs, Rd
+	case 2:
+		*o1 = opalu(p.As) | rrr(REG_ZR, 0, p.From.Reg, p.To.Reg)
+
+	// op $imm13, Rs, Rd	-> Rd = Rs op $imm13
+	case 3:
+		reg := p.To.Reg
+		if p.Reg != 0 {
+			reg = p.Reg
+		}
+		*o1 = opalu(p.As) | rsr(reg, p.From.Offset, p.To.Reg)
+
+	// MOVD $imm13, Rd
+	case 4:
+		*o1 = opalu(p.As) | rsr(REG_ZR, p.From.Offset, p.To.Reg)
+
+	// LDD (R1+R2), R	-> R = *(R1+R2)
+	case 5:
+		*o1 = opload(p.As) | rrr(p.From.Reg, 0, p.From.Index, p.To.Reg)
+
+	// STD R, (R1+R2)	-> *(R1+R2) = R
+	case 6:
+		*o1 = opstore(p.As) | rrr(p.To.Reg, 0, p.To.Index, p.From.Reg)
+
+	// LDD $imm13(Rs), R	-> R = *(Rs+$imm13)
+	case 7:
+		*o1 = opload(p.As) | rsr(p.From.Reg, p.From.Offset, p.To.Reg)
+
+	// STD Rs, $imm13(R)	-> *(R+$imm13) = Rs
+	case 8:
+		*o1 = opstore(p.As) | rsr(p.To.Reg, p.To.Offset, p.From.Reg)
+
+	// RD Rspecial, R
+	case 9:
+		*o1 = oprd(p.As) | uint32(p.From.Reg&0x1f)<<14 | rd(p.To.Reg)
+
+	// CASD/CASW
+	case 10:
+		*o1 = opcode(p.As) | rrr(p.From.Reg, 0x80, p.Reg, p.To.Reg)
+
+	// fop Fs, Fd
+	case 11:
+		*o1 = opcode(p.As) | rrr(0, 0, p.From.Reg, p.To.Reg)
+
+	// SETHI $const, R
+	// RNOP
+	case 12:
+		if p.From.Offset&0x3FF != 0 {
+			return nil, errors.New("SETHI constant not mod 1024")
+		}
+		*o1 = opcode(p.As) | ir(uint32(p.From.Offset)>>10, p.To.Reg)
+
+	// MEMBAR $mask
+	case 13:
+		if p.From.Offset > 127 {
+			return nil, errors.New("MEMBAR mask out of range")
+		}
+		*o1 = opcode(p.As) | uint32(p.From.Offset)
+
+	// FCMPD F, F, FCC
+	case 14:
+		*o1 = opcode(p.As) | rrr(p.Reg, 0, p.From.Reg, p.To.Reg&3)
+
+	// MOVD $imm32, R
+	// MOVD -$imm31, R
+	case 15, 16:
+		out := bigmove(p.Ctxt, &p.From, p.To.Reg)
+		return out, nil
+
+	// BLE XCC, n(PC)
+	// JMP n(PC)
+	case 17:
+		var offset int64
+		if p.Pcond != nil {
+			offset = p.Pcond.Pc - p.Pc
+		} else {
+			// obj.brloop will set p.Pcond to nil for jumps to the same instruction.
+			offset = p.To.Val.(*obj.Prog).Pc - p.Pc
+		}
+		if offset < -1<<20 || offset > 1<<20-1 {
+			return nil, errors.New("branch target out of range")
+		}
+		if offset%4 != 0 {
+			return nil, errors.New("branch target not mod 4")
+		}
+		*o1 = opcode(p.As) | uint32(p.From.Reg&3)<<20 | uint32(offset>>2)&(1<<19-1)
+		// default is to predict branch taken
+		if p.Scond == 0 {
+			*o1 |= 1 << 19
+		}
+
+	// BRZ R, n(PC)
+	case 18:
+		offset := p.Pcond.Pc - p.Pc
+		if offset < -1<<19 || offset > 1<<19-1 {
+			return nil, errors.New("branch target out of range")
+		}
+		if offset%4 != 0 {
+			return nil, errors.New("branch target not mod 4")
+		}
+		*o1 = opcode(p.As) | uint32((offset>>14)&3)<<20 | uint32(p.From.Reg&31)<<14 | uint32(offset>>2)&(1<<14-1)
+		// default is to predict branch taken
+		if p.Scond == 0 {
+			*o1 |= 1 << 19
+		}
+
+	// FBA n(PC)
+	case 19:
+		offset := p.Pcond.Pc - p.Pc
+		if offset < -1<<24 || offset > 1<<24-1 {
+			return nil, errors.New("branch target out of range")
+		}
+		if offset%4 != 0 {
+			return nil, errors.New("branch target not mod 4")
+		}
+		*o1 = opcode(p.As) | uint32(offset>>2)&(1<<22-1)
+
+	// JMPL $imm13(Rs1), Rd
+	case 20:
+		*o1 = opcode(p.As) | rsr(p.From.Reg, p.From.Offset, p.To.Reg)
+
+	// JMPL $(R1+R2), Rd
+	case 21:
+		*o1 = opcode(p.As) | rrr(p.From.Reg, 0, p.From.Index, p.To.Reg)
+
+	// CALL sym(SB)
+	// DUFFCOPY, DUFFZERO
+	case 22:
+		*o1 = opcode(p.As)
+		rel := obj.Addrel(cursym)
+		rel.Off = int32(p.Pc)
+		rel.Siz = 4
+		rel.Sym = p.To.Sym
+		rel.Add = p.To.Offset
+		rel.Type = obj.R_CALLSPARC64
+
+	// MOVD $sym(SB), R ->
+	// 	SETHI hh($sym), TMP
+	// 	OR TMP, hm($sym), TMP
+	//	SLLD	$32, TMP, TMP
+	// 	SETHI hi($sym), R
+	// 	OR R, lo($sym), R
+	// 	OR TMP, R, R
+	case 23:
+		*o1 = opcode(ASETHI) | ir(0, REG_TMP)
+		*o2 = opalu(AOR) | rsr(REG_TMP, 0, REG_TMP)
+		rel := obj.Addrel(cursym)
+		rel.Off = int32(p.Pc)
+		rel.Siz = 8
+		rel.Sym = p.From.Sym
+		rel.Add = p.From.Offset
+		rel.Type = obj.R_ADDRSPARC64HI
+		*o3 = opalu(ASLLD) | rsr(REG_TMP, 32, REG_TMP)
+		*o4 = opcode(ASETHI) | ir(0, p.To.Reg)
+		*o5 = opalu(AOR) | rsr(p.To.Reg, 0, p.To.Reg)
+		rel = obj.Addrel(cursym)
+		rel.Off = int32(p.Pc + 12)
+		rel.Siz = 8
+		rel.Sym = p.From.Sym
+		rel.Add = p.From.Offset
+		rel.Type = obj.R_ADDRSPARC64LO
+		*o6 = opalu(AOR) | rrr(REG_TMP, 0, p.To.Reg, p.To.Reg)
+
+	// MOV sym(SB), R ->
+	// 	SETHI hh($sym), TMP
+	// 	OR TMP, hm($sym), TMP
+	//	SLLD	$32, TMP, TMP
+	// 	SETHI hi($sym), TMP2
+	// 	OR TMP2, lo($sym), TMP2
+	// 	OR TMP, TMP2, TMP2
+	//	MOV (TMP2), R
+	case 24:
+		*o1 = opcode(ASETHI) | ir(0, REG_TMP)
+		*o2 = opalu(AOR) | rsr(REG_TMP, 0, REG_TMP)
+		rel := obj.Addrel(cursym)
+		rel.Off = int32(p.Pc)
+		rel.Siz = 8
+		rel.Sym = p.From.Sym
+		rel.Add = p.From.Offset
+		rel.Type = obj.R_ADDRSPARC64HI
+		*o3 = opalu(ASLLD) | rsr(REG_TMP, 32, REG_TMP)
+		*o4 = opcode(ASETHI) | ir(0, REG_TMP2)
+		*o5 = opalu(AOR) | rsr(REG_TMP2, 0, REG_TMP2)
+		rel = obj.Addrel(cursym)
+		rel.Off = int32(p.Pc + 12)
+		rel.Siz = 8
+		rel.Sym = p.From.Sym
+		rel.Add = p.From.Offset
+		rel.Type = obj.R_ADDRSPARC64LO
+		*o6 = opalu(AOR) | rrr(REG_TMP, 0, REG_TMP2, REG_TMP2)
+		*o7 = opload(p.As) | rsr(REG_TMP2, 0, p.To.Reg)
+
+	// MOV R, sym(SB) ->
+	// 	SETHI hh($sym), TMP
+	// 	OR TMP, hm($sym), TMP
+	//	SLLD	$32, TMP, TMP
+	// 	SETHI hi($sym), TMP2
+	// 	OR TMP2, lo($sym), TMP2
+	// 	OR TMP, TMP2, TMP2
+	//	MOV R, (TMP2)
+	case 25:
+		*o1 = opcode(ASETHI) | ir(0, REG_TMP)
+		*o2 = opalu(AOR) | rsr(REG_TMP, 0, REG_TMP)
+		rel := obj.Addrel(cursym)
+		rel.Off = int32(p.Pc)
+		rel.Siz = 8
+		rel.Sym = p.To.Sym
+		rel.Add = p.To.Offset
+		rel.Type = obj.R_ADDRSPARC64HI
+		*o3 = opalu(ASLLD) | rsr(REG_TMP, 32, REG_TMP)
+		*o4 = opcode(ASETHI) | ir(0, REG_TMP2)
+		*o5 = opalu(AOR) | rsr(REG_TMP2, 0, REG_TMP2)
+		rel = obj.Addrel(cursym)
+		rel.Off = int32(p.Pc + 12)
+		rel.Siz = 8
+		rel.Sym = p.To.Sym
+		rel.Add = p.To.Offset
+		rel.Type = obj.R_ADDRSPARC64LO
+		*o6 = opalu(AOR) | rrr(REG_TMP, 0, REG_TMP2, REG_TMP2)
+		*o7 = opstore(p.As) | rsr(REG_TMP2, 0, p.From.Reg)
+
+	// RET
+	case 26:
+		*o1 = opcode(AJMPL) | rsr(REG_OLR, 8, REG_ZR)
+
+	// TA $tn
+	case 27:
+		if p.From.Offset > 255 {
+			return nil, errors.New("trap number too big")
+		}
+		*o1 = cond(8) | opcode(p.As) | 1<<13 | uint32(p.From.Offset&0xff)
+
+	// MOVD	$imm13(R), Rd -> ADD R, $imm13, Rd
+	case 28:
+		*o1 = opalu(AADD) | rsr(p.From.Reg, p.From.Offset, p.To.Reg)
+
+	// MOVUB Rs, Rd
+	case 29:
+		*o1 = opalu(AAND) | rsr(p.From.Reg, 0xff, p.To.Reg)
+
+	// AMOVUH Rs, Rd
+	case 30:
+		*o1 = opalu(ASLLD) | rsr(p.From.Reg, 48, p.To.Reg)
+		*o2 = opalu(ASRLD) | rsr(p.To.Reg, 48, p.To.Reg)
+
+	// AMOVUW Rs, Rd
+	case 31:
+		*o1 = opalu(ASRLW) | rsr(p.From.Reg, 0, p.To.Reg)
+
+	// AMOVB Rs, Rd
+	case 32:
+		*o1 = opalu(ASLLD) | rsr(p.From.Reg, 56, p.To.Reg)
+		*o2 = opalu(ASRAD) | rsr(p.To.Reg, 56, p.To.Reg)
+
+	// AMOVH Rs, Rd
+	case 33:
+		*o1 = opalu(ASLLD) | rsr(p.From.Reg, 48, p.To.Reg)
+		*o2 = opalu(ASRAD) | rsr(p.To.Reg, 48, p.To.Reg)
+
+	// AMOVW Rs, Rd
+	case 34:
+		*o1 = opalu(ASRAW) | rsr(p.From.Reg, 0, p.To.Reg)
+
+	// ANEG Rs, Rd
+	case 35:
+		*o1 = opalu(ASUB) | rrr(REG_ZR, 0, p.From.Reg, p.To.Reg)
+
+	// CMP R1, R2
+	case 36:
+		*o1 = opalu(ASUBCC) | rrr(p.Reg, 0, p.From.Reg, REG_ZR)
+
+	// CMP $42, R2
+	case 37:
+		*o1 = opalu(ASUBCC) | rsr(p.Reg, p.From.Offset, REG_ZR)
+
+	// BLED, n(PC)
+	// JMP n(PC)
+	case 38:
+		offset := p.Pcond.Pc - p.Pc
+		if offset < -1<<20 || offset > 1<<20-1 {
+			return nil, errors.New("branch target out of range")
+		}
+		if offset%4 != 0 {
+			return nil, errors.New("branch target not mod 4")
+		}
+		*o1 = opcode(p.As) | 2<<20 | uint32(offset>>2)&(1<<19-1)
+		// default is to predict branch taken
+		if p.Scond == 0 {
+			*o1 |= 1 << 19
+		}
+
+	// UNDEF
+	// This is supposed to be something that stops execution.
+	// It's not supposed to be reached, ever, but if it is, we'd
+	// like to be able to tell how we got there.  Assemble as
+	// 0xdead0 which is guaranteed to raise undefined instruction
+	// exception.
+	case 39:
+		*o1 = 0xdead0 // ILLTRAP
+
+	// CALL R
+	// CALL (R)
+	// CALL R, R
+	case 40:
+		*o1 = opcode(AJMPL) | rsr(p.To.Reg, 0, REG_OLR)
+
+	// ADD $huge, Rd
+	// AND $huge, Rs, Rd
+	case 41:
+		move := bigmove(p.Ctxt, &p.From, REG_TMP)
+		*o1, *o2 = move[0], move[1]
+		reg := p.To.Reg
+		if p.Reg != 0 {
+			reg = p.Reg
+		}
+		*o3 = opalu(p.As) | rrr(reg, 0, REG_TMP, p.To.Reg)
+
+	// AMOVD $huge(R), R
+	case 42:
+		move := bigmove(p.Ctxt, &p.From, REG_TMP)
+		*o1, *o2 = move[0], move[1]
+		*o3 = opalu(AADD) | rrr(p.From.Reg, 0, REG_TMP, p.To.Reg)
+
+	// AMOVD R, huge(R)
+	case 43:
+		move := bigmove(p.Ctxt, &p.To, REG_TMP)
+		*o1, *o2 = move[0], move[1]
+		*o3 = opstore(p.As) | rrr(p.To.Reg, 0, REG_TMP, p.From.Reg)
+
+	// AMOVD huge(R), R
+	case 44:
+		move := bigmove(p.Ctxt, &p.From, REG_TMP)
+		*o1, *o2 = move[0], move[1]
+		*o3 = opload(p.As) | rrr(p.From.Reg, 0, REG_TMP, p.To.Reg)
+
+	// JMP sym(SB) ->
+	//	MOVD	$sym(SB), TMP2 ->
+	// 		SETHI hh($sym), TMP
+	// 		OR TMP, hm($sym), TMP
+	//		SLLD	$32, TMP, TMP
+	// 		SETHI hi($sym), TMP2
+	// 		OR TMP2, lo($sym), TMP2
+	// 		OR TMP, TMP2, TMP2
+	//	JMPL	TMP2, ZR
+	case 45:
+		*o1 = opcode(ASETHI) | ir(0, REG_TMP)
+		*o2 = opalu(AOR) | rsr(REG_TMP, 0, REG_TMP)
+		rel := obj.Addrel(cursym)
+		rel.Off = int32(p.Pc)
+		rel.Siz = 8
+		rel.Sym = p.To.Sym
+		rel.Add = p.To.Offset
+		rel.Type = obj.R_ADDRSPARC64HI
+		*o3 = opalu(ASLLD) | rsr(REG_TMP, 32, REG_TMP)
+		*o4 = opcode(ASETHI) | ir(0, REG_TMP2)
+		*o5 = opalu(AOR) | rsr(REG_TMP2, 0, REG_TMP2)
+		rel = obj.Addrel(cursym)
+		rel.Off = int32(p.Pc + 12)
+		rel.Siz = 8
+		rel.Sym = p.To.Sym
+		rel.Add = p.To.Offset
+		rel.Type = obj.R_ADDRSPARC64LO
+		*o6 = opalu(AOR) | rrr(REG_TMP, 0, REG_TMP2, REG_TMP2)
+		*o7 = opcode(AJMPL) | rsr(REG_TMP2, 0, REG_ZR)
+
+	// MOVPOS XCC, $simm11, R
+	case 46:
+		*o1 = opcode(p.As) | rsr(0, p.From3.Offset, p.To.Reg) | 1<<13 | uint32(p.From.Reg&3<<11)
+
+	// MOVPOS ICC, R, R
+	case 47:
+		*o1 = opcode(p.As) | rrr(0, 0, p.Reg, p.To.Reg) | uint32(p.From.Reg&3<<11)
+
+	// MOVRZ	R, $simm10, Rd
+	case 48:
+		*o1 = opcode(p.As) | rsr(p.From.Reg, p.From3.Offset, p.To.Reg) | 1<<13
+
+	// MOVRZ	R, Rs, Rd
+	case 49:
+		*o1 = opcode(p.As) | rrr(p.From.Reg, 0, p.Reg, p.To.Reg)
+
+	// MOVD $tlssym, R
+	case 50:
+		*o1 = opcode(ASETHI) | ir(0, p.To.Reg)
+		*o2 = opalu(AXOR) | rsr(p.To.Reg, 0, p.To.Reg)
+		rel := obj.Addrel(cursym)
+		rel.Off = int32(p.Pc)
+		rel.Siz = 8
+		rel.Sym = p.From.Sym
+		rel.Add = p.From.Offset
+		rel.Type = obj.R_SPARC64_TLS_LE
+		*o3 = opalu(AADD) | rrr(REG_TLS, 0, p.To.Reg, p.To.Reg)
+
+	// RETRESTORE
+	case 51:
+		*o1 = opload(AMOVD) | rsr(REG_RSP, StackBias+120, REG_ILR)
+		*o2 = opcode(AJMPL) | rsr(REG_ILR, 8, REG_ZR)
+		*o3 = opalu(ARESTORE) | rsr(REG_ZR, 0, REG_ZR)
+
+	// JMP $huge(n(PC)) ->
+	//	MOVD	OLR, TMP2
+	//	CALL	+0x4
+	//	MOVD	OLR, TMP
+	//	MOVD	TMP2, OLR
+	//	MOVD	$huge(n(PC)), TMP2
+	//	...
+	//	JMPL	TMP + TMP2
+	case 52:
+		var offset int64
+		if p.Pcond != nil {
+			offset = p.Pcond.Pc - p.Pc
+		} else {
+			// obj.brloop will set p.Pcond to nil for jumps to the same instruction.
+			offset = p.To.Val.(*obj.Prog).Pc - p.Pc
+		}
+
+		branch, err := largebranch(offset)
+		if err != nil {
+			return nil, err
+		}
+		*o1, *o2, *o3, *o4, *o5, *o6, *o7 =
+			branch[0], branch[1], branch[2],
+			branch[3], branch[4], branch[5],
+			branch[6]
+
+	// BLE XCC, $huge(n(PC)) ->
+	//	BLE	XCC, 4(PC)
+	//	NOP
+	//	BA	10(PC)
+	//	NOP
+	//	MOVD	OLR, TMP2
+	//	CALL	+0x4
+	//	MOVD	OLR, TMP
+	//	MOVD	TMP2, OLR
+	//	MOVD	$huge(n(PC)), TMP2
+	//	...
+	//	JMP	TMP + TMP2
+	//	NOP
+	case 53:
+		offset := int64(16)
+		*o1 = opcode(p.As) | uint32(p.From.Reg&3)<<20 | uint32(offset>>2)&(1<<19-1)
+		// default is to predict branch taken
+		if p.Scond == 0 {
+			*o1 |= 1 << 19
+		}
+
+		if p.Pcond != nil {
+			offset = p.Pcond.Pc - p.Pc
+		} else {
+			// obj.brloop will set p.Pcond to nil for jumps to the same instruction.
+			offset = p.To.Val.(*obj.Prog).Pc - p.Pc
+		}
+		*o2 = opcode(ARNOP)
+		*o3 = opcode(obj.AJMP) | uint32(10)&(1<<22-1)
+		*o4 = opcode(ARNOP)
+
+		offset = p.Pcond.Pc - p.Pc
+		offset -= 16 // make branch relative to first instruction
+		branch, err := largebranch(offset)
+		if err != nil {
+			return nil, err
+		}
+		*o5, *o6, *o7, *o8, *o9, *o10, *o11 =
+			branch[0], branch[1], branch[2],
+			branch[3], branch[4], branch[5],
+			branch[6]
+		*o12 = opcode(ARNOP)
+
+	// BRZ R, $huge(n(PC)) ->
+	//	BRZ	R, 4(PC)
+	//	NOP
+	//	BA	10(PC)
+	//	NOP
+	//	MOVD	OLR, TMP2
+	//	CALL	+0x4
+	//	MOVD	OLR, TMP
+	//	MOVD	TMP2, OLR
+	//	MOVD	$huge(n(PC)), TMP2
+	//	...
+	//	JMP	TMP + TMP2
+	//	NOP
+	case 54:
+		offset := int64(16)
+		*o1 = opcode(p.As) | uint32((offset>>14)&3)<<20 | uint32(p.From.Reg&31)<<14 | uint32(offset>>2)&(1<<14-1)
+		// default is to predict branch taken
+		if p.Scond == 0 {
+			*o1 |= 1 << 19
+		}
+		*o2 = opcode(ARNOP)
+		*o3 = opcode(obj.AJMP) | uint32(10)&(1<<22-1)
+		*o4 = opcode(ARNOP)
+
+		offset = p.Pcond.Pc - p.Pc
+		offset -= 16 // make branch relative to first instruction
+		branch, err := largebranch(offset)
+		if err != nil {
+			return nil, err
+		}
+		*o5, *o6, *o7, *o8, *o9, *o10, *o11 =
+			branch[0], branch[1], branch[2],
+			branch[3], branch[4], branch[5],
+			branch[6]
+		*o12 = opcode(ARNOP)
+
+	// FBA $huge(n(PC)) ->
+	//	FBA	4(PC)
+	//	NOP
+	//	BA	10(PC)
+	//	NOP
+	//	MOVD	OLR, TMP2
+	//	CALL	+0x4
+	//	MOVD	OLR, TMP
+	//	MOVD	TMP2, OLR
+	//	MOVD	$huge(n(PC)), TMP2
+	//	...
+	//	JMP	TMP + TMP2
+	//	NOP
+	case 55:
+		offset := int64(16)
+		*o1 = opcode(p.As) | uint32(offset>>2)&(1<<22-1)
+		*o2 = opcode(ARNOP)
+		*o3 = opcode(obj.AJMP) | uint32(10)&(1<<22-1)
+		*o4 = opcode(ARNOP)
+
+		offset = p.Pcond.Pc - p.Pc
+		offset -= 16 // make branch relative to first instruction
+		branch, err := largebranch(offset)
+		if err != nil {
+			return nil, err
+		}
+		*o5, *o6, *o7, *o8, *o9, *o10, *o11 =
+			branch[0], branch[1], branch[2],
+			branch[3], branch[4], branch[5],
+			branch[6]
+		*o12 = opcode(ARNOP)
+
+	// BLED, $huge(n(PC)) ->
+	//	BLED, 4(PC)
+	//	NOP
+	//	BA	10(PC)
+	//	NOP
+	//	MOVD	OLR, TMP2
+	//	CALL	+0x4
+	//	MOVD	OLR, TMP
+	//	MOVD	TMP2, OLR
+	//	MOVD	$huge(n(PC)), TMP2
+	//	...
+	//	JMP	TMP + TMP2
+	//	NOP
+	case 56:
+		offset := int64(16)
+		*o1 = opcode(p.As) | 2<<20 | uint32(offset>>2)&(1<<19-1)
+		// default is to predict branch taken
+		if p.Scond == 0 {
+			*o1 |= 1 << 19
+		}
+		*o2 = opcode(ARNOP)
+		*o3 = opcode(obj.AJMP) | uint32(10)&(1<<22-1)
+		*o4 = opcode(ARNOP)
+
+		offset = p.Pcond.Pc - p.Pc
+		offset -= 16 // make branch relative to first instruction
+		branch, err := largebranch(offset)
+		if err != nil {
+			return nil, err
+		}
+		*o5, *o6, *o7, *o8, *o9, *o10, *o11 =
+			branch[0], branch[1], branch[2],
+			branch[3], branch[4], branch[5],
+			branch[6]
+		*o12 = opcode(ARNOP)
+
+	// CALL n(PC)
+	// CALL $huge(n(PC))
+	case 57:
+		var offset int64
+		if p.Pcond != nil {
+			offset = p.Pcond.Pc - p.Pc
+		} else {
+			// obj.brloop will set p.Pcond to nil for jumps to the same instruction.
+			offset = p.To.Val.(*obj.Prog).Pc - p.Pc
+		}
+		if offset < -1<<31 || offset > 1<<31-4 {
+			return nil, errors.New("branch target out of range")
+		}
+		if offset%4 != 0 {
+			return nil, errors.New("branch target not mod 4")
+		}
+		*o1 = opcode(obj.ACALL) | d30(int(offset>>2))
+	}
+
+	return out[:o.size/4], nil
+}
diff --git a/src/cmd/internal/obj/sparc64/list.go b/src/cmd/internal/obj/sparc64/list.go
new file mode 100644
index 0000000..cb1ce02
--- /dev/null
+++ b/src/cmd/internal/obj/sparc64/list.go
@@ -0,0 +1,94 @@
+// Copyright 2015 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package sparc64
+
+import (
+	"cmd/internal/obj"
+	"fmt"
+)
+
+func init() {
+	obj.RegisterRegister(obj.RBaseSPARC64, REG_LAST, Rconv)
+	obj.RegisterOpcode(obj.ABaseSPARC64, Anames)
+}
+
+func Rconv(r int) string {
+	switch {
+	case r == REG_ZR:
+		return "ZR"
+	case r == REG_TLS:
+		return "TLS"
+	case r == REG_RSP:
+		return "RSP"
+	case r == REG_ILR:
+		return "ILR"
+	case r == REG_G:
+		return "g"
+	case r == REG_TMP2:
+		return "TMP2"
+	case r == REG_TMP:
+		return "TMP"
+	case r == REG_RT1:
+		return "RT1"
+	case r == REG_RT2:
+		return "RT2"
+	case r == REG_CTXT:
+		return "CTXT"
+	case r == REG_RFP:
+		return "RFP"
+	case r == REG_OLR:
+		return "OLR"
+	case r == REG_FTMP:
+		return "FTMP"
+	case r == REG_DTMP:
+		return "DTMP"
+	case r == REG_BSP:
+		return "BSP"
+	case r == REG_BFP:
+		return "BFP"
+	case r == REG_ICC:
+		return "ICC"
+	case r == REG_XCC:
+		return "XCC"
+	case r == REG_CCR:
+		return "CCR"
+	case r == REG_TICK:
+		return "TICK"
+	case r == REG_RPC:
+		return "RPC"
+	}
+	switch {
+	case REG_G0 <= r && r <= REG_G7:
+		return fmt.Sprintf("G%d", r-REG_G0)
+	case REG_O0 <= r && r <= REG_O7:
+		return fmt.Sprintf("O%d", r-REG_O0)
+	case REG_L0 <= r && r <= REG_L7:
+		return fmt.Sprintf("L%d", r-REG_L0)
+	case REG_I0 <= r && r <= REG_I7:
+		return fmt.Sprintf("I%d", r-REG_I0)
+	case REG_F0 <= r && r <= REG_F31:
+		return fmt.Sprintf("F%d", r-REG_F0)
+	case REG_D0 <= r && r <= REG_D30 && r%2 == 0:
+		return fmt.Sprintf("D%d", r-REG_D0)
+	case REG_D32 <= r && r <= REG_D62 && r%2 == 1:
+		return fmt.Sprintf("D%d", r-REG_D0+31)
+	case REG_Y0 <= r && r <= REG_Y15:
+		return fmt.Sprintf("Y%d", r-REG_Y0)
+	case REG_FCC0 <= r && r <= REG_FCC3:
+		return fmt.Sprintf("FCC%d", r-REG_FCC0)
+	}
+	return fmt.Sprintf("badreg(%d+%d)", REG_G0, r-REG_G0)
+}
+
+func DRconv(a int8) (cname string) {
+	if a&ClassBias != 0 {
+		cname = cnames[ClassBias] + "|"
+		a &= ^ClassBias
+	}
+	if a >= ClassUnknown && a <= ClassNone {
+		return cname + cnames[a]
+	}
+	return "C_??"
+}
diff --git a/src/cmd/internal/obj/sparc64/obj.go b/src/cmd/internal/obj/sparc64/obj.go
new file mode 100644
index 0000000..f3ac0d1
--- /dev/null
+++ b/src/cmd/internal/obj/sparc64/obj.go
@@ -0,0 +1,1093 @@
+// Copyright 2015 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package sparc64
+
+import (
+	"cmd/internal/obj"
+	"cmd/internal/sys"
+	"fmt"
+	"log"
+	"math"
+)
+
+var isUncondJump = map[obj.As]bool{
+	obj.ACALL:     true,
+	obj.ADUFFZERO: true,
+	obj.ADUFFCOPY: true,
+	obj.AJMP:      true,
+	obj.ARET:      true,
+	AFBA:          true,
+	AJMPL:         true,
+	ARETRESTORE:   true,
+}
+
+var isCondJump = map[obj.As]bool{
+	ABN:    true,
+	ABNE:   true,
+	ABE:    true,
+	ABG:    true,
+	ABLE:   true,
+	ABGE:   true,
+	ABL:    true,
+	ABGU:   true,
+	ABLEU:  true,
+	ABCC:   true,
+	ABCS:   true,
+	ABPOS:  true,
+	ABNEG:  true,
+	ABVC:   true,
+	ABVS:   true,
+	ABNW:   true,
+	ABNEW:  true,
+	ABEW:   true,
+	ABGW:   true,
+	ABLEW:  true,
+	ABGEW:  true,
+	ABLW:   true,
+	ABGUW:  true,
+	ABLEUW: true,
+	ABCCW:  true,
+	ABCSW:  true,
+	ABPOSW: true,
+	ABNEGW: true,
+	ABVCW:  true,
+	ABVSW:  true,
+	ABND:   true,
+	ABNED:  true,
+	ABED:   true,
+	ABGD:   true,
+	ABLED:  true,
+	ABGED:  true,
+	ABLD:   true,
+	ABGUD:  true,
+	ABLEUD: true,
+	ABCCD:  true,
+	ABCSD:  true,
+	ABPOSD: true,
+	ABNEGD: true,
+	ABVCD:  true,
+	ABVSD:  true,
+	ABRZ:   true,
+	ABRLEZ: true,
+	ABRLZ:  true,
+	ABRNZ:  true,
+	ABRGZ:  true,
+	ABRGEZ: true,
+	AFBN:   true,
+	AFBU:   true,
+	AFBG:   true,
+	AFBUG:  true,
+	AFBL:   true,
+	AFBUL:  true,
+	AFBLG:  true,
+	AFBNE:  true,
+	AFBE:   true,
+	AFBUE:  true,
+	AFBGE:  true,
+	AFBUGE: true,
+	AFBLE:  true,
+	AFBULE: true,
+	AFBO:   true,
+}
+
+var isJump = make(map[obj.As]bool)
+
+func init() {
+	for k := range isUncondJump {
+		isJump[k] = true
+	}
+	for k := range isCondJump {
+		isJump[k] = true
+	}
+}
+
+// The stacksplit code is the first thing emitted in the prologue, so must
+// carefully limit its register usage to only those safe for use by the
+// runtime (e.g. REG_RT1) when storing values in a register as it's
+// essentially a leaf function executing in the caller's frame.
+//
+// In addition, since it must be executed before the initial stack setup,
+// any arguments expected in registers (such as ILR) will instead be
+// found in the output registers (OLR) since a 'save' instruction has not
+// been executed yet.
+func stacksplit(ctxt *obj.Link, p *obj.Prog, framesize int32) *obj.Prog {
+	// MOV	g_stackguard(g), RT1
+	p = obj.Appendp(ctxt, p)
+
+	p.As = AMOVD
+	p.From.Type = obj.TYPE_MEM
+	p.From.Reg = REG_G
+	p.From.Offset = 2 * int64(ctxt.Arch.PtrSize) // G.stackguard0
+	if ctxt.Cursym.Cfunc {
+		p.From.Offset = 3 * int64(ctxt.Arch.PtrSize) // G.stackguard1
+	}
+	p.To.Type = obj.TYPE_REG
+	p.To.Reg = REG_RT1
+
+	q := (*obj.Prog)(nil)
+	if framesize <= obj.StackSmall {
+		// small stack: SP+StackBias <= stackguard
+		//	ADD	Stackbias, RSP, RT2
+		//	CMP	stackguard, RT2
+		p = obj.Appendp(ctxt, p)
+		p.As = AADD
+		p.From.Type = obj.TYPE_CONST
+		p.From.Offset = StackBias
+		p.Reg = REG_RSP
+		p.To.Type = obj.TYPE_REG
+		p.To.Reg = REG_RT2
+
+		p = obj.Appendp(ctxt, p)
+		p.As = ACMP
+		p.From.Type = obj.TYPE_REG
+		p.From.Reg = REG_RT1
+		p.Reg = REG_RT2
+	} else if framesize <= obj.StackBig {
+		// large stack: SP-framesize <= stackguard-StackSmall
+		//	ADD	$StackBias - $(framesize - StackSmall), RSP, RT2
+		//	CMP	stackguard, RT2
+		p = obj.Appendp(ctxt, p)
+		p.As = AADD
+		p.From.Type = obj.TYPE_CONST
+		p.From.Offset = int64(StackBias - (framesize - obj.StackSmall))
+		p.Reg = REG_RSP
+		p.To.Type = obj.TYPE_REG
+		p.To.Reg = REG_RT2
+
+		p = obj.Appendp(ctxt, p)
+		p.As = ACMP
+		p.From.Type = obj.TYPE_REG
+		p.From.Reg = REG_RT1
+		p.Reg = REG_RT2
+	} else {
+		// Such a large stack we need to protect against wraparound
+		// if SP is close to zero.
+		//	SP-stackguard+StackGuard <= framesize + (StackGuard-StackSmall)
+		// The +StackGuard on both sides is required to keep the left side positive:
+		// SP is allowed to be slightly below stackguard. See stack.h.
+		//	CMP	$StackPreempt, RT1
+		//	BED	label_of_call_to_morestack
+		//	ADD	$StackBias, RSP, RT2
+		//	ADD	$StackGuard, RT2, RT2
+		//	SUB	RT1, RT2
+		//	MOV	$(framesize+(StackGuard-StackSmall)), RT1
+		//	CMP	RT1, RT2
+		p = obj.Appendp(ctxt, p)
+
+		p.As = ACMP
+		p.From.Type = obj.TYPE_CONST
+		p.From.Offset = obj.StackPreempt
+		p.Reg = REG_RT1
+
+		p = obj.Appendp(ctxt, p)
+		q = p
+		p.As = ABED
+		p.To.Type = obj.TYPE_BRANCH
+
+		p = obj.Appendp(ctxt, p)
+		p.As = AADD
+		p.From.Type = obj.TYPE_CONST
+		p.From.Offset = StackBias
+		p.Reg = REG_RSP
+		p.To.Type = obj.TYPE_REG
+		p.To.Reg = REG_RT2
+
+		p = obj.Appendp(ctxt, p)
+		p.As = AADD
+		p.From.Type = obj.TYPE_CONST
+		p.From.Offset = obj.StackGuard
+		p.Reg = REG_RT2
+		p.To.Type = obj.TYPE_REG
+		p.To.Reg = REG_RT2
+
+		p = obj.Appendp(ctxt, p)
+		p.As = ASUB
+		p.From.Type = obj.TYPE_REG
+		p.From.Reg = REG_RT1
+		p.To.Type = obj.TYPE_REG
+		p.To.Reg = REG_RT2
+
+		p = obj.Appendp(ctxt, p)
+		p.As = AMOVD
+		p.From.Type = obj.TYPE_CONST
+		p.From.Offset = int64(framesize) + (obj.StackGuard - obj.StackSmall)
+		p.To.Type = obj.TYPE_REG
+		p.To.Reg = REG_RT1
+
+		p = obj.Appendp(ctxt, p)
+		p.As = ACMP
+		p.From.Type = obj.TYPE_REG
+		p.From.Reg = REG_RT1
+		p.Reg = REG_RT2
+	}
+
+	// BLEU	do-morestack; note the comparison here is explicitly unsigned,
+	// so that in the event that g_stackguardX is set to StackPreempt or
+	// StackFork, a call to morestack will be triggered.
+	bleu := obj.Appendp(ctxt, p)
+	bleu.As = ABLEUD
+	bleu.To.Type = obj.TYPE_BRANCH
+
+	var last *obj.Prog
+	for last = ctxt.Cursym.Text; last.Link != nil; last = last.Link {
+	}
+
+	// MOV	OLR, I0
+	movlr := obj.Appendp(ctxt, last)
+	movlr.As = AMOVD
+	movlr.From.Type = obj.TYPE_REG
+	movlr.From.Reg = REG_OLR
+	movlr.To.Type = obj.TYPE_REG
+	movlr.To.Reg = REG_I0
+	movlr.Spadj = -framesize
+
+	// CALL runtime.morestack(SB)
+	call := obj.Appendp(ctxt, movlr)
+	call.Lineno = ctxt.Cursym.Text.Lineno
+	call.Mode = ctxt.Cursym.Text.Mode
+	call.As = obj.ACALL
+	call.To.Type = obj.TYPE_MEM
+	call.To.Name = obj.NAME_EXTERN
+	morestack := "runtime.morestack"
+	switch {
+	case ctxt.Cursym.Cfunc:
+		morestack = "runtime.morestackc"
+	case ctxt.Cursym.Text.From3.Offset&obj.NEEDCTXT == 0:
+		morestack = "runtime.morestack_noctxt"
+	}
+	call.To.Sym = obj.Linklookup(ctxt, morestack, 0)
+
+	// A CALL is used here instead of a JMP so that we have a full 32
+	// bit-signed displacement that we can encode into the instruction.
+	// Since this instruction is never actually executed, but instead used
+	// by rewindmorestack(), this is safe (if it was actually executed, it
+	// would overwrite %o7 which would destroy our original return
+	// address).
+
+	// CALL start
+	jmp := obj.Appendp(ctxt, call)
+	jmp.As = obj.ACALL
+	jmp.To.Type = obj.TYPE_BRANCH
+	jmp.Pcond = ctxt.Cursym.Text.Link
+	jmp.Spadj = +framesize
+
+	bleu.Pcond = movlr
+	if q != nil {
+		q.Pcond = movlr
+	}
+
+	return bleu
+}
+
+// AutoeditProg returns a new obj.Prog, with off(SP), off(FP), $off(SP),
+// and $off(FP) replaced with new(RSP).
+func autoeditprog(ctxt *obj.Link, p *obj.Prog) *obj.Prog {
+	r := new(obj.Prog)
+	*r = *p
+	r.From = *autoeditaddr(ctxt, &r.From)
+	r.From3 = autoeditaddr(ctxt, r.From3)
+	r.To = *autoeditaddr(ctxt, &r.To)
+	return r
+}
+
+// Autoeditaddr returns a new obj.Addr, with off(SP), off(FP), $off(SP),
+// and $off(FP) replaced with new(RSP).
+func autoeditaddr(ctxt *obj.Link, a *obj.Addr) *obj.Addr {
+	if a == nil {
+		return nil
+	}
+	if a.Type != obj.TYPE_MEM && a.Type != obj.TYPE_ADDR {
+		return a
+	}
+	r := new(obj.Addr)
+	*r = *a
+	if r.Name == obj.NAME_PARAM {
+		r.Reg = REG_RSP
+		r.Name = obj.NAME_NONE
+		if ctxt.Cursym.Text.From3Offset()&obj.NOFRAME != 0 {
+			r.Offset += MinStackFrameSize + StackBias
+			return r
+		}
+		r.Offset += int64(ctxt.Cursym.Locals) + 2*MinStackFrameSize + StackBias
+		return r
+	}
+	if r.Name == obj.NAME_AUTO {
+		r.Reg = REG_RSP
+		r.Offset += int64(ctxt.Cursym.Locals) + MinStackFrameSize + StackBias
+		r.Name = obj.NAME_NONE
+	}
+	return r
+}
+
+// yfix rewrites references to Y registers (issued by compiler)
+// to F and D registers.
+func yfix(p *obj.Prog) {
+	if REG_Y0 <= p.From.Reg && p.From.Reg <= REG_Y15 {
+		if isInstDouble[p.As] || isSrcDouble[p.As] {
+			p.From.Reg = REG_D0 + (p.From.Reg-REG_Y0)*2
+		} else if isInstFloat[p.As] || isSrcFloat[p.As] {
+			p.From.Reg = REG_F0 + (p.From.Reg-REG_Y0)*2
+		}
+	}
+	if REG_Y0 <= p.Reg && p.Reg <= REG_Y15 {
+		if isInstDouble[p.As] {
+			p.Reg = REG_D0 + (p.Reg-REG_Y0)*2
+		} else {
+			p.Reg = REG_F0 + (p.Reg-REG_Y0)*2
+		}
+	}
+	if p.From3 != nil && REG_Y0 <= p.From3.Reg && p.From3.Reg <= REG_Y15 {
+		if isInstDouble[p.As] {
+			p.From3.Reg = REG_D0 + (p.From3.Reg-REG_Y0)*2
+		} else {
+			p.From3.Reg = REG_F0 + (p.From3.Reg-REG_Y0)*2
+		}
+	}
+	if REG_Y0 <= p.To.Reg && p.To.Reg <= REG_Y15 {
+		if isInstDouble[p.As] || isDstDouble[p.As] {
+			p.To.Reg = REG_D0 + (p.To.Reg-REG_Y0)*2
+		} else if isInstFloat[p.As] || isDstFloat[p.As] {
+			p.To.Reg = REG_F0 + (p.To.Reg-REG_Y0)*2
+		}
+	}
+}
+
+// biasfix rewrites referencing to BSP and BFP to RSP and RFP and
+// adding the stack bias.
+func biasfix(p *obj.Prog) {
+	// Only match 2-operand instructions.
+	if p.From3 != nil || p.Reg != 0 {
+		return
+	}
+	switch p.As {
+	case AMOVD:
+		switch aclass(p.Ctxt, &p.From) {
+		case ClassReg, ClassZero:
+			switch {
+			// MOVD	R, BSP	-> ADD	-$STACK_BIAS, R, RSP
+			case aclass(p.Ctxt, &p.To) == ClassReg|ClassBias:
+				p.As = AADD
+				p.Reg = p.From.Reg
+				if p.From.Type == obj.TYPE_CONST {
+					p.Reg = REG_ZR
+				}
+				p.From.Reg = 0
+				p.From.Offset = -StackBias
+				p.From.Type = obj.TYPE_CONST
+				p.From.Class = aclass(p.Ctxt, &p.From)
+				p.To.Reg -= 256 // must match a.out.go:/REG_BSP
+				p.To.Class = aclass(p.Ctxt, &p.To)
+			}
+
+		case ClassReg | ClassBias:
+			// MOVD	BSP, R	-> ADD	$STACK_BIAS, RSP, R
+			if aclass(p.Ctxt, &p.To) == ClassReg {
+				p.Reg = p.From.Reg - 256 // must match a.out.go:/REG_BSP
+				p.As = AADD
+				p.From.Reg = 0
+				p.From.Offset = StackBias
+				p.From.Type = obj.TYPE_CONST
+				p.From.Class = aclass(p.Ctxt, &p.From)
+			}
+
+		// MOVD	$off(BSP), R	-> MOVD	$(off+STACK_BIAS)(RSP), R
+		case ClassRegConst13 | ClassBias, ClassRegConst | ClassBias:
+			p.From.Reg -= 256 // must match a.out.go:/REG_BSP
+			p.From.Offset += StackBias
+			p.From.Class = aclass(p.Ctxt, &p.From)
+		}
+
+	case AADD, ASUB:
+		// ADD	$const, BSP	-> ADD	$const, RSP
+		if isAddrCompatible(p.Ctxt, &p.From, ClassConst) && aclass(p.Ctxt, &p.To) == ClassReg|ClassBias {
+			p.To.Reg -= 256 // must match a.out.go:/REG_BSP
+			p.To.Class = aclass(p.Ctxt, &p.To)
+		}
+	}
+	switch p.As {
+	case AMOVD, AMOVW, AMOVUW, AMOVH, AMOVUH, AMOVB, AMOVUB,
+		AFMOVD, AFMOVS:
+		switch aclass(p.Ctxt, &p.From) {
+		case ClassZero, ClassReg, ClassFReg, ClassDReg:
+			switch {
+			// MOVD	R, off(BSP)	-> MOVD	R, (off+STACK_BIAS)(RSP)
+			case aclass(p.Ctxt, &p.To)&ClassBias != 0 && isAddrCompatible(p.Ctxt, &p.To, ClassIndir):
+				p.To.Offset += StackBias
+				p.To.Reg -= 256 // must match a.out.go:/REG_BSP
+				p.To.Class = aclass(p.Ctxt, &p.To)
+			}
+
+		// MOVD	off(BSP), R	-> MOVD	(off+STACK_BIAS)(RSP), R
+		case ClassIndir0 | ClassBias, ClassIndir13 | ClassBias, ClassIndir | ClassBias:
+			p.From.Reg -= 256 // must match a.out.go:/REG_BSP
+			p.From.Offset += StackBias
+			p.From.Class = aclass(p.Ctxt, &p.From)
+		}
+	}
+}
+
+func progedit(ctxt *obj.Link, p *obj.Prog) {
+	// Rewrite constant moves to memory to go through an intermediary
+	// register
+	switch p.As {
+	case AMOVD:
+		if (p.From.Type == obj.TYPE_CONST || p.From.Type == obj.TYPE_ADDR) && (p.To.Type == obj.TYPE_MEM) {
+			q := obj.Appendp(ctxt, p)
+			q.As = p.As
+			q.To = p.To
+			q.From.Type = obj.TYPE_REG
+			q.From.Reg = REG_TMP
+			q.From.Offset = 0
+
+			p.To = obj.Addr{}
+			p.To.Type = obj.TYPE_REG
+			p.To.Reg = REG_TMP
+			p.To.Offset = 0
+		}
+
+	case AFMOVS:
+		if (p.From.Type == obj.TYPE_FCONST || p.From.Type == obj.TYPE_ADDR) && (p.To.Type == obj.TYPE_MEM) {
+			q := obj.Appendp(ctxt, p)
+			q.As = p.As
+			q.To = p.To
+			q.From.Type = obj.TYPE_REG
+			q.From.Reg = REG_FTMP
+			q.From.Offset = 0
+
+			p.To = obj.Addr{}
+			p.To.Type = obj.TYPE_REG
+			p.To.Reg = REG_FTMP
+			p.To.Offset = 0
+		}
+
+	case AFMOVD:
+		if (p.From.Type == obj.TYPE_FCONST || p.From.Type == obj.TYPE_ADDR) && (p.To.Type == obj.TYPE_MEM) {
+			q := obj.Appendp(ctxt, p)
+			q.As = p.As
+			q.To = p.To
+			q.From.Type = obj.TYPE_REG
+			q.From.Reg = REG_DTMP
+			q.From.Offset = 0
+
+			p.To = obj.Addr{}
+			p.To.Type = obj.TYPE_REG
+			p.To.Reg = REG_DTMP
+			p.To.Offset = 0
+		}
+	}
+
+	// Rewrite 64-bit integer constants and float constants
+	// to values stored in memory.
+	switch p.As {
+	case AMOVD:
+		if aclass(p.Ctxt, &p.From) == ClassConst {
+			literal := fmt.Sprintf("$i64.%016x", uint64(p.From.Offset))
+			s := obj.Linklookup(ctxt, literal, 0)
+			s.Size = 8
+			p.From.Type = obj.TYPE_MEM
+			p.From.Sym = s
+			p.From.Name = obj.NAME_EXTERN
+			p.From.Offset = 0
+		}
+
+	case AFMOVS:
+		if p.From.Type == obj.TYPE_FCONST {
+			f32 := float32(p.From.Val.(float64))
+			i32 := math.Float32bits(f32)
+			literal := fmt.Sprintf("$f32.%08x", uint32(i32))
+			s := obj.Linklookup(ctxt, literal, 0)
+			s.Size = 4
+			p.From.Type = obj.TYPE_MEM
+			p.From.Sym = s
+			p.From.Name = obj.NAME_EXTERN
+			p.From.Offset = 0
+		}
+
+	case AFMOVD:
+		if p.From.Type == obj.TYPE_FCONST {
+			i64 := math.Float64bits(p.From.Val.(float64))
+			literal := fmt.Sprintf("$f64.%016x", uint64(i64))
+			s := obj.Linklookup(ctxt, literal, 0)
+			s.Size = 8
+			p.From.Type = obj.TYPE_MEM
+			p.From.Sym = s
+			p.From.Name = obj.NAME_EXTERN
+			p.From.Offset = 0
+		}
+	}
+
+	// TODO(aram): remove this when compiler can use F and
+	// D registers directly.
+	yfix(p)
+
+	biasfix(p)
+}
+
+func isNOFRAME(p *obj.Prog) bool {
+	return p.From3Offset()&obj.NOFRAME != 0
+}
+
+func isREGWIN(p *obj.Prog) bool {
+	return p.From3Offset()&obj.REGWIN != 0
+}
+
+// TODO(aram):
+func preprocess(ctxt *obj.Link, cursym *obj.LSym) {
+	cursym.Text.Pc = 0
+	cursym.Args = cursym.Text.To.Val.(int32)
+	cursym.Locals = int32(cursym.Text.To.Offset)
+
+	// Find leaf subroutines,
+	// Strip NOPs.
+	var q *obj.Prog
+	var q1 *obj.Prog
+	for p := cursym.Text; p != nil; p = p.Link {
+		switch {
+		case p.As == obj.ATEXT:
+			p.Mark |= LEAF
+
+		case p.As == obj.ARET:
+			if cursym.Text.Mark&LEAF != 0 && p.To.Sym != nil { // RETJMP
+				cursym.Text.From3.Offset |= obj.NOFRAME
+			}
+			break
+
+		case p.As == obj.ANOP:
+			q1 = p.Link
+			q.Link = q1 /* q is non-nop */
+			q1.Mark |= p.Mark
+			continue
+
+		case isUncondJump[p.As]:
+			cursym.Text.Mark &^= LEAF
+			fallthrough
+
+		case isCondJump[p.As]:
+			q1 = p.Pcond
+
+			if q1 != nil {
+				for q1.As == obj.ANOP {
+					q1 = q1.Link
+					p.Pcond = q1
+				}
+			}
+
+			break
+		}
+
+		q = p
+	}
+
+	for p := cursym.Text; p != nil; p = p.Link {
+		switch p.As {
+		case obj.ATEXT:
+			if cursym.Text.Mark&LEAF != 0 {
+				cursym.Leaf = true
+			}
+		}
+	}
+
+	for p := cursym.Text; p != nil; p = p.Link {
+		switch p.As {
+		case obj.ATEXT:
+			frameSize := cursym.Locals
+			if frameSize < 0 {
+				ctxt.Diag("%v: negative frame size %d", p, frameSize)
+			}
+			if frameSize%16 != 0 {
+				ctxt.Diag("%v: unaligned frame size %d - must be 0 mod 16", p, frameSize)
+			}
+			if frameSize != 0 && isNOFRAME(p) {
+				ctxt.Diag("%v: non-zero framesize for NOFRAME function", p)
+			}
+
+			if frameSize == 0 && cursym.Leaf {
+				// promote leaves without automatics to NOFRAME.
+				cursym.Text.From3.Offset |= obj.NOFRAME
+			}
+
+			// Without these NOPs, DTrace changes the execution of the binary,
+			// This should never happen, but these NOPs seems to fix it.
+			// Keep these NOPs in here until we understand the DTrace behavior.
+			p = obj.Appendp(ctxt, p)
+			p.As = ARNOP
+			p = obj.Appendp(ctxt, p)
+			p.As = ARNOP
+
+			if isNOFRAME(cursym.Text) {
+				break
+			}
+
+			// split check must be done before reserving stack
+			// space or changing register windows.
+			if !(cursym.Text.From3.Offset&obj.NOSPLIT != 0) {
+				p = stacksplit(ctxt, p, frameSize+MinStackFrameSize) // emit split check
+			}
+
+			if isREGWIN(cursym.Text) {
+				// SAVE -(frameSize+MinStackFrameSize), RSP, RSP
+				p = obj.Appendp(ctxt, p)
+				p.As = ASAVE
+				p.From.Type = obj.TYPE_CONST
+				p.From.Offset = -int64(frameSize + MinStackFrameSize)
+				p.Reg = REG_RSP
+				p.To.Type = obj.TYPE_REG
+				p.To.Reg = REG_RSP
+				p.Spadj = frameSize + MinStackFrameSize
+
+				// FLUSHW
+				p = obj.Appendp(ctxt, p)
+				p.As = AFLUSHW
+
+				// MOVD RFP, (112+bias)(RSP)
+				p = obj.Appendp(ctxt, p)
+				p.As = AMOVD
+				p.From.Type = obj.TYPE_REG
+				p.From.Reg = REG_RFP
+				p.To.Type = obj.TYPE_MEM
+				p.To.Reg = REG_RSP
+				p.To.Offset = int64(112 + StackBias)
+
+				// MOVD ILR, (120+bias)(RSP)
+				p = obj.Appendp(ctxt, p)
+				p.As = AMOVD
+				p.From.Type = obj.TYPE_REG
+				p.From.Reg = REG_ILR
+				p.To.Type = obj.TYPE_MEM
+				p.To.Reg = REG_RSP
+				p.To.Offset = int64(120 + StackBias)
+			} else {
+				// ADD -(frameSize+MinStackFrameSize), RSP
+				p = obj.Appendp(ctxt, p)
+				p.As = AADD
+				p.From.Type = obj.TYPE_CONST
+				p.From.Offset = -int64(frameSize + MinStackFrameSize)
+				p.To.Type = obj.TYPE_REG
+				p.To.Reg = REG_RSP
+				p.Spadj = frameSize + MinStackFrameSize
+
+				// SUB -(frameSize+MinStackFrameSize), RSP, RFP
+				p = obj.Appendp(ctxt, p)
+				p.As = ASUB
+				p.From.Type = obj.TYPE_CONST
+				p.From.Offset = -int64(frameSize + MinStackFrameSize)
+				p.Reg = REG_RSP
+				p.To.Type = obj.TYPE_REG
+				p.To.Reg = REG_RFP
+
+				// MOVD RFP, (112+bias)(RSP)
+				p = obj.Appendp(ctxt, p)
+				p.As = AMOVD
+				p.From.Type = obj.TYPE_REG
+				p.From.Reg = REG_RFP
+				p.To.Type = obj.TYPE_MEM
+				p.To.Reg = REG_RSP
+				p.To.Offset = int64(112 + StackBias)
+
+				// MOVD OLR, ILR
+				p = obj.Appendp(ctxt, p)
+				p.As = AMOVD
+				p.From.Type = obj.TYPE_REG
+				p.From.Reg = REG_OLR
+				p.To.Type = obj.TYPE_REG
+				p.To.Reg = REG_ILR
+
+				// MOVD ILR, (120+bias)(RSP)
+				p = obj.Appendp(ctxt, p)
+				p.As = AMOVD
+				p.From.Type = obj.TYPE_REG
+				p.From.Reg = REG_ILR
+				p.To.Type = obj.TYPE_MEM
+				p.To.Reg = REG_RSP
+				p.To.Offset = int64(120 + StackBias)
+
+				// MOVD 0, OLR
+				p = obj.Appendp(ctxt, p)
+				p.As = AMOVD
+				p.From.Type = obj.TYPE_REG
+				p.From.Reg = REG_ZR
+				p.To.Type = obj.TYPE_REG
+				p.To.Reg = REG_OLR
+			}
+
+			if cursym.Text.From3.Offset&obj.WRAPPER != 0 {
+				// if(g->panic != nil && g->panic->argp == FP) g->panic->argp = bottom-of-frame
+				//
+				//	MOVD	g_panic(g), L1
+				//	CMP	ZR, L1
+				//	BED	end
+				//	MOVD	panic_argp(L1), L2
+				//	ADD	$(STACK_BIAS+MinStackFrameSize), RFP, L3
+				//	CMP	L2, L3
+				//	BNED	end
+				//	ADD	$(STACK_BIAS+MinStackFrameSize), RSP, L4
+				//	MOVD	L4, panic_argp(L1)
+				// end:
+				//	RNOP
+				//
+				// The RNOP is needed to give the jumps somewhere to land.
+				q = obj.Appendp(ctxt, p)
+				q.As = AMOVD
+				q.From.Type = obj.TYPE_MEM
+				q.From.Reg = REG_G
+				q.From.Offset = 4 * int64(ctxt.Arch.PtrSize) // G.panic
+				q.To.Type = obj.TYPE_REG
+				q.To.Reg = REG_L1
+
+				q = obj.Appendp(ctxt, q)
+				q.As = ACMP
+				q.From.Type = obj.TYPE_REG
+				q.From.Reg = REG_ZR
+				q.Reg = REG_L1
+
+				q = obj.Appendp(ctxt, q)
+				q.As = ABED
+				q.To.Type = obj.TYPE_BRANCH
+				q1 := q
+
+				q = obj.Appendp(ctxt, q)
+				q.As = AMOVD
+				q.From.Type = obj.TYPE_MEM
+				q.From.Reg = REG_L1
+				q.From.Offset = 0 // Panic.argp
+				q.To.Type = obj.TYPE_REG
+				q.To.Reg = REG_L2
+
+				q = obj.Appendp(ctxt, q)
+				q.As = AADD
+				q.From.Type = obj.TYPE_CONST
+				q.From.Offset = StackBias + MinStackFrameSize
+				q.Reg = REG_RFP
+				q.To.Type = obj.TYPE_REG
+				q.To.Reg = REG_L3
+
+				q = obj.Appendp(ctxt, q)
+				q.As = ACMP
+				q.From.Type = obj.TYPE_REG
+				q.From.Reg = REG_L2
+				q.Reg = REG_L3
+
+				q = obj.Appendp(ctxt, q)
+				q.As = ABNED
+				q.To.Type = obj.TYPE_BRANCH
+				q2 := q
+
+				q = obj.Appendp(ctxt, q)
+				q.As = AADD
+				q.From.Type = obj.TYPE_CONST
+				q.From.Offset = StackBias + MinStackFrameSize
+				q.Reg = REG_RSP
+				q.To.Type = obj.TYPE_REG
+				q.To.Reg = REG_L4
+
+				q = obj.Appendp(ctxt, q)
+				q.As = AMOVD
+				q.From.Type = obj.TYPE_REG
+				q.From.Reg = REG_L4
+				q.To.Type = obj.TYPE_MEM
+				q.To.Reg = REG_L1
+				q.To.Offset = 0 // Panic.argp
+
+				q = obj.Appendp(ctxt, q)
+				q.As = ARNOP
+				q1.Pcond = q
+				q2.Pcond = q
+			}
+
+		case obj.ARET:
+			if isNOFRAME(cursym.Text) {
+				if p.To.Sym != nil { // RETJMP
+					p.As = obj.AJMP
+				}
+				break
+			}
+
+			if isREGWIN(cursym.Text) {
+				// MOVD (112+bias)(RSP), RFP
+				q = obj.Appendp(ctxt, p)
+				p.As = AMOVD
+				p.From.Type = obj.TYPE_MEM
+				p.From.Reg = REG_RSP
+				p.From.Offset = int64(112 + StackBias)
+				p.To.Type = obj.TYPE_REG
+				p.To.Reg = REG_RFP
+
+				q.As = ARETRESTORE
+				q.From.Type = obj.TYPE_NONE
+				q.From.Offset = 0
+				q.Reg = 0
+				q.To.Type = obj.TYPE_NONE
+				q.To.Reg = 0
+				// The SP restore operation needs a Spadj of
+				// -(cursym.Locals + MinStackFrameSize),
+				// and the JMP operation needs a Spadj of
+				// +(cursym.Locals + MinStackFrameSize).
+				//
+				// Since this operation does both, they cancel out
+				// so we don't do any Spadj adjustment.
+				//
+				// The best solution would be to split RETRESTORE
+				// into the constituent instructions, but that requires
+				// more sophisticated delay-slot processing,
+				// since the RESTORE has to be in the delay
+				// slot of the branch.
+
+				break
+			}
+
+			frameSize := cursym.Locals
+
+			// MOVD RFP, TMP
+			q1 = p
+			p = obj.Appendp(ctxt, p)
+			p.As = AJMPL
+			p.From.Type = obj.TYPE_ADDR
+			p.From.Offset = 8
+			p.From.Reg = REG_OLR
+			p.From.Index = 0
+			p.Reg = 0
+			p.To.Type = obj.TYPE_REG
+			p.To.Reg = REG_ZR
+			p.Spadj = frameSize + MinStackFrameSize
+			q1.As = AMOVD
+			q1.From.Type = obj.TYPE_REG
+			q1.From.Reg = REG_RFP
+			q1.To.Type = obj.TYPE_REG
+			q1.To.Reg = REG_TMP
+
+			// restore registers before resetting the stack
+			// pointer; otherwise a spill will overwrite the saved
+			// link register.
+
+			// MOVD (120+StackBias)(RSP), OLR
+			q1 = obj.Appendp(ctxt, q1)
+			q1.As = AMOVD
+			q1.From.Type = obj.TYPE_MEM
+			q1.From.Reg = REG_RSP
+			q1.From.Offset = 120 + StackBias
+			q1.To.Type = obj.TYPE_REG
+			q1.To.Reg = REG_OLR
+
+			// MOVD (120+StackBias)(RFP), ILR
+			q1 = obj.Appendp(ctxt, q1)
+			q1.As = AMOVD
+			q1.From.Type = obj.TYPE_MEM
+			q1.From.Reg = REG_RFP
+			q1.From.Offset = 120 + StackBias
+			q1.To.Type = obj.TYPE_REG
+			q1.To.Reg = REG_ILR
+
+			// MOVD (112+StackBias)(RFP), RFP
+			q1 = obj.Appendp(ctxt, q1)
+			q1.As = AMOVD
+			q1.From.Type = obj.TYPE_MEM
+			q1.From.Reg = REG_RFP
+			q1.From.Offset = 112 + StackBias
+			q1.To.Type = obj.TYPE_REG
+			q1.To.Reg = REG_RFP
+
+			// MOVD TMP, RSP
+			q1 = obj.Appendp(ctxt, q1)
+			q1.As = AMOVD
+			q1.From.Type = obj.TYPE_REG
+			q1.From.Reg = REG_TMP
+			q1.To.Type = obj.TYPE_REG
+			q1.To.Reg = REG_RSP
+			q1.Spadj = -(frameSize + MinStackFrameSize)
+
+		case AADD, ASUB:
+			if p.To.Type == obj.TYPE_REG && p.To.Reg == REG_BSP && p.From.Type == obj.TYPE_CONST {
+				if p.As == AADD {
+					p.Spadj = int32(-p.From.Offset)
+				} else {
+					p.Spadj = int32(+p.From.Offset)
+				}
+			}
+		}
+	}
+
+	// Schedule delay-slots. Only RNOPs for now.
+	for p := cursym.Text; p != nil; p = p.Link {
+		if !isJump[p.As] || p.As == ARETRESTORE {
+			continue
+		}
+		if p.Link != nil && p.Link.As == ARNOP {
+			continue
+		}
+		p = obj.Appendp(ctxt, p)
+		p.As = ARNOP
+	}
+
+	// For future use by oplook and friends.
+	for p := cursym.Text; p != nil; p = p.Link {
+		p.From.Class = aclass(ctxt, &p.From)
+		if p.From3 != nil {
+			p.From3.Class = aclass(ctxt, p.From3)
+		}
+		p.To.Class = aclass(ctxt, &p.To)
+	}
+}
+
+func relinv(a obj.As) obj.As {
+	switch a {
+	case obj.AJMP:
+		return ABN
+	case ABN:
+		return obj.AJMP
+	case ABE:
+		return ABNE
+	case ABNE:
+		return ABE
+	case ABG:
+		return ABLE
+	case ABLE:
+		return ABG
+	case ABGE:
+		return ABL
+	case ABL:
+		return ABGE
+	case ABGU:
+		return ABLEU
+	case ABLEU:
+		return ABGU
+	case ABCC:
+		return ABCS
+	case ABCS:
+		return ABCC
+	case ABPOS:
+		return ABNEG
+	case ABNEG:
+		return ABPOS
+	case ABVC:
+		return ABVS
+	case ABVS:
+		return ABVC
+	case ABNW:
+		return obj.AJMP
+	case ABEW:
+		return ABNEW
+	case ABNEW:
+		return ABEW
+	case ABGW:
+		return ABLEW
+	case ABLEW:
+		return ABGW
+	case ABGEW:
+		return ABLW
+	case ABLW:
+		return ABGEW
+	case ABGUW:
+		return ABLEUW
+	case ABLEUW:
+		return ABGUW
+	case ABCCW:
+		return ABCSW
+	case ABCSW:
+		return ABCCW
+	case ABPOSW:
+		return ABNEGW
+	case ABNEGW:
+		return ABPOSW
+	case ABVCW:
+		return ABVSW
+	case ABVSW:
+		return ABVCW
+	case ABND:
+		return obj.AJMP
+	case ABED:
+		return ABNED
+	case ABNED:
+		return ABED
+	case ABGD:
+		return ABLED
+	case ABLED:
+		return ABGD
+	case ABGED:
+		return ABLD
+	case ABLD:
+		return ABGED
+	case ABGUD:
+		return ABLEUD
+	case ABLEUD:
+		return ABGUD
+	case ABCCD:
+		return ABCSD
+	case ABCSD:
+		return ABCCD
+	case ABPOSD:
+		return ABNEGD
+	case ABNEGD:
+		return ABPOSD
+	case ABVCD:
+		return ABVSD
+	case ABVSD:
+		return ABVCD
+	case AFBN:
+		return AFBA
+	case AFBA:
+		return AFBN
+	case AFBE:
+		return AFBNE
+	case AFBNE:
+		return AFBE
+	case AFBG:
+		return AFBLE
+	case AFBLE:
+		return AFBG
+	case AFBGE:
+		return AFBL
+	case AFBL:
+		return AFBGE
+	}
+
+	log.Fatalf("unknown relation: %s", obj.Aconv(a))
+	return 0
+}
+
+var unaryDst = map[obj.As]bool{
+	obj.ACALL: true,
+	obj.AJMP:  true,
+	AWORD:     true,
+	ADWORD:    true,
+	ABNW:      true,
+	ABNEW:     true,
+	ABEW:      true,
+	ABGW:      true,
+	ABLEW:     true,
+	ABGEW:     true,
+	ABLW:      true,
+	ABGUW:     true,
+	ABLEUW:    true,
+	ABCCW:     true,
+	ABCSW:     true,
+	ABPOSW:    true,
+	ABNEGW:    true,
+	ABVCW:     true,
+	ABVSW:     true,
+	ABND:      true,
+	ABNED:     true,
+	ABED:      true,
+	ABGD:      true,
+	ABLED:     true,
+	ABGED:     true,
+	ABLD:      true,
+	ABGUD:     true,
+	ABLEUD:    true,
+	ABCCD:     true,
+	ABCSD:     true,
+	ABPOSD:    true,
+	ABNEGD:    true,
+	ABVCD:     true,
+	ABVSD:     true,
+}
+
+var Linksparc64 = obj.LinkArch{
+	Arch:       sys.ArchSPARC64,
+	Preprocess: preprocess,
+	Assemble:   span,
+	Follow:     follow,
+	Progedit:   progedit,
+	UnaryDst:   unaryDst,
+}
diff --git a/src/cmd/internal/obj/sparc64/vn.go b/src/cmd/internal/obj/sparc64/vn.go
new file mode 100644
index 0000000..478e29b
--- /dev/null
+++ b/src/cmd/internal/obj/sparc64/vn.go
@@ -0,0 +1,182 @@
+// 	Copyright © 1994-1999 Lucent Technologies Inc.  All rights reserved.
+// 	Portions Copyright © 1995-1997 C H Forsyth (forsyth@terzarima.net)
+// 	Portions Copyright © 1997-1999 Vita Nuova Limited
+// 	Portions Copyright © 2000-2007 Vita Nuova Holdings Limited (www.vitanuova.com)
+// 	Portions Copyright © 2004,2006 Bruce Ellis
+// 	Portions Copyright © 2005-2007 C H Forsyth (forsyth@terzarima.net)
+// 	Revisions Copyright © 2000-2007 Lucent Technologies Inc. and others
+// 	Portions Copyright © 2009 The Go Authors.  All rights reserved.
+//
+// Permission is hereby granted, free of charge, to any person obtaining a copy
+// of this software and associated documentation files (the "Software"), to deal
+// in the Software without restriction, including without limitation the rights
+// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+// copies of the Software, and to permit persons to whom the Software is
+// furnished to do so, subject to the following conditions:
+//
+// The above copyright notice and this permission notice shall be included in
+// all copies or substantial portions of the Software.
+//
+// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL THE
+// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
+// THE SOFTWARE.
+
+// Unlike all existing ports (386, amd64, amd64p32, arm, arm64, ppc64,
+// ppc64le), the sparc64 back-end is truly new code, not derived from
+// Plan9/Inferno code from Vita Nuova. However, the sparc64 back-end
+// has to fit into the Inferno-derived middle-end (cmd/internal/obj),
+// and inherits certain idiosyncrasies. To deal with them, a small
+// amount of code is inherited from the arm64 port. To track copyrights,
+// that code is isolated in this file.
+//
+// Someday we'll refactor the middle-end, and this code will go away.
+
+package sparc64
+
+import (
+	"cmd/internal/obj"
+	"fmt"
+)
+
+func follow(ctxt *obj.Link, s *obj.LSym) {
+	ctxt.Cursym = s
+
+	firstp := ctxt.NewProg()
+	lastp := firstp
+	xfol(ctxt, s.Text, &lastp)
+	lastp.Link = nil
+	s.Text = firstp.Link
+}
+
+func xfol(ctxt *obj.Link, p *obj.Prog, last **obj.Prog) {
+	var q *obj.Prog
+	var r *obj.Prog
+	var a obj.As
+	var i int
+
+loop:
+	if p == nil {
+		return
+	}
+	a = p.As
+	if a == obj.AJMP {
+		q = p.Pcond
+		if q != nil {
+			p.Mark |= FOLL
+			p = q
+			if !(p.Mark&FOLL != 0) {
+				goto loop
+			}
+		}
+	}
+
+	if p.Mark&FOLL != 0 {
+		i = 0
+		q = p
+		for ; i < 4; i, q = i+1, q.Link {
+			if q == *last || q == nil {
+				break
+			}
+			a = q.As
+			if a == obj.ANOP {
+				i--
+				continue
+			}
+
+			if a == obj.AJMP || a == obj.ARET || a == ARETRESTORE {
+				goto copy
+			}
+			if q.Pcond == nil || (q.Pcond.Mark&FOLL != 0) {
+				continue
+			}
+			if a != ABE && a != ABNE {
+				continue
+			}
+
+		copy:
+			for {
+				r = ctxt.NewProg()
+				*r = *p
+				if !(r.Mark&FOLL != 0) {
+					fmt.Printf("cant happen 1\n")
+				}
+				r.Mark |= FOLL
+				if p != q {
+					p = p.Link
+					(*last).Link = r
+					*last = r
+					continue
+				}
+
+				(*last).Link = r
+				*last = r
+				if a == obj.AJMP || a == obj.ARET || a == ARETRESTORE {
+					return
+				}
+				if a == ABNE {
+					r.As = ABE
+				} else {
+					r.As = ABNE
+				}
+				r.Pcond = p.Link
+				r.Link = p.Pcond
+				if !(r.Link.Mark&FOLL != 0) {
+					xfol(ctxt, r.Link, last)
+				}
+				if !(r.Pcond.Mark&FOLL != 0) {
+					fmt.Printf("cant happen 2\n")
+				}
+				return
+			}
+		}
+
+		a = obj.AJMP
+		q = ctxt.NewProg()
+		q.As = a
+		q.Lineno = p.Lineno
+		q.To.Type = obj.TYPE_BRANCH
+		q.To.Offset = p.Pc
+		q.Pcond = p
+		p = q
+	}
+
+	p.Mark |= FOLL
+	(*last).Link = p
+	*last = p
+	if a == obj.AJMP || a == obj.ARET || a == ARETRESTORE {
+		return
+	}
+	if p.Pcond != nil {
+		if a != ABL && p.Link != nil {
+			q = obj.Brchain(ctxt, p.Link)
+			if a != obj.ATEXT {
+				if q != nil && (q.Mark&FOLL != 0) {
+					p.As = relinv(a)
+					p.Link = p.Pcond
+					p.Pcond = q
+				}
+			}
+
+			xfol(ctxt, p.Link, last)
+			q = obj.Brchain(ctxt, p.Pcond)
+			if q == nil {
+				q = p.Pcond
+			}
+			if q.Mark&FOLL != 0 {
+				p.Pcond = q
+				return
+			}
+
+			p = q
+			goto loop
+		}
+	}
+
+	p = p.Link
+	goto loop
+
+}
diff --git a/src/cmd/internal/obj/stack.go b/src/cmd/internal/obj/stack.go
index 712a10f..b14984d 100644
--- a/src/cmd/internal/obj/stack.go
+++ b/src/cmd/internal/obj/stack.go
@@ -11,8 +11,8 @@ const (
 	STACKSYSTEM = 0
 	StackSystem = STACKSYSTEM
 	StackBig    = 4096
-	StackGuard  = 720*stackGuardMultiplier + StackSystem
-	StackSmall  = 128
+	StackGuard  = 2048*stackGuardMultiplier + StackSystem
+	StackSmall  = 256
 	StackLimit  = StackGuard - StackSystem - StackSmall
 )
 
diff --git a/src/cmd/internal/obj/textflag.go b/src/cmd/internal/obj/textflag.go
index d8a52da..619f2ba 100644
--- a/src/cmd/internal/obj/textflag.go
+++ b/src/cmd/internal/obj/textflag.go
@@ -42,9 +42,12 @@ const (
 
 	// Do not insert instructions to allocate a stack frame for this function.
 	// Only valid on functions that declare a frame size of 0.
-	// TODO(mwhudson): only implemented for ppc64x at present.
+	// TODO(mwhudson): only implemented for ppc64x and sparc64 at present.
 	NOFRAME = 512
 
 	// Function can call reflect.Type.Method or reflect.Type.MethodByName.
 	REFLECTMETHOD = 1024
+
+	// This function switches the register window. Only for SPARC64.
+	REGWIN = 2048
 )
diff --git a/src/cmd/internal/obj/util.go b/src/cmd/internal/obj/util.go
index 18813c3..3ab69b2 100644
--- a/src/cmd/internal/obj/util.go
+++ b/src/cmd/internal/obj/util.go
@@ -11,6 +11,8 @@ import (
 	"os"
 	"strings"
 	"time"
+
+	"cmd/internal/sys"
 )
 
 const REG_NONE = 0
@@ -104,23 +106,30 @@ const (
 	C_SCOND_XOR = 14
 )
 
-// CConv formats ARM condition codes.
-func CConv(s uint8) string {
+// CConv formats ARM condition codes and SPARC64 instruction suffixes.
+func CConv(ctxt *Link, s uint8) (sc string) {
 	if s == 0 {
 		return ""
 	}
-	sc := armCondCode[(s&C_SCOND)^C_SCOND_XOR]
-	if s&C_SBIT != 0 {
-		sc += ".S"
-	}
-	if s&C_PBIT != 0 {
-		sc += ".P"
-	}
-	if s&C_WBIT != 0 {
-		sc += ".W"
-	}
-	if s&C_UBIT != 0 { /* ambiguous with FBIT */
-		sc += ".U"
+	switch ctxt.Arch.Family {
+	case sys.ARM, sys.ARM64:
+		sc = armCondCode[(s&C_SCOND)^C_SCOND_XOR]
+		if s&C_SBIT != 0 {
+			sc += ".S"
+		}
+		if s&C_PBIT != 0 {
+			sc += ".P"
+		}
+		if s&C_WBIT != 0 {
+			sc += ".W"
+		}
+		if s&C_UBIT != 0 { /* ambiguous with FBIT */
+			sc += ".U"
+		}
+	case sys.SPARC64:
+		if s == 1 {
+			sc = ".PN"
+		}
 	}
 	return sc
 }
@@ -134,7 +143,7 @@ func (p *Prog) String() string {
 		return "<Prog without ctxt>"
 	}
 
-	sc := CConv(p.Scond)
+	sc := CConv(p.Ctxt, p.Scond)
 
 	var buf bytes.Buffer
 
@@ -283,6 +292,9 @@ func Dconv(p *Prog, a *Addr) string {
 
 	case TYPE_ADDR:
 		str = fmt.Sprintf("$%s", Mconv(a))
+		if a.Index != REG_NONE {
+			str += fmt.Sprintf("(%v*%d)", Rconv(int(a.Index)), int(a.Scale))
+		}
 
 	case TYPE_SHIFT:
 		v := int(a.Offset)
@@ -323,7 +335,12 @@ func Mconv(a *Addr) string {
 		case a.Offset == 0:
 			str = fmt.Sprintf("(%v)", Rconv(int(a.Reg)))
 		case a.Offset != 0:
-			str = fmt.Sprintf("%d(%v)", a.Offset, Rconv(int(a.Reg)))
+			// TODO(aram): remove hack
+			reg := Rconv(int(a.Reg))
+			if (reg == "RSP" || reg == "RFP") && a.Offset > 0x7ff {
+				return fmt.Sprintf("%d+176+2047(%v)", a.Offset-0x7ff-176, reg)
+			}
+			str = fmt.Sprintf("%d(%v)", a.Offset, reg)
 		}
 
 	case NAME_EXTERN:
@@ -390,13 +407,14 @@ var regSpace []regSet
 const (
 	// Because of masking operations in the encodings, each register
 	// space should start at 0 modulo some power of 2.
-	RBase386    = 1 * 1024
-	RBaseAMD64  = 2 * 1024
-	RBaseARM    = 3 * 1024
-	RBasePPC64  = 4 * 1024  // range [4k, 8k)
-	RBaseARM64  = 8 * 1024  // range [8k, 13k)
-	RBaseMIPS64 = 13 * 1024 // range [13k, 14k)
-	RBaseS390X  = 14 * 1024 // range [14k, 15k)
+	RBase386     = 1 * 1024
+	RBaseAMD64   = 2 * 1024
+	RBaseARM     = 3 * 1024
+	RBasePPC64   = 4 * 1024  // range [4k, 8k)
+	RBaseARM64   = 8 * 1024  // range [8k, 13k)
+	RBaseMIPS64  = 13 * 1024 // range [13k, 14k)
+	RBaseS390X   = 14 * 1024 // range [14k, 15k)
+	RBaseSPARC64 = 15 * 1024 // range [15k, 16k)
 )
 
 // RegisterRegister binds a pretty-printer (Rconv) for register
diff --git a/src/cmd/internal/objfile/disasm.go b/src/cmd/internal/objfile/disasm.go
index 25c3301..6858bfe 100644
--- a/src/cmd/internal/objfile/disasm.go
+++ b/src/cmd/internal/objfile/disasm.go
@@ -246,4 +246,5 @@ var byteOrders = map[string]binary.ByteOrder{
 	"ppc64":   binary.BigEndian,
 	"ppc64le": binary.LittleEndian,
 	"s390x":   binary.BigEndian,
+	"sparc64": binary.BigEndian,
 }
diff --git a/src/cmd/internal/objfile/elf.go b/src/cmd/internal/objfile/elf.go
index c811460..d0957a2 100644
--- a/src/cmd/internal/objfile/elf.go
+++ b/src/cmd/internal/objfile/elf.go
@@ -102,6 +102,8 @@ func (f *elfFile) goarch() string {
 		return "ppc64"
 	case elf.EM_S390:
 		return "s390x"
+	case elf.EM_SPARCV9:
+		return "sparc64"
 	}
 	return ""
 }
diff --git a/src/cmd/internal/sys/arch.go b/src/cmd/internal/sys/arch.go
index 18accde..812c6ab 100644
--- a/src/cmd/internal/sys/arch.go
+++ b/src/cmd/internal/sys/arch.go
@@ -19,6 +19,7 @@ const (
 	MIPS64
 	PPC64
 	S390X
+	SPARC64
 )
 
 // Arch represents an individual architecture.
@@ -146,3 +147,13 @@ var ArchS390X = &Arch{
 	RegSize:   8,
 	MinLC:     2,
 }
+
+var ArchSPARC64 = &Arch{
+	Name:      "sparc64",
+	Family:    SPARC64,
+	ByteOrder: binary.BigEndian,
+	IntSize:   8,
+	PtrSize:   8,
+	RegSize:   8,
+	MinLC:     4,
+}
diff --git a/src/cmd/link/internal/ld/data.go b/src/cmd/link/internal/ld/data.go
index 57a0dad..0bd431b 100644
--- a/src/cmd/link/internal/ld/data.go
+++ b/src/cmd/link/internal/ld/data.go
@@ -408,7 +408,7 @@ func relocsym(s *LSym) {
 				r.Xsym = r.Sym
 				r.Xadd = r.Add
 				o = 0
-				if SysArch.Family != sys.AMD64 {
+				if !SysArch.InFamily(sys.AMD64, sys.SPARC64) {
 					o = r.Add
 				}
 				break
@@ -442,7 +442,7 @@ func relocsym(s *LSym) {
 				r.Xsym = r.Sym
 				r.Xadd = r.Add
 				o = 0
-				if SysArch.Family != sys.AMD64 {
+				if !SysArch.InFamily(sys.AMD64, sys.SPARC64) {
 					o = r.Add
 				}
 				break
@@ -469,7 +469,7 @@ func relocsym(s *LSym) {
 
 				o = r.Xadd
 				if Iself {
-					if SysArch.Family == sys.AMD64 {
+					if SysArch.InFamily(sys.AMD64, sys.SPARC64) {
 						o = 0
 					}
 				} else if HEADTYPE == obj.Hdarwin {
@@ -519,7 +519,7 @@ func relocsym(s *LSym) {
 				r.Xadd = r.Add + Symaddr(r.Sym) - int64(r.Sym.Sect.Vaddr)
 				o = r.Xadd
 				rs = r.Xsym
-				if Iself && SysArch.Family == sys.AMD64 {
+				if Iself && SysArch.InFamily(sys.AMD64, sys.SPARC64) {
 					o = 0
 				}
 				break
@@ -551,7 +551,7 @@ func relocsym(s *LSym) {
 
 				o = r.Xadd
 				if Iself {
-					if SysArch.Family == sys.AMD64 {
+					if SysArch.InFamily(sys.AMD64, sys.SPARC64) {
 						o = 0
 					}
 				} else if HEADTYPE == obj.Hdarwin {
@@ -1857,7 +1857,7 @@ func dodataSect(symn int, syms []*LSym) (result []*LSym, maxAlign int32) {
 	}
 
 	if Iself && symn == obj.SELFROSECT {
-		// Make .rela and .rela.plt contiguous, the ELF ABI requires this
+		// Make .rela, .rela.plt, .rela.got contiguous, the ELF ABI requires this
 		// and Solaris actually cares.
 		reli, plti := -1, -1
 		for i, s := range syms {
@@ -1868,6 +1868,7 @@ func dodataSect(symn int, syms []*LSym) (result []*LSym, maxAlign int32) {
 				reli = i
 			}
 		}
+
 		if reli >= 0 && plti >= 0 && plti != reli+1 {
 			var first, second int
 			if plti > reli {
@@ -1880,6 +1881,42 @@ func dodataSect(symn int, syms []*LSym) (result []*LSym, maxAlign int32) {
 			syms[first+0] = rel
 			syms[first+1] = plt
 		}
+
+		reli, plti, goti := -1, -1, -1
+		for i, s := range syms {
+			switch s.Name {
+			case ".rel.got", ".rela.got":
+				goti = i
+			case ".rel.plt", ".rela.plt":
+				plti = i
+			case ".rel", ".rela":
+				reli = i
+			}
+		}
+
+		if plti >= 0 && goti >= 0 && goti != plti+1 {
+			var first, second int
+			if goti > plti {
+				first, second = plti, goti
+			} else {
+				first, second = goti, plti
+			}
+			plt, got := syms[plti], syms[goti]
+			copy(syms[first+2:], syms[first+1:second])
+			syms[first+0] = plt
+			syms[first+1] = got
+		} else if reli >= 0 && plti < 0 && goti >= 0 && goti != reli+1 {
+			var first, second int
+			if goti > reli {
+				first, second = reli, goti
+			} else {
+				first, second = goti, reli
+			}
+			rel, got := syms[reli], syms[goti]
+			copy(syms[first+2:], syms[first+1:second])
+			syms[first+0] = rel
+			syms[first+1] = got
+		}
 	}
 
 	return syms, maxAlign
@@ -1976,7 +2013,12 @@ func address() {
 		// rodata and executable text.
 		va = uint64(Rnd(int64(va), int64(INITRND)))
 
-		Segrodata.Rwx = 04
+		if SysArch.Family == sys.SPARC64 {
+			// sparc ABI requires executable data segment
+			Segrodata.Rwx = 05
+		} else {
+			Segrodata.Rwx = 04
+		}
 		Segrodata.Vaddr = va
 		Segrodata.Fileoff = va - Segtext.Vaddr + Segtext.Fileoff
 		Segrodata.Filelen = 0
@@ -1991,7 +2033,12 @@ func address() {
 	}
 
 	va = uint64(Rnd(int64(va), int64(INITRND)))
-	Segdata.Rwx = 06
+	if SysArch.Family == sys.SPARC64 {
+		// sparc ABI requires executable data segment
+		Segdata.Rwx = 07
+	} else {
+		Segdata.Rwx = 06
+	}
 	Segdata.Vaddr = va
 	Segdata.Fileoff = va - Segtext.Vaddr + Segtext.Fileoff
 	Segdata.Filelen = 0
diff --git a/src/cmd/link/internal/ld/dwarf.go b/src/cmd/link/internal/ld/dwarf.go
index fa7105f..2618509 100644
--- a/src/cmd/link/internal/ld/dwarf.go
+++ b/src/cmd/link/internal/ld/dwarf.go
@@ -16,6 +16,7 @@ package ld
 
 import (
 	"cmd/internal/obj"
+	"cmd/internal/sys"
 	"fmt"
 	"log"
 	"os"
@@ -174,6 +175,7 @@ var abbrevs = [DW_NABRV]DWAbbrev{
 			{DW_AT_low_pc, DW_FORM_addr},
 			{DW_AT_high_pc, DW_FORM_addr},
 			{DW_AT_external, DW_FORM_flag},
+			{DW_AT_frame_base, DW_FORM_block1},
 		},
 	},
 
@@ -1383,6 +1385,7 @@ func putpclcdelta(s *LSym, delta_pc int64, delta_lc int64) {
 	Adduint8(Ctxt, s, DW_LNS_copy)
 }
 
+// TODO(shawn): replaced by newfboffsetattr?
 func newcfaoffsetattr(die *DWDie, offs int32) {
 	var block [20]byte
 	b := append(block[:0], DW_OP_call_frame_cfa)
@@ -1396,6 +1399,13 @@ func newcfaoffsetattr(die *DWDie, offs int32) {
 	newattr(die, DW_AT_location, DW_CLS_BLOCK, int64(len(b)), b)
 }
 
+func newfboffsetattr(die *DWDie, offs int32) {
+	var block [20]byte
+	b := append(block[:0], DW_OP_fbreg)
+	b = appendSleb128(b, int64(offs))
+	newattr(die, DW_AT_location, DW_CLS_BLOCK, int64(len(b)), b)
+}
+
 func mkvarname(name string, da int) string {
 	buf := fmt.Sprintf("%s#%d", name, da)
 	n := buf
@@ -1507,6 +1517,7 @@ func writelines(prev *LSym) *LSym {
 		if s.Version == 0 {
 			newattr(dwfunc, DW_AT_external, DW_CLS_FLAG, 1, 0)
 		}
+		newattr(dwfunc, DW_AT_frame_base, DW_CLS_BLOCK, 1, []byte{DW_OP_call_frame_cfa})
 
 		if s.FuncInfo == nil {
 			continue
@@ -1589,7 +1600,7 @@ func writelines(prev *LSym) *LSym {
 			}
 
 			dwvar := newdie(dwfunc, dt, n, 0)
-			newcfaoffsetattr(dwvar, int32(offs))
+			newfboffsetattr(dwvar, int32(offs))
 			newrefattr(dwvar, DW_AT_type, defgotype(a.Gotype))
 
 			// push dwvar down dwfunc->child to preserve order
@@ -1649,6 +1660,22 @@ func appendPCDeltaCFA(b []byte, deltapc, cfa int64) []byte {
 	return b
 }
 
+// appendPCDeltaCFA_SPARC appends per-PC CFA deltas to b and returns the final slice.
+func appendPCDeltaCFA_SPARC(b []byte, deltapc, cfa int64) []byte {
+	if deltapc < 176 {
+		b = append(b, DW_CFA_advance_loc4)
+		b = Thearch.Append32(b, uint32(deltapc))
+		return b
+	}
+	b = append(b, DW_CFA_GNU_window_save)
+	b = append(b, DW_CFA_register)
+	b = appendUleb128(b, uint64(15))
+	b = appendUleb128(b, uint64(31))
+	b = append(b, DW_CFA_def_cfa_register)
+	b = appendUleb128(b, uint64(30))
+	return b
+}
+
 func writeframes(prev *LSym) *LSym {
 	if framesec == nil {
 		framesec = Linklookup(Ctxt, ".debug_frame", 0)
@@ -1674,7 +1701,7 @@ func writeframes(prev *LSym) *LSym {
 	Adduint8(Ctxt, fs, DW_CFA_def_cfa)        // Set the current frame address..
 	uleb128put(fs, int64(Thearch.Dwarfregsp)) // ...to use the value in the platform's SP register (defined in l.go)...
 	if haslinkregister() {
-		uleb128put(fs, int64(0)) // ...plus a 0 offset.
+		uleb128put(fs, int64(0) + int64(Thearch.StackBias)) // ...plus a 0 offset.
 
 		Adduint8(Ctxt, fs, DW_CFA_same_value) // The platform's link register is unchanged during the prologue.
 		uleb128put(fs, int64(Thearch.Dwarfreglr))
@@ -1738,7 +1765,10 @@ func writeframes(prev *LSym) *LSym {
 					deltaBuf = append(deltaBuf, DW_CFA_same_value)
 					deltaBuf = appendUleb128(deltaBuf, uint64(Thearch.Dwarfreglr))
 				}
-				deltaBuf = appendPCDeltaCFA(deltaBuf, int64(nextpc)-int64(pcsp.pc), int64(pcsp.value))
+				if SysArch.Family == sys.SPARC64 {
+					deltaBuf = appendPCDeltaCFA_SPARC(deltaBuf, int64(nextpc)-int64(pcsp.pc), int64(pcsp.value)) } else {
+					deltaBuf = appendPCDeltaCFA(deltaBuf, int64(nextpc)-int64(pcsp.pc), int64(pcsp.value))
+				}
 			} else {
 				deltaBuf = appendPCDeltaCFA(deltaBuf, int64(nextpc)-int64(pcsp.pc), int64(SysArch.PtrSize)+int64(pcsp.value))
 			}
diff --git a/src/cmd/link/internal/ld/dwarf_defs.go b/src/cmd/link/internal/ld/dwarf_defs.go
index c52879c..5715787 100644
--- a/src/cmd/link/internal/ld/dwarf_defs.go
+++ b/src/cmd/link/internal/ld/dwarf_defs.go
@@ -513,4 +513,6 @@ const (
 	DW_CFA_advance_loc = 0x1 << 6 // +delta
 	DW_CFA_offset      = 0x2 << 6 // +register (ULEB128 offset)
 	DW_CFA_restore     = 0x3 << 6 // +register
+
+	DW_CFA_GNU_window_save = 0x2d
 )
diff --git a/src/cmd/link/internal/ld/elf.go b/src/cmd/link/internal/ld/elf.go
index 39d3609..dedfec5 100644
--- a/src/cmd/link/internal/ld/elf.go
+++ b/src/cmd/link/internal/ld/elf.go
@@ -639,62 +639,95 @@ const (
 	R_PPC64_REL16_HI          = 251
 	R_PPC64_REL16_HA          = 252
 
-	R_SPARC_NONE     = 0
-	R_SPARC_8        = 1
-	R_SPARC_16       = 2
-	R_SPARC_32       = 3
-	R_SPARC_DISP8    = 4
-	R_SPARC_DISP16   = 5
-	R_SPARC_DISP32   = 6
-	R_SPARC_WDISP30  = 7
-	R_SPARC_WDISP22  = 8
-	R_SPARC_HI22     = 9
-	R_SPARC_22       = 10
-	R_SPARC_13       = 11
-	R_SPARC_LO10     = 12
-	R_SPARC_GOT10    = 13
-	R_SPARC_GOT13    = 14
-	R_SPARC_GOT22    = 15
-	R_SPARC_PC10     = 16
-	R_SPARC_PC22     = 17
-	R_SPARC_WPLT30   = 18
-	R_SPARC_COPY     = 19
-	R_SPARC_GLOB_DAT = 20
-	R_SPARC_JMP_SLOT = 21
-	R_SPARC_RELATIVE = 22
-	R_SPARC_UA32     = 23
-	R_SPARC_PLT32    = 24
-	R_SPARC_HIPLT22  = 25
-	R_SPARC_LOPLT10  = 26
-	R_SPARC_PCPLT32  = 27
-	R_SPARC_PCPLT22  = 28
-	R_SPARC_PCPLT10  = 29
-	R_SPARC_10       = 30
-	R_SPARC_11       = 31
-	R_SPARC_64       = 32
-	R_SPARC_OLO10    = 33
-	R_SPARC_HH22     = 34
-	R_SPARC_HM10     = 35
-	R_SPARC_LM22     = 36
-	R_SPARC_PC_HH22  = 37
-	R_SPARC_PC_HM10  = 38
-	R_SPARC_PC_LM22  = 39
-	R_SPARC_WDISP16  = 40
-	R_SPARC_WDISP19  = 41
-	R_SPARC_GLOB_JMP = 42
-	R_SPARC_7        = 43
-	R_SPARC_5        = 44
-	R_SPARC_6        = 45
-	R_SPARC_DISP64   = 46
-	R_SPARC_PLT64    = 47
-	R_SPARC_HIX22    = 48
-	R_SPARC_LOX10    = 49
-	R_SPARC_H44      = 50
-	R_SPARC_M44      = 51
-	R_SPARC_L44      = 52
-	R_SPARC_REGISTER = 53
-	R_SPARC_UA64     = 54
-	R_SPARC_UA16     = 55
+	R_SPARC_NONE          = 0
+	R_SPARC_8             = 1
+	R_SPARC_16            = 2
+	R_SPARC_32            = 3
+	R_SPARC_DISP8         = 4
+	R_SPARC_DISP16        = 5
+	R_SPARC_DISP32        = 6
+	R_SPARC_WDISP30       = 7
+	R_SPARC_WDISP22       = 8
+	R_SPARC_HI22          = 9
+	R_SPARC_22            = 10
+	R_SPARC_13            = 11
+	R_SPARC_LO10          = 12
+	R_SPARC_GOT10         = 13
+	R_SPARC_GOT13         = 14
+	R_SPARC_GOT22         = 15
+	R_SPARC_PC10          = 16
+	R_SPARC_PC22          = 17
+	R_SPARC_WPLT30        = 18
+	R_SPARC_COPY          = 19
+	R_SPARC_GLOB_DAT      = 20
+	R_SPARC_JMP_SLOT      = 21
+	R_SPARC_RELATIVE      = 22
+	R_SPARC_UA32          = 23
+	R_SPARC_PLT32         = 24
+	R_SPARC_HIPLT22       = 25
+	R_SPARC_LOPLT10       = 26
+	R_SPARC_PCPLT32       = 27
+	R_SPARC_PCPLT22       = 28
+	R_SPARC_PCPLT10       = 29
+	R_SPARC_10            = 30
+	R_SPARC_11            = 31
+	R_SPARC_64            = 32
+	R_SPARC_OLO10         = 33
+	R_SPARC_HH22          = 34
+	R_SPARC_HM10          = 35
+	R_SPARC_LM22          = 36
+	R_SPARC_PC_HH22       = 37
+	R_SPARC_PC_HM10       = 38
+	R_SPARC_PC_LM22       = 39
+	R_SPARC_WDISP16       = 40
+	R_SPARC_WDISP19       = 41
+	R_SPARC_GLOB_JMP      = 42
+	R_SPARC_7             = 43
+	R_SPARC_5             = 44
+	R_SPARC_6             = 45
+	R_SPARC_DISP64        = 46
+	R_SPARC_PLT64         = 47
+	R_SPARC_HIX22         = 48
+	R_SPARC_LOX10         = 49
+	R_SPARC_H44           = 50
+	R_SPARC_M44           = 51
+	R_SPARC_L44           = 52
+	R_SPARC_REGISTER      = 53
+	R_SPARC_UA64          = 54
+	R_SPARC_UA16          = 55
+	R_SPARC_TLS_GD_HI22   = 56
+	R_SPARC_TLS_GD_LO10   = 57
+	R_SPARC_TLS_GD_ADD    = 58
+	R_SPARC_TLS_GD_CALL   = 59
+	R_SPARC_TLS_LDM_HI22  = 60
+	R_SPARC_TLS_LDM_LO10  = 61
+	R_SPARC_TLS_LDM_ADD   = 62
+	R_SPARC_TLS_LDM_CALL  = 63
+	R_SPARC_TLS_LDO_HIX22 = 64
+	R_SPARC_TLS_LDO_LOX10 = 65
+	R_SPARC_TLS_LDO_ADD   = 66
+	R_SPARC_TLS_IE_HI22   = 67
+	R_SPARC_TLS_IE_LO10   = 68
+	R_SPARC_TLS_IE_LD     = 69
+	R_SPARC_TLS_IE_LDX    = 70
+	R_SPARC_TLS_IE_ADD    = 71
+	R_SPARC_TLS_LE_HIX22  = 72
+	R_SPARC_TLS_LE_LOX10  = 73
+	R_SPARC_TLS_DTPMOD32  = 74
+	R_SPARC_TLS_DTPMOD64  = 75
+	R_SPARC_TLS_DTPOFF32  = 76
+	R_SPARC_TLS_DTPOFF64  = 77
+	R_SPARC_TLS_TPOFF32   = 78
+	R_SPARC_TLS_TPOFF64   = 79
+	R_SPARC_GOTDATA_HIX22 = 80
+	R_SPARC_GOTDATA_LOX10 = 81
+	R_SPARC_GOTDATA_OP_HIX22 = 82
+	R_SPARC_GOTDATA_OP_LOX10 = 83
+	R_SPARC_GOTDATA_OP = 84
+	R_SPARC_H34 = 85
+	R_SPARC_SIZE32 = 86
+	R_SPARC_SIZE64 = 87
+	R_SPARC_WDISP10 = 88
 
 	R_390_NONE        = 0
 	R_390_8           = 1
@@ -916,7 +949,7 @@ var buildinfo []byte
 func Elfinit() {
 	Iself = true
 
-	if SysArch.InFamily(sys.AMD64, sys.ARM64, sys.MIPS64, sys.PPC64, sys.S390X) {
+	if SysArch.InFamily(sys.AMD64, sys.ARM64, sys.MIPS64, sys.PPC64, sys.S390X, sys.SPARC64) {
 		elfRelType = ".rela"
 	} else {
 		elfRelType = ".rel"
@@ -931,7 +964,7 @@ func Elfinit() {
 			ehdr.flags = 2 /* Version 2 ABI */
 		}
 		fallthrough
-	case sys.AMD64, sys.ARM64, sys.MIPS64:
+	case sys.AMD64, sys.ARM64, sys.MIPS64, sys.SPARC64:
 		if SysArch.Family == sys.MIPS64 {
 			ehdr.flags = 0x20000000 /* MIPS 3 */
 		}
@@ -1919,12 +1952,17 @@ func doelf() {
 		if SysArch.Family == sys.PPC64 {
 			Addstring(shstrtab, ".glink")
 		}
-		Addstring(shstrtab, ".got.plt")
+		if !SysArch.InFamily(sys.PPC64, sys.SPARC64) {
+			Addstring(shstrtab, ".got.plt")
+		}
 		Addstring(shstrtab, ".dynamic")
 		Addstring(shstrtab, ".dynsym")
 		Addstring(shstrtab, ".dynstr")
 		Addstring(shstrtab, elfRelType)
 		Addstring(shstrtab, elfRelType+".plt")
+		if SysArch.Family == sys.SPARC64 {
+			Addstring(shstrtab, elfRelType+".got")
+		}
 
 		Addstring(shstrtab, ".plt")
 		Addstring(shstrtab, ".gnu.version")
@@ -1955,6 +1993,13 @@ func doelf() {
 		s = Linklookup(Ctxt, elfRelType, 0)
 		s.Attr |= AttrReachable
 		s.Type = obj.SELFROSECT
+		// TODO(shawn): should this simply assign SysArch.PtrSize or
+		// differentiate based on elf64?
+		if SysArch.InFamily (sys.AMD64, sys.ARM64, sys.MIPS64, sys.PPC64, sys.S390X, sys.SPARC64) {
+			s.Align = 8
+		} else {
+			s.Align = 4
+		}
 
 		/* global offset table */
 		s = Linklookup(Ctxt, ".got", 0)
@@ -1975,15 +2020,17 @@ func doelf() {
 		s.Attr |= AttrReachable
 		s.Type = obj.SELFROSECT
 
-		s = Linklookup(Ctxt, ".got.plt", 0)
-		s.Attr |= AttrReachable
-		s.Type = obj.SELFSECT // writable
+		if !SysArch.InFamily(sys.PPC64, sys.SPARC64) {
+			s = Linklookup(Ctxt, ".got.plt", 0)
+			s.Attr |= AttrReachable
+			s.Type = obj.SELFSECT // writable
+		}
 
 		s = Linklookup(Ctxt, ".plt", 0)
 
 		s.Attr |= AttrReachable
-		if SysArch.Family == sys.PPC64 {
-			// In the ppc64 ABI, .plt is a data section
+		if SysArch.InFamily(sys.PPC64, sys.SPARC64) {
+			// In the ppc64/sparc64 ABI, .plt is a data section
 			// written by the dynamic linker.
 			s.Type = obj.SELFSECT
 		} else {
@@ -1996,6 +2043,12 @@ func doelf() {
 		s.Attr |= AttrReachable
 		s.Type = obj.SELFROSECT
 
+		if SysArch.Family == sys.SPARC64 {
+			s = Linklookup(Ctxt, elfRelType+".got", 0)
+			s.Attr |= AttrReachable
+			s.Type = obj.SELFROSECT
+		}
+
 		s = Linklookup(Ctxt, ".gnu.version", 0)
 		s.Attr |= AttrReachable
 		s.Type = obj.SELFROSECT
@@ -2013,6 +2066,9 @@ func doelf() {
 		/*
 		 * .dynamic table
 		 */
+		if SysArch.Family == sys.SPARC64 && HEADTYPE == obj.Hsolaris {
+			Elfwritedynent(Linklookup(Ctxt, ".dynamic", 0), DT_NEEDED, uint64(Addstring(Linklookup(Ctxt, ".dynstr", 0), "libc.so.1")))
+		}
 		elfwritedynentsym(s, DT_HASH, Linklookup(Ctxt, ".hash", 0))
 
 		elfwritedynentsym(s, DT_SYMTAB, Linklookup(Ctxt, ".dynsym", 0))
@@ -2037,7 +2093,7 @@ func doelf() {
 			Elfwritedynent(s, DT_RUNPATH, uint64(Addstring(dynstr, rpath.val)))
 		}
 
-		if SysArch.Family == sys.PPC64 {
+		if SysArch.InFamily(sys.PPC64, sys.SPARC64) {
 			elfwritedynentsym(s, DT_PLTGOT, Linklookup(Ctxt, ".plt", 0))
 		} else if SysArch.Family == sys.S390X {
 			elfwritedynentsym(s, DT_PLTGOT, Linklookup(Ctxt, ".got", 0))
@@ -2054,6 +2110,11 @@ func doelf() {
 		// DT_PLTRELSZ, and DT_JMPREL dynamic entries until after we know the
 		// size of .rel(a).plt section.
 		Elfwritedynent(s, DT_DEBUG, 0)
+
+		// Lazy-loading causes problems with Go stacks; tell the
+		// runtime linker all relocations for this object must be
+		// processed before returning control to the program.
+		Elfwritedynent(s, DT_BIND_NOW, 0)
 	}
 
 	if Buildmode == BuildmodeShared {
@@ -2141,6 +2202,8 @@ func Asmbelf(symo int64) {
 		eh.machine = EM_PPC64
 	case sys.S390X:
 		eh.machine = EM_S390
+	case sys.SPARC64:
+		eh.machine = EM_SPARCV9
 	}
 
 	elfreserve := int64(ELFRESERVE)
@@ -2375,6 +2438,12 @@ func Asmbelf(symo int64) {
 			sh.entsize = 16
 		} else if eh.machine == EM_S390 {
 			sh.entsize = 32
+		} else if eh.machine == EM_SPARCV9 {
+			// On sparc64, the plt is rewritten by the dynamic
+			// linker based on the existing entries
+			sh.flags |= SHF_WRITE
+			sh.entsize = 32
+			sh.addralign = 256
 		} else if eh.machine == EM_PPC64 {
 			// On ppc64, this is just a table of addresses
 			// filled by the dynamic linker
@@ -2385,11 +2454,26 @@ func Asmbelf(symo int64) {
 		} else {
 			sh.entsize = 4
 		}
-		sh.addralign = sh.entsize
+
+		if eh.machine != EM_SPARCV9 {
+			sh.addralign = sh.entsize
+		}
+
 		shsym(sh, Linklookup(Ctxt, ".plt", 0))
 
+		if eh.machine == EM_SPARCV9 {
+			sh := elfshname(".rela.got")
+			sh.type_ = SHT_RELA
+			sh.flags = SHF_ALLOC
+			sh.entsize = ELF64RELASIZE
+			sh.addralign = uint64(SysArch.RegSize)
+			sh.link = uint32(elfshname(".dynsym").shnum)
+			sh.info = uint32(elfshname(".got").shnum)
+			shsym(sh, Linklookup(Ctxt, ".rela.got", 0))
+		}
+
 		// On ppc64, .got comes from the input files, so don't
-		// create it here, and .got.plt is not used.
+		// create it here
 		if eh.machine != EM_PPC64 {
 			sh := elfshname(".got")
 			sh.type_ = SHT_PROGBITS
@@ -2397,8 +2481,11 @@ func Asmbelf(symo int64) {
 			sh.entsize = uint64(SysArch.RegSize)
 			sh.addralign = uint64(SysArch.RegSize)
 			shsym(sh, Linklookup(Ctxt, ".got", 0))
+		}
 
-			sh = elfshname(".got.plt")
+		// On ppc64 and sparc64, .got.plt is not used.
+		if eh.machine != EM_PPC64 && eh.machine != EM_SPARCV9 {
+			sh := elfshname(".got.plt")
 			sh.type_ = SHT_PROGBITS
 			sh.flags = SHF_ALLOC + SHF_WRITE
 			sh.entsize = uint64(SysArch.RegSize)
@@ -2544,6 +2631,8 @@ elfobj:
 		eh.ident[EI_OSABI] = ELFOSABI_NETBSD
 	} else if HEADTYPE == obj.Hopenbsd {
 		eh.ident[EI_OSABI] = ELFOSABI_OPENBSD
+	} else if HEADTYPE == obj.Hsolaris {
+		eh.ident[EI_OSABI] = ELFOSABI_SOLARIS
 	} else if HEADTYPE == obj.Hdragonfly {
 		eh.ident[EI_OSABI] = ELFOSABI_NONE
 	}
diff --git a/src/cmd/link/internal/ld/ldelf.go b/src/cmd/link/internal/ld/ldelf.go
index af60a5c..0a7f366 100644
--- a/src/cmd/link/internal/ld/ldelf.go
+++ b/src/cmd/link/internal/ld/ldelf.go
@@ -594,6 +594,12 @@ func ldelf(f *bio.Reader, pkg string, length int64, pn string) {
 			Diag("%s: elf object but not s390x", pn)
 			return
 		}
+
+	case sys.SPARC64:
+		if elfobj.machine != ElfMachSparc9 || hdr.Ident[4] != ElfClass64 {
+			Diag("%s: elf object but not sparc64", pn)
+			return
+		}
 	}
 
 	// load section list into memory.
@@ -1132,11 +1138,12 @@ func relSize(pn string, elftype uint32) uint8 {
 	// performance.
 
 	const (
-		AMD64 = uint32(sys.AMD64)
-		ARM   = uint32(sys.ARM)
-		I386  = uint32(sys.I386)
-		PPC64 = uint32(sys.PPC64)
-		S390X = uint32(sys.S390X)
+		AMD64   = uint32(sys.AMD64)
+		ARM     = uint32(sys.ARM)
+		I386    = uint32(sys.I386)
+		PPC64   = uint32(sys.PPC64)
+		S390X   = uint32(sys.S390X)
+		SPARC64 = uint32(sys.SPARC64)
 	)
 
 	switch uint32(SysArch.Family) | elftype<<24 {
@@ -1144,6 +1151,93 @@ func relSize(pn string, elftype uint32) uint8 {
 		Diag("%s: unknown relocation type %d; compiled without -fpic?", pn, elftype)
 		fallthrough
 
+	case SPARC64 | R_SPARC_GOTDATA_OP<<24:
+		// These are "special" and don't technically have a size.
+		return 0
+
+	case SPARC64 | R_SPARC_8<<24,
+		SPARC64 | R_SPARC_DISP8<<24:
+		return 1
+
+	case SPARC64 | R_SPARC_16<<24,
+		SPARC64 | R_SPARC_DISP16<<24,
+		SPARC64 | R_SPARC_UA16<<24:
+		return 2
+
+	case SPARC64 | R_SPARC_32<<24,
+		SPARC64 | R_SPARC_DISP32<<24,
+		SPARC64 | R_SPARC_WDISP30<<24,
+		SPARC64 | R_SPARC_WDISP22<<24,
+		SPARC64 | R_SPARC_HI22<<24,
+		SPARC64 | R_SPARC_22<<24,
+		SPARC64 | R_SPARC_13<<24,
+		SPARC64 | R_SPARC_LO10<<24,
+		SPARC64 | R_SPARC_GOT10<<24,
+		SPARC64 | R_SPARC_GOT13<<24,
+		SPARC64 | R_SPARC_GOT22<<24,
+		SPARC64 | R_SPARC_PC10<<24,
+		SPARC64 | R_SPARC_PC22<<24,
+		SPARC64 | R_SPARC_WPLT30<<24,
+		SPARC64 | R_SPARC_UA32<<24,
+		SPARC64 | R_SPARC_PLT32<<24,
+		SPARC64 | R_SPARC_HIPLT22<<24,
+		SPARC64 | R_SPARC_LOPLT10<<24,
+		SPARC64 | R_SPARC_PCPLT32<<24,
+		SPARC64 | R_SPARC_PCPLT22<<24,
+		SPARC64 | R_SPARC_PCPLT10<<24,
+		SPARC64 | R_SPARC_10<<24,
+		SPARC64 | R_SPARC_11<<24,
+		SPARC64 | R_SPARC_OLO10<<24,
+		SPARC64 | R_SPARC_HH22<<24,
+		SPARC64 | R_SPARC_HM10<<24,
+		SPARC64 | R_SPARC_LM22<<24,
+		SPARC64 | R_SPARC_PC_HH22<<24,
+		SPARC64 | R_SPARC_PC_HM10<<24,
+		SPARC64 | R_SPARC_PC_LM22<<24,
+		SPARC64 | R_SPARC_WDISP16<<24,
+		SPARC64 | R_SPARC_WDISP19<<24,
+		SPARC64 | R_SPARC_7<<24,
+		SPARC64 | R_SPARC_5<<24,
+		SPARC64 | R_SPARC_6<<24,
+		SPARC64 | R_SPARC_HIX22<<24,
+		SPARC64 | R_SPARC_LOX10<<24,
+		SPARC64 | R_SPARC_H44<<24,
+		SPARC64 | R_SPARC_M44<<24,
+		SPARC64 | R_SPARC_L44<<24,
+		SPARC64 | R_SPARC_TLS_GD_HI22<<24,
+		SPARC64 | R_SPARC_TLS_GD_LO10<<24,
+		SPARC64 | R_SPARC_TLS_LDM_HI22<<24,
+		SPARC64 | R_SPARC_TLS_LDM_LO10<<24,
+		SPARC64 | R_SPARC_TLS_LDO_HIX22<<24,
+		SPARC64 | R_SPARC_TLS_LDO_LOX10<<24,
+		SPARC64 | R_SPARC_TLS_IE_HI22<<24,
+		SPARC64 | R_SPARC_TLS_IE_LO10<<24,
+		SPARC64 | R_SPARC_TLS_LE_HIX22<<24,
+		SPARC64 | R_SPARC_TLS_LE_LOX10<<24,
+		SPARC64 | R_SPARC_TLS_DTPMOD32<<24,
+		SPARC64 | R_SPARC_TLS_DTPOFF32<<24,
+		SPARC64 | R_SPARC_TLS_TPOFF32<<24,
+		SPARC64 | R_SPARC_GOTDATA_HIX22<<24,
+		SPARC64 | R_SPARC_GOTDATA_LOX10<<24,
+		SPARC64 | R_SPARC_GOTDATA_OP_HIX22<<24,
+		SPARC64 | R_SPARC_GOTDATA_OP_LOX10<<24,
+		SPARC64 | R_SPARC_H34<<24,
+		SPARC64 | R_SPARC_SIZE32<<24,
+		SPARC64 | R_SPARC_WDISP10<<24:
+		return 4
+
+	case SPARC64 | R_SPARC_GLOB_DAT<<24,
+		SPARC64 | R_SPARC_RELATIVE<<24,
+		SPARC64 | R_SPARC_64<<24,
+		SPARC64 | R_SPARC_DISP64<<24,
+		SPARC64 | R_SPARC_PLT64<<24,
+		SPARC64 | R_SPARC_UA64<<24,
+		SPARC64 | R_SPARC_TLS_DTPMOD64<<24,
+		SPARC64 | R_SPARC_TLS_DTPOFF64<<24,
+		SPARC64 | R_SPARC_TLS_TPOFF64<<24,
+		SPARC64 | R_SPARC_SIZE64<<24:
+		return 8
+
 	case S390X | R_390_8<<24:
 		return 1
 
diff --git a/src/cmd/link/internal/ld/lib.go b/src/cmd/link/internal/ld/lib.go
index 14f4fa9..cfaa378 100644
--- a/src/cmd/link/internal/ld/lib.go
+++ b/src/cmd/link/internal/ld/lib.go
@@ -89,6 +89,7 @@ type Arch struct {
 	Minalign         int
 	Dwarfregsp       int
 	Dwarfreglr       int
+	StackBias        int
 	Linuxdynld       string
 	Freebsddynld     string
 	Netbsddynld      string
@@ -563,6 +564,11 @@ func loadlib() {
 			Linkmode = LinkExternal
 		}
 
+		// cgo on SPARC64 requires external linking.
+		if SysArch.InFamily(sys.SPARC64) && iscgo {
+			Linkmode = LinkExternal
+		}
+
 		// Force external linking for msan.
 		if flag_msan != 0 {
 			Linkmode = LinkExternal
@@ -1122,7 +1128,7 @@ func hostlink() {
 		argv = append(argv, "-shared")
 	}
 
-	if Iself && DynlinkingGo() {
+	if Iself && (DynlinkingGo() || goos == "solaris") {
 		// We force all symbol resolution to be done at program startup
 		// because lazy PLT resolution can use large amounts of stack at
 		// times we cannot allow it to do so.
@@ -1131,7 +1137,9 @@ func hostlink() {
 		// Do not let the host linker generate COPY relocations. These
 		// can move symbols out of sections that rely on stable offsets
 		// from the beginning of the section (like STYPE).
-		argv = append(argv, "-Wl,-znocopyreloc")
+		if goos != "solaris" {
+			argv = append(argv, "-Wl,-znocopyreloc")
+		}
 
 		if SysArch.InFamily(sys.ARM, sys.ARM64) {
 			// On ARM, the GNU linker will generate COPY relocations
@@ -1826,7 +1834,7 @@ func stkcheck(up *Chain, depth int) int {
 			r = &s.R[ri]
 			switch r.Type {
 			// Direct call.
-			case obj.R_CALL, obj.R_CALLARM, obj.R_CALLARM64, obj.R_CALLPOWER, obj.R_CALLMIPS:
+			case obj.R_CALL, obj.R_CALLARM, obj.R_CALLARM64, obj.R_CALLPOWER, obj.R_CALLMIPS, obj.R_CALLSPARC64:
 				ch.limit = int(int32(limit) - pcsp.value - int32(callsize()))
 				ch.sym = r.Sym
 				if stkcheck(&ch, depth+1) < 0 {
@@ -1976,7 +1984,10 @@ func genasmsym(put func(*LSym, string, int, int64, int64, int, *LSym)) {
 			obj.SINITARR,
 			obj.SDATA,
 			obj.SNOPTRDATA,
+			obj.SELFSECT,
 			obj.SELFROSECT,
+			obj.SELFRXSECT,
+			obj.SELFGOT,
 			obj.SMACHOGOT,
 			obj.STYPE,
 			obj.SSTRING,
@@ -2164,7 +2175,7 @@ func callgraph() {
 			if r.Sym == nil {
 				continue
 			}
-			if (r.Type == obj.R_CALL || r.Type == obj.R_CALLARM || r.Type == obj.R_CALLPOWER || r.Type == obj.R_CALLMIPS) && r.Sym.Type == obj.STEXT {
+			if (r.Type == obj.R_CALL || r.Type == obj.R_CALLARM || r.Type == obj.R_CALLPOWER || r.Type == obj.R_CALLMIPS || r.Type == obj.R_CALLSPARC64) && r.Sym.Type == obj.STEXT {
 				fmt.Fprintf(Bso, "%s calls %s\n", s.Name, r.Sym.Name)
 			}
 		}
diff --git a/src/cmd/link/internal/ld/link.go b/src/cmd/link/internal/ld/link.go
index 9bab68b..4ba66f8 100644
--- a/src/cmd/link/internal/ld/link.go
+++ b/src/cmd/link/internal/ld/link.go
@@ -196,6 +196,9 @@ func (ctxt *Link) FixedFrameSize() int64 {
 		// PIC code on ppc64le requires 32 bytes of stack, and it's easier to
 		// just use that much stack always on ppc64x.
 		return int64(4 * ctxt.Arch.PtrSize)
+	case sys.SPARC64:
+		// SPARC64 always requires 176 bytes of stack.
+		return int64(16*8 + 6*8)
 	default:
 		return int64(ctxt.Arch.PtrSize)
 	}
diff --git a/src/cmd/link/internal/ld/symtab.go b/src/cmd/link/internal/ld/symtab.go
index 06d7792..cf6a37e 100644
--- a/src/cmd/link/internal/ld/symtab.go
+++ b/src/cmd/link/internal/ld/symtab.go
@@ -145,7 +145,8 @@ func putelfsym(x *LSym, s string, t int, addr int64, size int64, ver int, go_ *L
 	// mark all Go symbols local (not global) in the final executable.
 	// But when we're dynamically linking, we need all those global symbols.
 	if !DynlinkingGo() && Linkmode == LinkExternal && !x.Attr.CgoExportStatic() && elfshnum != SHN_UNDEF {
-		bind = STB_LOCAL
+		// TODO(aram): re-enable
+		//bind = STB_LOCAL
 	}
 
 	if Linkmode == LinkExternal && elfshnum != SHN_UNDEF {
@@ -319,7 +320,7 @@ func symtab() {
 	dosymtype()
 
 	// Define these so that they'll get put into the symbol table.
-	// data.c:/^address will provide the actual values.
+	// data.go:/^address will provide the actual values.
 	xdefine("runtime.text", obj.STEXT, 0)
 
 	xdefine("runtime.etext", obj.STEXT, 0)
diff --git a/src/cmd/link/internal/sparc64/asm.go b/src/cmd/link/internal/sparc64/asm.go
new file mode 100644
index 0000000..aeb099f
--- /dev/null
+++ b/src/cmd/link/internal/sparc64/asm.go
@@ -0,0 +1,506 @@
+// Inferno utils/5l/asm.c
+// http://code.google.com/p/inferno-os/source/browse/utils/5l/asm.c
+//
+//	Copyright © 1994-1999 Lucent Technologies Inc.  All rights reserved.
+//	Portions Copyright © 1995-1997 C H Forsyth (forsyth@terzarima.net)
+//	Portions Copyright © 1997-1999 Vita Nuova Limited
+//	Portions Copyright © 2000-2007 Vita Nuova Holdings Limited (www.vitanuova.com)
+//	Portions Copyright © 2004,2006 Bruce Ellis
+//	Portions Copyright © 2005-2007 C H Forsyth (forsyth@terzarima.net)
+//	Revisions Copyright © 2000-2007 Lucent Technologies Inc. and others
+//	Portions Copyright © 2009 The Go Authors.  All rights reserved.
+//
+// Permission is hereby granted, free of charge, to any person obtaining a copy
+// of this software and associated documentation files (the "Software"), to deal
+// in the Software without restriction, including without limitation the rights
+// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+// copies of the Software, and to permit persons to whom the Software is
+// furnished to do so, subject to the following conditions:
+//
+// The above copyright notice and this permission notice shall be included in
+// all copies or substantial portions of the Software.
+//
+// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL THE
+// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
+// THE SOFTWARE.
+
+package sparc64
+
+import (
+	"cmd/internal/obj"
+	"cmd/link/internal/ld"
+	"fmt"
+	"log"
+)
+
+func gentext() {
+	if !ld.DynlinkingGo() {
+		return
+	}
+	log.Fatalf("gentext() not implemented")
+}
+
+func addgotsym(s *ld.LSym) {
+	if s.Got >= 0 {
+		return
+	}
+
+	ld.Adddynsym(ld.Ctxt, s)
+	got := ld.Linklookup(ld.Ctxt, ".got", 0)
+	s.Got = int32(got.Size)
+	ld.Adduint64(ld.Ctxt, got, 0)
+
+	if ld.Iself {
+		rela := ld.Linklookup(ld.Ctxt, ".rela.got", 0)
+		ld.Addaddrplus(ld.Ctxt, rela, got, int64(s.Got))
+		ld.Adduint64(ld.Ctxt, rela, ld.ELF64_R_INFO(uint32(s.Dynid), ld.R_SPARC_GLOB_DAT))
+		ld.Adduint64(ld.Ctxt, rela, 0)
+	} else {
+		ld.Diag("addgotsym: unsupported binary format")
+	}
+}
+
+func adddynrela(rela *ld.LSym, s *ld.LSym, r *ld.Reloc) {
+	log.Fatalf("adddynrela not implemented")
+}
+
+func adddynrel(s *ld.LSym, r *ld.Reloc) {
+	targ := r.Sym
+	ld.Ctxt.Cursym = s
+
+	switch r.Type {
+	default:
+		if r.Type >= 256 {
+			ld.Diag("unexpected relocation type %d (%d)", r.Type, r.Type-256)
+			return
+		}
+	case 256 + ld.R_SPARC_PC10:
+		if targ.Type == obj.SDYNIMPORT {
+			ld.Diag("unexpected R_SPARC_PC10 relocation for dynamic symbol %s", targ.Name)
+		}
+		if targ.Type == 0 || targ.Type == obj.SXREF {
+			ld.Diag("unknown symbol %s in pcrel", targ.Name)
+		}
+		r.Type = obj.R_PCREL
+		r.Add += int64(r.Siz)
+		println("R_SPARC_PC10 relocation for symbol ", targ.Name)
+		return
+
+	case 256 + ld.R_SPARC_PC22:
+		if targ.Type == obj.SDYNIMPORT {
+			ld.Diag("unexpected R_SPARC_PC22 relocation for dynamic symbol %s", targ.Name)
+		}
+		if targ.Type == 0 || targ.Type == obj.SXREF {
+			ld.Diag("unknown symbol %s in pcrel", targ.Name)
+		}
+		r.Type = obj.R_PCREL
+		r.Add += int64(r.Siz)
+		println("R_SPARC_PC22 relocation for symbol ", targ.Name)
+		return
+
+	case 256 + ld.R_SPARC_WPLT30:
+		r.Add += int64(r.Siz)
+		if targ.Type == obj.SDYNIMPORT {
+			addpltsym(targ)
+			r.Sym = ld.Linklookup(ld.Ctxt, ".plt", 0)
+			r.Add += int64(targ.Plt)
+		}
+		r.Type = obj.R_CALLSPARC64
+		println("R_SPARC_WPLT30 relocation for symbol ", targ.Name)
+		fmt.Printf("r %#v\n", r)
+		return
+
+	// TODO(shawn):
+	// The R_SPARC_GOTDATA_OP* relocations are an optimized form of
+	// relocation that only supports a range of +/- 2 Gbytes.  We should
+	// eventually support these, but for now, simplify them to standard GOT
+	// relocations for simplicity in implementation.
+	case 256 + ld.R_SPARC_GOT10, 256 + ld.R_SPARC_GOTDATA_OP_LOX10:
+		addgotsym(targ)
+		r.Sym = ld.Linklookup(ld.Ctxt, ".got", 0)
+		r.Add += int64(targ.Got)
+		r.Type = obj.R_GOTOFF
+		println("R_SPARC_GOT10 relocation for symbol ", targ.Name)
+		return
+
+	case 256 + ld.R_SPARC_GOT22, 256 + ld.R_SPARC_GOTDATA_OP_HIX22:
+		addgotsym(targ)
+		r.Sym = ld.Linklookup(ld.Ctxt, ".got", 0)
+		r.Add += int64(targ.Got)
+		r.Type = obj.R_GOTOFF
+		println("R_SPARC_GOT22 relocation for symbol ", targ.Name)
+		return
+
+	case 256 + ld.R_SPARC_GOTDATA_OP:
+		r.Type = ld.R_SPARC_NONE
+		return
+	}
+
+	// Handle references to ELF symbols from our own object files.
+	if targ.Type != obj.SDYNIMPORT {
+		return
+	}
+
+	switch r.Type {
+	case obj.R_ADDRSPARC64HI, obj.R_ADDRSPARC64LO:
+		if s.Type == obj.STEXT && ld.Iself {
+			addpltsym(targ)
+			r.Sym = ld.Linkrlookup(ld.Ctxt, ".plt", 0)
+			r.Add += int64(targ.Plt)
+			return
+		}
+	}
+
+	ld.Ctxt.Cursym = s
+	ld.Diag("unsupported relocation for dynamic symbol %s (type=%d stype=%d)", targ.Name, r.Type, targ.Type)
+
+}
+
+func elfreloc1(r *ld.Reloc, sectoff int64) int {
+	ld.Thearch.Vput(uint64(sectoff))
+
+	elfsym := r.Xsym.ElfsymForReloc()
+	switch r.Type {
+	default:
+		return -1
+
+	case obj.R_ADDR:
+		switch r.Siz {
+		case 4:
+			ld.Thearch.Vput(ld.R_SPARC_32 | uint64(elfsym)<<32)
+		case 8:
+			ld.Thearch.Vput(ld.R_SPARC_64 | uint64(elfsym)<<32)
+		default:
+			return -1
+		}
+
+	case obj.R_ADDRSPARC64LO:
+		ld.Thearch.Vput(ld.R_SPARC_LM22 | uint64(elfsym)<<32)
+		ld.Thearch.Vput(uint64(r.Xadd))
+		ld.Thearch.Vput(uint64(sectoff + 4))
+		ld.Thearch.Vput(ld.R_SPARC_LO10 | uint64(elfsym)<<32)
+
+	case obj.R_ADDRSPARC64HI:
+		ld.Thearch.Vput(ld.R_SPARC_HH22 | uint64(elfsym)<<32)
+		ld.Thearch.Vput(uint64(r.Xadd))
+		ld.Thearch.Vput(uint64(sectoff + 4))
+		ld.Thearch.Vput(ld.R_SPARC_HM10 | uint64(elfsym)<<32)
+
+	case obj.R_SPARC64_TLS_LE:
+		ld.Thearch.Vput(ld.R_SPARC_TLS_LE_HIX22 | uint64(elfsym)<<32)
+		ld.Thearch.Vput(uint64(r.Xadd))
+		ld.Thearch.Vput(uint64(sectoff + 4))
+		ld.Thearch.Vput(ld.R_SPARC_TLS_LE_LOX10 | uint64(elfsym)<<32)
+
+	case obj.R_CALLSPARC64:
+		if r.Siz != 4 {
+			return -1
+		}
+		ld.Thearch.Vput(ld.R_SPARC_WDISP30 | uint64(elfsym)<<32)
+
+	case obj.R_PCREL:
+		if r.Siz != 4 {
+			return -1
+		}
+		ld.Thearch.Vput(ld.R_SPARC_RELATIVE | uint64(elfsym)<<32)
+	}
+	ld.Thearch.Vput(uint64(r.Xadd))
+
+	return 0
+}
+
+func elfsetupplt() {
+	plt := ld.Linklookup(ld.Ctxt, ".plt", 0)
+	if plt.Size == 0 {
+		// .plt entries are aligned at 32-byte boundaries, but the
+		// entire section at 256-byte boundaries.
+		plt.Align = 256
+
+		// Runtime linker will provide the initial plt; each entry is
+		// 32 bytes; reserve the first four entries for its use.
+		plt.Size = 4 * 32
+
+		// Create relocation table for .plt
+		rela := ld.Linklookup(ld.Ctxt, ".rela.plt", 0)
+		rela.Align = int32(ld.SysArch.RegSize)
+
+		// Create global offset table; first entry reserved for
+		// address of .dynamic section.
+		got := ld.Linklookup(ld.Ctxt, ".got", 0)
+		dyn := ld.Linklookup(ld.Ctxt, ".dynamic", 0)
+		ld.Addaddrplus(ld.Ctxt, got, dyn, 0)
+
+		// TODO(srwalker): pad end of plt with 10 entries for elfedit, etc.
+		// .strtab too; aslr-tagging, etc.
+	}
+}
+
+func machoreloc1(r *ld.Reloc, sectoff int64) int {
+	log.Fatalf("machoreloc1 not implemented")
+	return 0
+}
+
+func archreloc(r *ld.Reloc, s *ld.LSym, val *int64) int {
+	if ld.Linkmode == ld.LinkExternal {
+		switch r.Type {
+		default:
+			ld.Diag("unsupported LinkExternal archreloc %s", r.Type)
+			return -1
+
+		case obj.R_ADDRSPARC64LO, obj.R_ADDRSPARC64HI:
+			r.Done = 0
+
+			// set up addend for eventual relocation via outer symbol.
+			rs := r.Sym
+			r.Xadd = r.Add
+			for rs.Outer != nil {
+				r.Xadd += ld.Symaddr(rs) - ld.Symaddr(rs.Outer)
+				rs = rs.Outer
+			}
+
+			if rs.Type != obj.SHOSTOBJ && rs.Type != obj.SDYNIMPORT && rs.Sect == nil {
+				ld.Diag("missing section for %s", rs.Name)
+			}
+			r.Xsym = rs
+
+			return 0
+
+		case obj.R_CALLSPARC64, obj.R_SPARC64_TLS_LE:
+			r.Done = 0
+			r.Xsym = r.Sym
+			r.Xadd = r.Add
+			return 0
+		}
+	}
+
+	switch r.Type {
+	case obj.R_CONST:
+		*val = r.Add
+		return 0
+
+	case obj.R_ADDRSPARC64LO:
+		t := ld.Symaddr(r.Sym) + r.Add
+
+		o0 := uint32(*val >> 32)
+		o1 := uint32(*val)
+
+		o0 |= uint32(t) >> 10
+		o1 |= uint32(t) & 0x3ff
+
+		*val = int64(o0)<<32 | int64(o1)
+		return 0
+
+	case obj.R_ADDRSPARC64HI:
+		t := ld.Symaddr(r.Sym) + r.Add
+
+		o0 := uint32(*val >> 32)
+		o1 := uint32(*val)
+
+		o0 |= uint32(uint64(t)>>32) >> 10
+		o1 |= uint32(uint64(t)>>32) & 0x3ff
+
+		*val = int64(o0)<<32 | int64(o1)
+		return 0
+
+	case obj.R_CALLSPARC64:
+		t := (ld.Symaddr(r.Sym) + r.Add) - (s.Value + int64(r.Off))
+		if t > 1<<31-4 || t < -1<<31 {
+			ld.Diag("program too large, call relocation distance = %d", t)
+		}
+		*val |= (t >> 2) & 0x3fffffff
+		return 0
+
+	case obj.R_GOTOFF:
+		// TODO(shawn): GOT10 needs (val) & 0x3ff
+		// GOT22 needs (val) >> 10
+		*val = ld.Symaddr(r.Sym) + r.Add - ld.Symaddr(ld.Linklookup(ld.Ctxt, ".got", 0))
+		return 0
+
+	case obj.R_SPARC64_TLS_LE:
+		// The thread pointer points to the TCB, and then the
+		// address of the first TLS block follows, giving an
+		// offset of -16 for our static TLS variables.
+		v := r.Sym.Value - 16
+		if v < -4096 || 4095 < v {
+			ld.Diag("TLS offset out of range %d", v)
+		}
+		*val = (*val &^ 0x1fff) | (v & 0x1fff)
+		return 0
+	}
+
+	ld.Diag("unsupported LinkInternal archreloc %s", r.Type)
+	return -1
+}
+
+func archrelocvariant(r *ld.Reloc, s *ld.LSym, t int64) int64 {
+	log.Fatalf("unexpected relocation variant")
+	return -1
+}
+
+func addpltsym(s *ld.LSym) {
+	if s.Plt >= 0 {
+		return
+	}
+
+	ld.Adddynsym(ld.Ctxt, s)
+
+	if ld.Iself {
+		elfsetupplt()
+		plt := ld.Linkrlookup(ld.Ctxt, ".plt", 0)
+		rela := ld.Linkrlookup(ld.Ctxt, ".rela.plt", 0)
+
+		// Each of the first 32,767 procedure linkage table entries occupies
+		// 8 words (32 bytes), and must be aligned on a 32-byte boundary.
+		//
+		// NOTE: This only supports "near" .plt; entries beyond
+		// 32,767 are considered "far" and have a different format.
+
+		// The first eight bytes of each entry (excluding the initially
+		// reserved ones) should transfer control to the first or second
+		// reserved plt entry.  For our use, the second reserved entry (.PLT1)
+		// should always be the target.
+		//
+		// 03 00 00 80 sethi (.-.PLT0), %g1 sethi     %hi(0x20000), %g1
+		sethi := uint32(0x03000000)
+		sethi |= uint32(plt.Size)
+		ld.Adduint32(ld.Ctxt, plt, sethi)
+
+		// 30 6f ff e7 ba,a,pt   %xcc, .PLT1
+		ba := uint32(0x30680000)
+		ba |= (((-uint32(plt.Size)) + 32) >> 2) & ((1 << (19)) - 1)
+		ld.Adduint32(ld.Ctxt, plt, ba)
+
+		// Fill remaining 24 bytes with nop; these will be provided by the
+		// runtime linker.
+		ld.Adduint32(ld.Ctxt, plt, 0x01000000)
+		ld.Adduint32(ld.Ctxt, plt, 0x01000000)
+		ld.Adduint32(ld.Ctxt, plt, 0x01000000)
+		ld.Adduint32(ld.Ctxt, plt, 0x01000000)
+		ld.Adduint32(ld.Ctxt, plt, 0x01000000)
+		ld.Adduint32(ld.Ctxt, plt, 0x01000000)
+
+		// rela
+		// offset
+		ld.Addaddrplus(ld.Ctxt, rela, plt, plt.Size-32)
+		// info
+		ld.Adduint64(ld.Ctxt, rela, ld.ELF64_R_INFO(uint32(s.Dynid), ld.R_SPARC_JMP_SLOT))
+		// addend
+		ld.Adduint64(ld.Ctxt, rela, 0)
+
+		s.Plt = int32(plt.Size - 32)
+	} else {
+		ld.Diag("addpltsym: unsupported binary format")
+	}
+
+	return
+}
+
+func asmb() {
+	if ld.Debug['v'] != 0 {
+		fmt.Fprintf(ld.Bso, "%5.2f asmb\n", obj.Cputime())
+	}
+	ld.Bso.Flush()
+
+	if ld.Iself {
+		ld.Asmbelfsetup()
+	}
+
+	sect := ld.Segtext.Sect
+	ld.Cseek(int64(sect.Vaddr - ld.Segtext.Vaddr + ld.Segtext.Fileoff))
+	ld.CodeblkPad(int64(sect.Vaddr), int64(sect.Length), []byte{0x0, 0x0d, 0xea, 0xd1})
+	for sect = sect.Next; sect != nil; sect = sect.Next {
+		ld.Cseek(int64(sect.Vaddr - ld.Segtext.Vaddr + ld.Segtext.Fileoff))
+		ld.Datblk(int64(sect.Vaddr), int64(sect.Length))
+	}
+
+	if ld.Segrodata.Filelen > 0 {
+		if ld.Debug['v'] != 0 {
+			fmt.Fprintf(ld.Bso, "%5.2f rodatblk\n", obj.Cputime())
+		}
+		ld.Bso.Flush()
+
+		ld.Cseek(int64(ld.Segrodata.Fileoff))
+		ld.Datblk(int64(ld.Segrodata.Vaddr), int64(ld.Segrodata.Filelen))
+	}
+
+	if ld.Debug['v'] != 0 {
+		fmt.Fprintf(ld.Bso, "%5.2f datblk\n", obj.Cputime())
+	}
+	ld.Bso.Flush()
+
+	ld.Cseek(int64(ld.Segdata.Fileoff))
+	ld.Datblk(int64(ld.Segdata.Vaddr), int64(ld.Segdata.Filelen))
+
+	/* output symbol table */
+	ld.Symsize = 0
+
+	ld.Lcsize = 0
+	symo := uint32(0)
+	if ld.Debug['s'] == 0 {
+		// TODO: rationalize
+		if ld.Debug['v'] != 0 {
+			fmt.Fprintf(ld.Bso, "%5.2f sym\n", obj.Cputime())
+		}
+		ld.Bso.Flush()
+		switch ld.HEADTYPE {
+		default:
+			if ld.Iself {
+				symo = uint32(ld.Segdata.Fileoff + ld.Segdata.Filelen)
+				symo = uint32(ld.Rnd(int64(symo), int64(ld.INITRND)))
+			}
+		}
+
+		ld.Cseek(int64(symo))
+		switch ld.HEADTYPE {
+		default:
+			if ld.Iself {
+				if ld.Debug['v'] != 0 {
+					fmt.Fprintf(ld.Bso, "%5.2f elfsym\n", obj.Cputime())
+				}
+				ld.Asmelfsym()
+				ld.Cflush()
+				ld.Cwrite(ld.Elfstrdat)
+
+				if ld.Debug['v'] != 0 {
+					fmt.Fprintf(ld.Bso, "%5.2f dwarf\n", obj.Cputime())
+				}
+
+				if ld.Linkmode == ld.LinkExternal {
+					ld.Elfemitreloc()
+				}
+			}
+		}
+	}
+
+	ld.Ctxt.Cursym = nil
+	if ld.Debug['v'] != 0 {
+		fmt.Fprintf(ld.Bso, "%5.2f header\n", obj.Cputime())
+	}
+	ld.Bso.Flush()
+	ld.Cseek(0)
+	switch ld.HEADTYPE {
+	default:
+
+	case obj.Hlinux,
+		obj.Hfreebsd,
+		obj.Hnetbsd,
+		obj.Hopenbsd,
+		obj.Hsolaris,
+		obj.Hnacl:
+		ld.Asmbelf(int64(symo))
+	}
+
+	ld.Cflush()
+	if ld.Debug['c'] != 0 {
+		fmt.Printf("textsize=%d\n", ld.Segtext.Filelen)
+		fmt.Printf("datsize=%d\n", ld.Segdata.Filelen)
+		fmt.Printf("bsssize=%d\n", ld.Segdata.Length-ld.Segdata.Filelen)
+		fmt.Printf("symsize=%d\n", ld.Symsize)
+		fmt.Printf("lcsize=%d\n", ld.Lcsize)
+		fmt.Printf("total=%d\n", ld.Segtext.Filelen+ld.Segdata.Length+uint64(ld.Symsize)+uint64(ld.Lcsize))
+	}
+}
diff --git a/src/cmd/link/internal/sparc64/l.go b/src/cmd/link/internal/sparc64/l.go
new file mode 100644
index 0000000..bb1b8da
--- /dev/null
+++ b/src/cmd/link/internal/sparc64/l.go
@@ -0,0 +1,74 @@
+// Inferno utils/5l/asm.c
+// http://code.google.com/p/inferno-os/source/browse/utils/5l/asm.c
+//
+//	Copyright © 1994-1999 Lucent Technologies Inc.  All rights reserved.
+//	Portions Copyright © 1995-1997 C H Forsyth (forsyth@terzarima.net)
+//	Portions Copyright © 1997-1999 Vita Nuova Limited
+//	Portions Copyright © 2000-2007 Vita Nuova Holdings Limited (www.vitanuova.com)
+//	Portions Copyright © 2004,2006 Bruce Ellis
+//	Portions Copyright © 2005-2007 C H Forsyth (forsyth@terzarima.net)
+//	Revisions Copyright © 2000-2007 Lucent Technologies Inc. and others
+//	Portions Copyright © 2009 The Go Authors.  All rights reserved.
+//
+// Permission is hereby granted, free of charge, to any person obtaining a copy
+// of this software and associated documentation files (the "Software"), to deal
+// in the Software without restriction, including without limitation the rights
+// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+// copies of the Software, and to permit persons to whom the Software is
+// furnished to do so, subject to the following conditions:
+//
+// The above copyright notice and this permission notice shall be included in
+// all copies or substantial portions of the Software.
+//
+// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL THE
+// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
+// THE SOFTWARE.
+
+package sparc64
+
+// Writing object files.
+
+// cmd/9l/l.h from Vita Nuova.
+//
+//	Copyright © 1994-1999 Lucent Technologies Inc.  All rights reserved.
+//	Portions Copyright © 1995-1997 C H Forsyth (forsyth@terzarima.net)
+//	Portions Copyright © 1997-1999 Vita Nuova Limited
+//	Portions Copyright © 2000-2008 Vita Nuova Holdings Limited (www.vitanuova.com)
+//	Portions Copyright © 2004,2006 Bruce Ellis
+//	Portions Copyright © 2005-2007 C H Forsyth (forsyth@terzarima.net)
+//	Revisions Copyright © 2000-2008 Lucent Technologies Inc. and others
+//	Portions Copyright © 2009 The Go Authors.  All rights reserved.
+//
+// Permission is hereby granted, free of charge, to any person obtaining a copy
+// of this software and associated documentation files (the "Software"), to deal
+// in the Software without restriction, including without limitation the rights
+// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+// copies of the Software, and to permit persons to whom the Software is
+// furnished to do so, subject to the following conditions:
+//
+// The above copyright notice and this permission notice shall be included in
+// all copies or substantial portions of the Software.
+//
+// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL THE
+// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
+// THE SOFTWARE.
+
+const (
+	MaxAlign  = 32 // max data alignment
+	MinAlign  = 1  // min data alignment
+	FuncAlign = 16
+)
+
+/* Used by ../internal/ld/dwarf.go */
+const (
+	DWARFREGSP = 14
+	DWARFREGLR = 15
+)
diff --git a/src/cmd/link/internal/sparc64/obj.go b/src/cmd/link/internal/sparc64/obj.go
new file mode 100644
index 0000000..6c66816
--- /dev/null
+++ b/src/cmd/link/internal/sparc64/obj.go
@@ -0,0 +1,126 @@
+// Inferno utils/5l/obj.c
+// http://code.google.com/p/inferno-os/source/browse/utils/5l/obj.c
+//
+//	Copyright © 1994-1999 Lucent Technologies Inc.  All rights reserved.
+//	Portions Copyright © 1995-1997 C H Forsyth (forsyth@terzarima.net)
+//	Portions Copyright © 1997-1999 Vita Nuova Limited
+//	Portions Copyright © 2000-2007 Vita Nuova Holdings Limited (www.vitanuova.com)
+//	Portions Copyright © 2004,2006 Bruce Ellis
+//	Portions Copyright © 2005-2007 C H Forsyth (forsyth@terzarima.net)
+//	Revisions Copyright © 2000-2007 Lucent Technologies Inc. and others
+//	Portions Copyright © 2009 The Go Authors.  All rights reserved.
+//
+// Permission is hereby granted, free of charge, to any person obtaining a copy
+// of this software and associated documentation files (the "Software"), to deal
+// in the Software without restriction, including without limitation the rights
+// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+// copies of the Software, and to permit persons to whom the Software is
+// furnished to do so, subject to the following conditions:
+//
+// The above copyright notice and this permission notice shall be included in
+// all copies or substantial portions of the Software.
+//
+// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL THE
+// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
+// THE SOFTWARE.
+
+package sparc64
+
+import (
+	"cmd/internal/obj"
+	"cmd/internal/sys"
+	"cmd/link/internal/ld"
+	"fmt"
+	"log"
+)
+
+// Reading object files.
+
+func Main() {
+	linkarchinit()
+	ld.Ldmain()
+}
+
+func linkarchinit() {
+	ld.SysArch = sys.ArchSPARC64
+
+	ld.Thearch.Funcalign = FuncAlign
+	ld.Thearch.Maxalign = MaxAlign
+	ld.Thearch.Minalign = MinAlign
+	ld.Thearch.Dwarfregsp = DWARFREGSP
+	ld.Thearch.Dwarfreglr = DWARFREGLR
+	ld.Thearch.StackBias = 0x7ff
+
+	ld.Thearch.Adddynrel = adddynrel
+	ld.Thearch.Archinit = archinit
+	ld.Thearch.Archreloc = archreloc
+	ld.Thearch.Archrelocvariant = archrelocvariant
+	ld.Thearch.Asmb = asmb
+	ld.Thearch.Elfreloc1 = elfreloc1
+	ld.Thearch.Elfsetupplt = elfsetupplt
+	ld.Thearch.Gentext = gentext
+	ld.Thearch.Machoreloc1 = machoreloc1
+	ld.Thearch.Lput = ld.Lputb
+	ld.Thearch.Wput = ld.Wputb
+	ld.Thearch.Vput = ld.Vputb
+	ld.Thearch.Append16 = ld.Append16b
+	ld.Thearch.Append32 = ld.Append32b
+	ld.Thearch.Append64 = ld.Append64b
+
+	ld.Thearch.Linuxdynld = "XXX"
+	ld.Thearch.Freebsddynld = "XXX"
+	ld.Thearch.Openbsddynld = "XXX"
+	ld.Thearch.Netbsddynld = "XXX"
+	ld.Thearch.Dragonflydynld = "XXX"
+	ld.Thearch.Solarisdynld = "/usr/lib/sparcv9/ld.so.1"
+}
+
+func archinit() {
+	// TODO(shawn): DWARF generation causes invalid elf binaries to be
+	// generated on sparc64
+	ld.Debug['w'] = 1 // disable DWARF generation
+
+	// getgoextlinkenabled is based on GO_EXTLINK_ENABLED when
+	// Go was built; see ../../make.bash.
+	if ld.Linkmode == ld.LinkAuto && obj.Getgoextlinkenabled() == "0" {
+		ld.Linkmode = ld.LinkInternal
+	}
+
+	switch ld.HEADTYPE {
+	default:
+		if ld.Linkmode == ld.LinkAuto {
+			ld.Linkmode = ld.LinkInternal
+		}
+		if ld.Linkmode == ld.LinkExternal && obj.Getgoextlinkenabled() != "1" {
+			log.Fatalf("cannot use -linkmode=external with -H %s", ld.Headstr(int(ld.HEADTYPE)))
+		}
+	case obj.Hlinux, obj.Hsolaris:
+		break
+	}
+
+	switch ld.HEADTYPE {
+	default:
+		ld.Exitf("unknown -H option: %v", ld.HEADTYPE)
+
+	case obj.Hlinux, obj.Hsolaris: /* sparc64 elf */
+		ld.Elfinit()
+		ld.HEADR = ld.ELFRESERVE
+		if ld.INITTEXT == -1 {
+			ld.INITTEXT = 0x10000 + int64(ld.HEADR)
+		}
+		if ld.INITDAT == -1 {
+			ld.INITDAT = 0
+		}
+		if ld.INITRND == -1 {
+			ld.INITRND = 0x10000
+		}
+	}
+
+	if ld.INITDAT != 0 && ld.INITRND != 0 {
+		fmt.Printf("warning: -D0x%x is ignored because of -R0x%x\n", uint64(ld.INITDAT), uint32(ld.INITRND))
+	}
+}
diff --git a/src/cmd/link/main.go b/src/cmd/link/main.go
index f92e02e..627a966 100644
--- a/src/cmd/link/main.go
+++ b/src/cmd/link/main.go
@@ -12,6 +12,7 @@ import (
 	"cmd/link/internal/mips64"
 	"cmd/link/internal/ppc64"
 	"cmd/link/internal/s390x"
+	"cmd/link/internal/sparc64"
 	"cmd/link/internal/x86"
 	"fmt"
 	"os"
@@ -36,5 +37,7 @@ func main() {
 		ppc64.Main()
 	case "s390x":
 		s390x.Main()
+	case "sparc64":
+		sparc64.Main()
 	}
 }
diff --git a/src/cmd/objdump/objdump_test.go b/src/cmd/objdump/objdump_test.go
index 899db06..0753872 100644
--- a/src/cmd/objdump/objdump_test.go
+++ b/src/cmd/objdump/objdump_test.go
@@ -109,6 +109,8 @@ func TestDisasm(t *testing.T) {
 		t.Skipf("skipping on %s, issue 12559", runtime.GOARCH)
 	case "s390x":
 		t.Skipf("skipping on %s, issue 15255", runtime.GOARCH)
+	case "sparc64":
+		t.Skipf("skipping on %s, not yet supported", runtime.GOARCH)
 	}
 	testDisasm(t)
 }
@@ -127,6 +129,8 @@ func TestDisasmExtld(t *testing.T) {
 		t.Skipf("skipping on %s, issue 12559 and 12560", runtime.GOARCH)
 	case "s390x":
 		t.Skipf("skipping on %s, issue 15255", runtime.GOARCH)
+	case "sparc64":
+		t.Skipf("skipping on %s, not yet supported", runtime.GOARCH)
 	}
 	// TODO(jsing): Reenable once openbsd/arm has external linking support.
 	if runtime.GOOS == "openbsd" && runtime.GOARCH == "arm" {
diff --git a/src/cmd/vet/asmdecl.go b/src/cmd/vet/asmdecl.go
index d543b2e..6213786 100644
--- a/src/cmd/vet/asmdecl.go
+++ b/src/cmd/vet/asmdecl.go
@@ -67,6 +67,7 @@ var (
 	asmArchMips64LE = asmArch{"mips64", 8, 8, 8, false, "R29", true}
 	asmArchPpc64    = asmArch{"ppc64", 8, 8, 8, true, "R1", true}
 	asmArchPpc64LE  = asmArch{"ppc64le", 8, 8, 8, false, "R1", true}
+	asmArchSparc64  = asmArch{"sparc64", 8, 8, 8, true, "RSP", true}
 
 	arches = []*asmArch{
 		&asmArch386,
@@ -78,6 +79,7 @@ var (
 		&asmArchMips64LE,
 		&asmArchPpc64,
 		&asmArchPpc64LE,
+		&asmArchSparc64,
 	}
 )
 
@@ -624,6 +626,17 @@ func asmCheckVar(badf func(string, ...interface{}), fn *asmFunc, line, expr stri
 			case "MOVV", "MOVD":
 				src = 8
 			}
+		case "sparc64":
+			switch op[len(op)-1] {
+			case 'B':
+				src = 1
+			case 'H':
+				src = 2
+			case 'W':
+				src = 4
+			case 'D':
+				src = 8
+			}
 		}
 	}
 	if dst == 0 {
diff --git a/src/debug/elf/file.go b/src/debug/elf/file.go
index c173ea9..b55e9ee 100644
--- a/src/debug/elf/file.go
+++ b/src/debug/elf/file.go
@@ -598,6 +598,8 @@ func (f *File) applyRelocations(dst []byte, rels []byte) error {
 		return f.applyRelocationsMIPS64(dst, rels)
 	case f.Class == ELFCLASS64 && f.Machine == EM_S390:
 		return f.applyRelocationss390x(dst, rels)
+	case f.Class == ELFCLASS64 && f.Machine == EM_SPARCV9:
+		return f.applyRelocationsSPARC64(dst, rels)
 	default:
 		return errors.New("applyRelocations: not implemented")
 	}
@@ -962,6 +964,51 @@ func (f *File) applyRelocationss390x(dst []byte, rels []byte) error {
 	return nil
 }
 
+func (f *File) applyRelocationsSPARC64(dst []byte, rels []byte) error {
+	// 24 is the size of Rela64.
+	if len(rels)%24 != 0 {
+		return errors.New("length of relocation section is not a multiple of 24")
+	}
+
+	symbols, _, err := f.getSymbols(SHT_SYMTAB)
+	if err != nil {
+		return err
+	}
+
+	b := bytes.NewReader(rels)
+	var rela Rela64
+
+	for b.Len() > 0 {
+		binary.Read(b, f.ByteOrder, &rela)
+		symNo := rela.Info >> 32
+		t := R_SPARC(rela.Info & 0xff)
+
+		if symNo == 0 || symNo > uint64(len(symbols)) {
+			continue
+		}
+		sym := &symbols[symNo-1]
+		if SymType(sym.Info&0xf) != STT_SECTION {
+			// We don't handle non-section relocations for now.
+			continue
+		}
+
+		switch t {
+		case R_SPARC_UA64:
+			if rela.Off+8 >= uint64(len(dst)) || rela.Addend < 0 {
+				continue
+			}
+			f.ByteOrder.PutUint64(dst[rela.Off:rela.Off+8], uint64(rela.Addend))
+		case R_SPARC_UA32:
+			if rela.Off+4 >= uint64(len(dst)) || rela.Addend < 0 {
+				continue
+			}
+			f.ByteOrder.PutUint32(dst[rela.Off:rela.Off+4], uint32(rela.Addend))
+		}
+	}
+
+	return nil
+}
+
 func (f *File) DWARF() (*dwarf.Data, error) {
 	// sectionData gets the data for s, checks its size, and
 	// applies any applicable relations.
diff --git a/src/go/token/position.go b/src/go/token/position.go
index 7306083..d4171d8 100644
--- a/src/go/token/position.go
+++ b/src/go/token/position.go
@@ -446,7 +446,9 @@ func (s *FileSet) File(p Pos) (f *File) {
 func (s *FileSet) PositionFor(p Pos, adjusted bool) (pos Position) {
 	if p != NoPos {
 		if f := s.file(p); f != nil {
+			s.mutex.RLock()
 			pos = f.position(p, adjusted)
+			s.mutex.RUnlock()
 		}
 	}
 	return
diff --git a/src/go/token/position_test.go b/src/go/token/position_test.go
index d26939c..63984bc 100644
--- a/src/go/token/position_test.go
+++ b/src/go/token/position_test.go
@@ -214,7 +214,7 @@ func TestFileSetCacheUnlikely(t *testing.T) {
 	}
 }
 
-// issue 4345. Test concurrent use of FileSet.Pos does not trigger a
+// issue 4345. Test that concurrent use of FileSet.Pos does not trigger a
 // race in the FileSet position cache.
 func TestFileSetRace(t *testing.T) {
 	fset := NewFileSet()
@@ -237,6 +237,35 @@ func TestFileSetRace(t *testing.T) {
 	stop.Wait()
 }
 
+// issue 16548. Test that concurrent use of File.AddLine and FileSet.PositionFor
+// does not trigger a race in the FileSet position cache.
+func TestFileSetRace2(t *testing.T) {
+	const N = 1e3
+	var (
+		fset = NewFileSet()
+		file = fset.AddFile("", -1, N)
+		ch   = make(chan int, 2)
+	)
+
+	go func() {
+		for i := 0; i < N; i++ {
+			file.AddLine(i)
+		}
+		ch <- 1
+	}()
+
+	go func() {
+		pos := file.Pos(0)
+		for i := 0; i < N; i++ {
+			fset.PositionFor(pos, false)
+		}
+		ch <- 1
+	}()
+
+	<-ch
+	<-ch
+}
+
 func TestPositionFor(t *testing.T) {
 	src := []byte(`
 foo
diff --git a/src/math/big/arith_sparc64.s b/src/math/big/arith_sparc64.s
new file mode 100644
index 0000000..b74a089
--- /dev/null
+++ b/src/math/big/arith_sparc64.s
@@ -0,0 +1,46 @@
+// Copyright 2016 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// +build !math_big_pure_go,sparc64
+
+#include "textflag.h"
+
+// This file provides fast assembly versions for the elementary
+// arithmetic operations on vectors implemented in arith.go.
+
+TEXT ·mulWW(SB),NOSPLIT|NOFRAME,$0
+	JMP ·mulWW_g(SB)
+
+TEXT ·divWW(SB),NOSPLIT|NOFRAME,$0
+	JMP ·divWW_g(SB)
+
+TEXT ·addVV(SB),NOSPLIT|NOFRAME,$0
+	JMP ·addVV_g(SB)
+
+TEXT ·subVV(SB),NOSPLIT|NOFRAME,$0
+	JMP ·subVV_g(SB)
+
+TEXT ·addVW(SB),NOSPLIT|NOFRAME,$0
+	JMP ·addVW_g(SB)
+
+TEXT ·subVW(SB),NOSPLIT|NOFRAME,$0
+	JMP ·subVW_g(SB)
+
+TEXT ·shlVU(SB),NOSPLIT|NOFRAME,$0
+	JMP ·shlVU_g(SB)
+
+TEXT ·shrVU(SB),NOSPLIT|NOFRAME,$0
+	JMP ·shrVU_g(SB)
+
+TEXT ·mulAddVWW(SB),NOSPLIT|NOFRAME,$0
+	JMP ·mulAddVWW_g(SB)
+
+TEXT ·addMulVVW(SB),NOSPLIT|NOFRAME,$0
+	JMP ·addMulVVW_g(SB)
+
+TEXT ·divWVW(SB),NOSPLIT|NOFRAME,$0
+	JMP ·divWVW_g(SB)
+
+TEXT ·bitLen(SB),NOSPLIT|NOFRAME,$0
+	JMP ·bitLen_g(SB)
diff --git a/src/math/stubs_sparc64.s b/src/math/stubs_sparc64.s
new file mode 100644
index 0000000..41a47d2
--- /dev/null
+++ b/src/math/stubs_sparc64.s
@@ -0,0 +1,89 @@
+// Copyright 2016 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+#include "textflag.h"
+
+TEXT ·Asin(SB),NOSPLIT|NOFRAME,$0
+	JMP ·asin(SB)
+
+TEXT ·Acos(SB),NOSPLIT|NOFRAME,$0
+	JMP ·acos(SB)
+
+TEXT ·Atan2(SB),NOSPLIT|NOFRAME,$0
+	JMP ·atan2(SB)
+
+TEXT ·Atan(SB),NOSPLIT|NOFRAME,$0
+	JMP ·atan(SB)
+
+TEXT ·Dim(SB),NOSPLIT|NOFRAME,$0
+	JMP ·dim(SB)
+
+TEXT ·Min(SB),NOSPLIT|NOFRAME,$0
+	JMP ·min(SB)
+
+TEXT ·Max(SB),NOSPLIT|NOFRAME,$0
+	JMP ·max(SB)
+
+TEXT ·Exp2(SB),NOSPLIT|NOFRAME,$0
+	JMP ·exp2(SB)
+
+TEXT ·Expm1(SB),NOSPLIT|NOFRAME,$0
+	JMP ·expm1(SB)
+
+TEXT ·Exp(SB),NOSPLIT|NOFRAME,$0
+	JMP ·exp(SB)
+
+TEXT ·Floor(SB),NOSPLIT|NOFRAME,$0
+	JMP ·floor(SB)
+
+TEXT ·Ceil(SB),NOSPLIT|NOFRAME,$0
+	JMP ·ceil(SB)
+
+TEXT ·Trunc(SB),NOSPLIT|NOFRAME,$0
+	JMP ·trunc(SB)
+
+TEXT ·Frexp(SB),NOSPLIT|NOFRAME,$0
+	JMP ·frexp(SB)
+
+TEXT ·Hypot(SB),NOSPLIT|NOFRAME,$0
+	JMP ·hypot(SB)
+
+TEXT ·Ldexp(SB),NOSPLIT|NOFRAME,$0
+	JMP ·ldexp(SB)
+
+TEXT ·Log10(SB),NOSPLIT|NOFRAME,$0
+	JMP ·log10(SB)
+
+TEXT ·Log2(SB),NOSPLIT|NOFRAME,$0
+	JMP ·log2(SB)
+
+TEXT ·Log1p(SB),NOSPLIT|NOFRAME,$0
+	JMP ·log1p(SB)
+
+TEXT ·Log(SB),NOSPLIT|NOFRAME,$0
+	JMP ·log(SB)
+
+TEXT ·Modf(SB),NOSPLIT|NOFRAME,$0
+	JMP ·modf(SB)
+
+TEXT ·Mod(SB),NOSPLIT|NOFRAME,$0
+	JMP ·mod(SB)
+
+TEXT ·Remainder(SB),NOSPLIT|NOFRAME,$0
+	JMP ·remainder(SB)
+
+TEXT ·Sincos(SB),NOSPLIT|NOFRAME,$0
+	JMP ·sincos(SB)
+
+TEXT ·Sin(SB),NOSPLIT|NOFRAME,$0
+	JMP ·sin(SB)
+
+TEXT ·Cos(SB),NOSPLIT|NOFRAME,$0
+	JMP ·cos(SB)
+
+TEXT ·Sqrt(SB),NOSPLIT|NOFRAME,$0
+	JMP ·sqrt(SB)
+
+TEXT ·Tan(SB),NOSPLIT|NOFRAME,$0
+	JMP ·tan(SB)
diff --git a/src/reflect/asm_sparc64.s b/src/reflect/asm_sparc64.s
new file mode 100644
index 0000000..0564153
--- /dev/null
+++ b/src/reflect/asm_sparc64.s
@@ -0,0 +1,31 @@
+// Copyright 2016 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+#include "textflag.h"
+#include "funcdata.h"
+#include "asm_sparc64.h"
+
+// makeFuncStub is the code half of the function returned by MakeFunc.
+// See the comment on the declaration of makeFuncStub in makefunc.go
+// for more details.
+// No arg size here, runtime pulls arg map out of the func value.
+TEXT ·makeFuncStub(SB),(NOSPLIT|WRAPPER),$16
+	NO_LOCAL_POINTERS
+	MOVD	CTXT, FIXED_FRAME+0(BSP)
+	MOVD	$argframe+0(FP), RT1
+	MOVD	RT1, FIXED_FRAME+8(BSP)
+	CALL	·callReflect(SB)
+	RET
+
+// methodValueCall is the code half of the function returned by makeMethodValue.
+// See the comment on the declaration of methodValueCall in makefunc.go
+// for more details.
+// No arg size here; runtime pulls arg map out of the func value.
+TEXT ·methodValueCall(SB),(NOSPLIT|WRAPPER),$16
+	NO_LOCAL_POINTERS
+	MOVD	CTXT, FIXED_FRAME+0(BSP)
+	MOVD	$argframe+0(FP), RT1
+	MOVD	RT1, FIXED_FRAME+8(BSP)
+	CALL	·callMethod(SB)
+	RET
diff --git a/src/runtime/asm_sparc64.h b/src/runtime/asm_sparc64.h
new file mode 100644
index 0000000..bb83490
--- /dev/null
+++ b/src/runtime/asm_sparc64.h
@@ -0,0 +1,43 @@
+// Copyright 2016 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// SPARC64 TOS is STACK_BIAS bytes *above* from RSP (R14, %o6).
+// Current frame is STACK_BIAS bytes *above* RFP (R30, %i6).
+#define STACK_BIAS 0x7ff
+
+// FIXED_FRAME defines the size of the fixed part of a stack frame. A stack
+// frame looks like this:
+//
+// +---------------------+
+// | local variable area |
+// +---------------------+
+// | argument area       |
+// +---------------------+ <- BSP+FIXED_FRAME
+// | fixed area          |
+// +---------------------+ <- BSP
+//
+// So a function that sets up a stack frame at all uses as least FIXED_FRAME
+// bytes of stack. This mostly affects assembly that calls other functions
+// with arguments (the arguments should be stored at FIXED_FRAME+0(BSP),
+// FIXED_FRAME+8(BSP) etc.) and some other low-level places.
+//
+// Register usage conventions keeping the above in mind:
+//
+// * FP will make use of RFP, which is not always appropiate, e.g. in NOFRAME
+//   functions.
+// * Generally, push arguments to off(BFP); in NOFRAME functions use
+//   use off(BSP).
+// * Example: setcallerpc does not return any arguments, it modifies the
+//   link register saved on the caller's stack, so it has to use BFP.
+// * Generally, use ret+off(FP) so go vet can actually check it for
+//   corectness if there's a go declaration.  Although this cannot be used in
+//   NOFRAME functions, etc.
+//
+// See runtime/stack.go for details.
+#define ARG_PUSH_SIZE 6*8
+#define WINDOW_SIZE 16*8
+#define FIXED_FRAME WINDOW_SIZE+ARG_PUSH_SIZE
+
+// membar #MemIssue|#Sync|#LoadLoad|#StoreLoad|#LoadStore|#StoreStore
+#define MEM_SYNC MEMBAR $111
diff --git a/src/runtime/asm_sparc64.s b/src/runtime/asm_sparc64.s
new file mode 100644
index 0000000..fc5f22b
--- /dev/null
+++ b/src/runtime/asm_sparc64.s
@@ -0,0 +1,1131 @@
+// Copyright 2016 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+#include "go_asm.h"
+#include "funcdata.h"
+#include "textflag.h"
+#include "asm_sparc64.h"
+
+DATA dbgbuf(SB)/8, $"\n\n"
+GLOBL dbgbuf(SB), $8
+
+TEXT runtime·rt0_go(SB),NOSPLIT,$16-0
+	// BSP = stack; O0 = argc; O1 = argv
+
+	// initialize essential registers
+	CALL	runtime·reginit(SB)
+
+	MOVW	O0, FIXED_FRAME+0(BSP)	// copy argc
+	MOVD	O1, FIXED_FRAME+8(BSP)	// copy argv
+
+	// create istack out of the given (operating system) stack.
+	// _cgo_init may update stackguard.
+	MOVD	$runtime·g0(SB), g
+	MOVD	BSP, RT1
+	// must be larger than _StackSystem
+	MOVD	$(-64*1024)(BSP), RT2
+	MOVD	RT2, g_stackguard0(g)
+	MOVD	RT2, g_stackguard1(g)
+	MOVD	RT2, (g_stack+stack_lo)(g)
+	MOVD	RT1, (g_stack+stack_hi)(g)
+
+	CALL	runtime·do_cgo_init(SB)
+
+	// update stackguard after _cgo_init
+	MOVD	(g_stack+stack_lo)(g), I1
+	ADD	$const__StackGuard, I1
+	MOVD	I1, g_stackguard0(g)
+	MOVD	I1, g_stackguard1(g)
+
+	// set the per-goroutine and per-mach "registers"
+	MOVD	$runtime·m0(SB), I1
+
+	// save m->g0 = g0
+	MOVD	g, m_g0(I1)
+	// save m0 to g0->m
+	MOVD	I1, g_m(g)
+	CALL	runtime·save_g(SB)
+
+	CALL	runtime·check(SB)
+
+	// argc, argv already copied.
+	CALL	runtime·args(SB)
+	CALL	runtime·osinit(SB)
+	CALL	runtime·schedinit(SB)
+
+	// create a new goroutine to start program
+	MOVD	ZR, FIXED_FRAME+0(BSP)
+	MOVD	$runtime·mainPC(SB), RT1		// entry
+	MOVD	RT1, FIXED_FRAME+8(BSP)
+	CALL	runtime·newproc(SB)
+
+	// start this M
+	CALL	runtime·mstart(SB)
+
+	MOVD	ZR, (ZR)	// boom
+	UNDEF
+
+DATA	runtime·mainPC+0(SB)/8,$runtime·main(SB)
+GLOBL	runtime·mainPC(SB),RODATA,$8
+
+TEXT runtime·breakpoint(SB),NOSPLIT|NOFRAME,$0-0
+	TA	$0x81
+	RET
+
+TEXT runtime·asminit(SB),NOSPLIT|NOFRAME,$0-0
+	RET
+
+TEXT runtime·reginit(SB),NOSPLIT|NOFRAME,$0-0
+	// initialize essential FP registers
+	FMOVD	$2.0, D28
+	RET
+
+/*
+ *  go-routine
+ */
+
+// void gosave(Gobuf*)
+// save state in Gobuf; setjmp
+TEXT runtime·gosave(SB), NOSPLIT|NOFRAME, $0-8
+	MOVD	buf+0(FP), O0
+	MOVD	$0(BSP), gobuf_sp(O0)
+	MOVD	$8(OLR), gobuf_pc(O0)
+	MOVD	g, gobuf_g(O0)
+	MOVD	ZR, gobuf_lr(O0)
+	MOVD	ZR, gobuf_ret(O0)
+	MOVD	ZR, gobuf_ctxt(O0)
+	MOVD	$0(BFP), gobuf_bp(O0)
+	RET
+
+// void gogo(Gobuf*)
+// restore state from Gobuf; longjmp
+TEXT runtime·gogo(SB), NOSPLIT|NOFRAME, $0-8
+	MOVD	buf+0(FP), O0
+	MOVD	gobuf_g(O0), g
+	CALL	runtime·save_g(SB)
+
+	MOVD	0(g), ZR	// make sure g is not nil
+	MOVD	gobuf_sp(O0), O1
+	MOVD	gobuf_bp(O0), O2
+	MOVD	gobuf_lr(O0), O3
+	// retrieve ILR *before* switching stacks since a spill may
+	// overwrite the saved value
+	MOVD	120(O1), O4
+	MOVD	gobuf_ret(O0), RT1
+	MOVD	gobuf_ctxt(O0), CTXT
+	MOVD	gobuf_pc(O0), O5
+
+	MOVD	ZR, gobuf_sp(O0)
+	MOVD	ZR, gobuf_bp(O0)
+	MOVD	ZR, gobuf_lr(O0)
+	MOVD	ZR, gobuf_ret(O0)
+	MOVD	ZR, gobuf_ctxt(O0)
+	MOVD	ZR, gobuf_pc(O0)
+
+	// switch stacks
+	MOVD	O1, BSP
+
+	// set registers *after* switching stacks to avoid spills to
+	// original stack; then manually spill to new stack to ensure Go
+	// itself can read the new values
+	MOVD	ZR, O0
+	MOVD	ZR, O1
+	MOVD	O3, OLR
+	MOVD	O4, ILR
+	MOVD	O4, 120(BSP)
+
+	// special case; if bp is zero, assume this is topofstack()
+	// and add bias before setting BFP so that it will be zero.
+	CMP	O2, ZR
+	BNED	nottop
+	MOVD	ZR, 112(BSP)
+	ADD	$STACK_BIAS, O2
+	MOVD	O2, BFP
+	MOVD	ZR, O2
+	JMPL	O5, L7
+
+nottop:
+	MOVD	O2, BFP
+	SUB	$STACK_BIAS, O2
+	MOVD	O2, 112(BSP)
+	MOVD	ZR, O2
+	JMPL	O5, L7
+
+
+// void mcall(fn func(*g))
+// Switch to m->g0's stack, call fn(g).
+// Fn must never return. It should gogo(&g->sched)
+// to keep running g.
+TEXT runtime·mcall(SB), NOSPLIT|NOFRAME, $0-8
+	// Save caller state in g->sched
+	MOVD	$0(BSP), (g_sched+gobuf_sp)(g)
+	MOVD	$8(OLR), (g_sched+gobuf_pc)(g)
+	MOVD	ZR, (g_sched+gobuf_lr)(g)
+	MOVD	g, (g_sched+gobuf_g)(g)
+	MOVD	$0(BFP), (g_sched+gobuf_bp)(g)
+
+	// Preserve return address for use when switching stacks later.
+	MOVD	OLR, O1
+
+	// Switch to m->g0 & its stack, call fn.
+	MOVD	g, O0
+	MOVD	g_m(g), TMP
+	MOVD	m_g0(TMP), g
+	CALL	runtime·save_g(SB)
+	CMP	g, O0
+	BNED	ok
+	JMP	runtime·badmcall(SB)
+
+ok:
+	MOVD	fn+0(FP), CTXT			// context
+	MOVD	0(CTXT), O2			// code pointer
+	MOVD	(g_sched+gobuf_sp)(g), O3
+	SUB	$FIXED_FRAME+16, O3, O4
+	MOVD	O4, BSP	// sp = m->g0->sched.sp
+	// set BFP/ILR *after* switching stacks to avoid spills to original
+	// stack; then manually spill to new stack to ensure Go itself can
+	// read the new values
+	MOVD	O3, BFP
+	SUB	$STACK_BIAS, O3
+	MOVD	O3, 112(BSP)
+	MOVD	O1, ILR
+	MOVD	ILR, 120(BSP)
+	MOVD	O0, (FIXED_FRAME+0)(BSP)
+	MOVD	ZR, (FIXED_FRAME+8)(BSP)
+	CALL	(O2)
+	JMP	runtime·badmcall2(SB)
+
+// systemstack_switch is a dummy routine that systemstack leaves at the bottom
+// of the G stack. We need to distinguish the routine that
+// lives at the bottom of the G stack from the one that lives
+// at the top of the system stack because the one at the top of
+// the system stack terminates the stack walk (see topofstack()).
+TEXT runtime·systemstack_switch(SB), NOSPLIT, $0-0
+	UNDEF
+	UNDEF
+	UNDEF
+	UNDEF
+	UNDEF
+	UNDEF
+	CALL	(ILR)	// make sure this function is not leaf
+	RET
+
+// func systemstack(fn func())
+TEXT runtime·systemstack(SB), NOSPLIT, $0-8
+	MOVD	fn+0(FP), CTXT	// context (fn)
+	MOVD	g_m(g), O0	// O0 = m
+
+	MOVD	m_gsignal(O0), O1	// O1 = gsignal
+	CMP	g, O1
+	BED	noswitch
+
+	MOVD	m_g0(O0), O1	// O1 = g0
+	CMP	g, O1
+	BED	noswitch
+
+	MOVD	m_curg(O0), O0
+	CMP	g, O0
+	BED	switch
+
+	// Bad: g is not gsignal, not g0, not curg. What is it?
+	// Hide call from linker nosplit analysis.
+	MOVD	$runtime·badsystemstack(SB), O0
+	CALL	(O0)
+
+switch:
+	// save our state in g->sched. Pretend to
+	// be systemstack_switch if the G stack is scanned.
+	MOVD	$runtime·systemstack_switch(SB), O0
+	ADD	$40, O0	// get past prologue
+	MOVD	O0, (g_sched+gobuf_pc)(g)
+	MOVD	BSP, TMP
+	MOVD	TMP, (g_sched+gobuf_sp)(g)
+	MOVD	BFP, TMP
+	MOVD	TMP, (g_sched+gobuf_bp)(g)
+	MOVD	ZR, (g_sched+gobuf_lr)(g)
+	MOVD	ZR, (g_sched+gobuf_ret)(g)
+	MOVD	ZR, (g_sched+gobuf_ctxt)(g)
+	MOVD	g, (g_sched+gobuf_g)(g)
+
+	// switch to g0
+	MOVD	O1, g
+	CALL	runtime·save_g(SB)
+	MOVD	(g_sched+gobuf_sp)(g), O0
+	// make it look like mstart called systemstack on g0, to stop traceback.
+	SUB	$FIXED_FRAME, O0, RT1
+	MOVD	RT1, BSP
+	// set BFP/ILR *after* switching stacks to avoid spills to original
+	// stack; then manually spill to new stack to ensure Go itself can
+	// read the new values
+	MOVD	O0, BFP	// subtle
+	SUB	$STACK_BIAS, O0
+	MOVD	O0, 112(BSP)
+	MOVD	$runtime·mstart(SB), ILR
+	MOVD	ILR, 120(BSP)
+
+	// call target function
+	MOVD	0(CTXT), O0	// code pointer
+	CALL	(O0)
+
+	// switch back to g
+	MOVD	g_m(g), O0
+	MOVD	m_curg(O0), g
+	CALL	runtime·save_g(SB)
+	MOVD	(g_sched+gobuf_bp)(g), RT1
+	MOVD	(g_sched+gobuf_sp)(g), O0
+	// retrieve ILR *before* switching stacks since a spill may
+	// overwrite the saved value
+	MOVD	120(O0), O1
+	MOVD	O0, BSP
+	// set BFP/ILR *after* switching stacks to avoid spills to original
+	// stack; then manually spill to new stack to ensure Go itself can
+	// read the new values
+	MOVD	RT1, BFP
+	SUB	$STACK_BIAS, RT1
+	MOVD	RT1, 112(BSP)
+	MOVD	O1, ILR
+	MOVD	ILR, 120(BSP)
+
+	MOVD	ZR, (g_sched+gobuf_sp)(g)
+	MOVD	ZR, (g_sched+gobuf_bp)(g)
+	MOVD	ZR, O0
+	MOVD	ZR, O1
+	RET
+
+noswitch:
+	// already on m stack, just call directly
+	MOVD	0(CTXT), O0	// code pointer
+	CALL	(O0)
+	RET
+
+/*
+ * support for morestack
+ */
+
+// Called during function prolog when more stack is needed.
+// Caller has already loaded:
+// I1 prolog's LR
+//
+// The traceback routines see morestack on a g0 as being
+// the top of a stack (for example, morestack calling newstack
+// calling the scheduler calling newm calling gc), so we must
+// record an argument size. For that purpose, it has no arguments.
+TEXT runtime·morestack(SB),NOSPLIT|NOFRAME,$0-0
+	// Cannot grow scheduler stack (m->g0).
+	MOVD	g_m(g), O0
+	MOVD	m_g0(O0), O1
+	CMP	g, O1
+	BNED	5(PC)
+	// stomp on saved link register to force callee to return to
+	// runtime.abort
+	MOVD	$runtime·abort(SB), OLR
+	MOVD	OLR, 120(BSP)
+	JMP	runtime·threaddump(SB)
+	JMPL	ZR, ZR
+
+	// Cannot grow signal stack (m->gsignal).
+	MOVD	m_gsignal(O0), O1
+	CMP	g, O1
+	BNED	2(PC)
+	CALL	runtime·abort(SB)
+
+	// Called from f.
+	// Set g->sched to context in f
+	MOVD	CTXT, (g_sched+gobuf_ctxt)(g)
+	MOVD	BSP, TMP
+	MOVD	TMP, (g_sched+gobuf_sp)(g)
+	MOVD	BFP, TMP
+	MOVD	TMP, (g_sched+gobuf_bp)(g)
+	MOVD	OLR, (g_sched+gobuf_pc)(g)
+	MOVD	I0, (g_sched+gobuf_lr)(g)
+
+	// Called from f.
+	// Set m->morebuf to f's callers.
+	MOVD	I0, (m_morebuf+gobuf_pc)(O0)	// f's caller's PC
+	MOVD	BSP, TMP
+	MOVD	TMP, (m_morebuf+gobuf_sp)(O0)	// f's caller's BSP
+	MOVD	g, (m_morebuf+gobuf_g)(O0)
+
+	// Preserve return address for use when switching stacks later.
+	MOVD	OLR, O2
+
+	// Call newstack on m->g0's stack.
+	MOVD	m_g0(O0), g
+	CALL	runtime·save_g(SB)
+	MOVD	(g_sched+gobuf_sp)(g), O0
+	SUB	$FIXED_FRAME, O0, O1
+	MOVD	O1, BSP
+	// set BFP/ILR *after* switching stacks to avoid spills to original
+	// stack; then manually spill to new stack to ensure Go itself can
+	// read the new values
+	MOVD	O0, BFP
+	SUB	$STACK_BIAS, O0
+	MOVD	O0, 112(BSP)
+	MOVD	O2, ILR
+	MOVD	ILR, 120(BSP)
+
+	CALL	runtime·newstack(SB)
+
+	// Not reached, but make sure the return PC from the call to newstack
+	// is still in this function, and not the beginning of the next.
+	UNDEF
+
+TEXT runtime·morestack_noctxt(SB),NOSPLIT|NOFRAME,$0-0
+	MOVD	ZR, CTXT
+	JMP	runtime·morestack(SB)
+
+TEXT runtime·stackBarrier(SB),NOSPLIT|NOFRAME,$0
+	// Functions that hit a barrier will jump into this function typically via
+	// JMP $8(OLR). Currently, the compiler begins all function prologues with
+	// two NOP instructions to workaround DTrace issues, but if that ever
+	// changes, callers would enter at the wrong place.  To avoid a
+	// future trap, we pad this function to avoid that assumption.
+	MOVD	L1, L1 // NOP
+	MOVD	L1, L1 // NOP
+
+	// We came here via a RET to an overwritten LR.
+	// RT1 may be live (see return0). Only the REG_TMP*, REG_L*, or
+	// REG_G* registers should be used here (except for ILR/OLR) to
+	// avoid accidental alterations of the in or out registers of
+	// functions that use register windows and use the Sys V calling
+	// ABI.
+
+	// Get the original return PC, g.stkbar[g.stkbarPos].savedLRVal.
+	MOVD	(g_stkbar+slice_array)(g), L1
+	MOVD	g_stkbarPos(g), L2
+	MULD	$stkbar__size, L2, L3
+	ADD	L1, L3
+	MOVD	stkbar_savedLRVal(L3), OLR
+	// Record that this stack barrier was hit.
+	MOVD	$1(L2), g_stkbarPos(g)
+	// Jump to the original return PC.
+	JMPL	$8(OLR), L7
+
+// reflectcall: call a function with the given argument list
+// func call(argtype *_type, f *FuncVal, arg *byte, argsize, retoffset uint32).
+// we don't have variable-sized frames, so we use a small number
+// of constant-sized-frame functions to encode a few bits of size in the pc.
+// Caution: ugly multiline assembly macros in your future!
+
+#define DISPATCH(NAME,MAXSIZE)		\
+	MOVD	$MAXSIZE, TMP;		\
+	CMP	TMP, RT1;		\
+	BGD	3(PC);			\
+	MOVD	$NAME(SB), RT1;	\
+	JMPL	RT1, L7
+// Note: can't just "B NAME(SB)" - bad inlining results.
+
+TEXT reflect·call(SB), NOSPLIT|NOFRAME, $0-0
+	JMP	·reflectcall(SB)
+
+TEXT ·reflectcall(SB), NOSPLIT|NOFRAME, $0-32
+	MOVUW argsize+24(FP), RT1
+	// NOTE(rsc): No call16, because CALLFN needs four words
+	// of argument space to invoke callwritebarrier.
+	DISPATCH(runtime·call32, 32)
+	DISPATCH(runtime·call64, 64)
+	DISPATCH(runtime·call128, 128)
+	DISPATCH(runtime·call256, 256)
+	DISPATCH(runtime·call512, 512)
+	DISPATCH(runtime·call1024, 1024)
+	DISPATCH(runtime·call2048, 2048)
+	DISPATCH(runtime·call4096, 4096)
+	DISPATCH(runtime·call8192, 8192)
+	DISPATCH(runtime·call16384, 16384)
+	DISPATCH(runtime·call32768, 32768)
+	DISPATCH(runtime·call65536, 65536)
+	DISPATCH(runtime·call131072, 131072)
+	DISPATCH(runtime·call262144, 262144)
+	DISPATCH(runtime·call524288, 524288)
+	DISPATCH(runtime·call1048576, 1048576)
+	DISPATCH(runtime·call2097152, 2097152)
+	DISPATCH(runtime·call4194304, 4194304)
+	DISPATCH(runtime·call8388608, 8388608)
+	DISPATCH(runtime·call16777216, 16777216)
+	DISPATCH(runtime·call33554432, 33554432)
+	DISPATCH(runtime·call67108864, 67108864)
+	DISPATCH(runtime·call134217728, 134217728)
+	DISPATCH(runtime·call268435456, 268435456)
+	DISPATCH(runtime·call536870912, 536870912)
+	DISPATCH(runtime·call1073741824, 1073741824)
+	MOVD	$runtime·badreflectcall(SB), I3
+	JMPL	I3, L7
+
+#define CALLFN(NAME,MAXSIZE)			\
+TEXT NAME(SB), WRAPPER, $MAXSIZE-24;		\
+	NO_LOCAL_POINTERS;			\
+	/* copy arguments to stack */		\
+	MOVD	arg+16(FP), I1;			\
+	MOVUW	argsize+24(FP), I4;		\
+	MOVD	BSP, L6;			\
+	ADD	$(FIXED_FRAME-1), L6;		\
+	SUB	$1, I1;				\
+	ADD	L6, I4;				\
+	CMP	L6, I4;				\
+	BED	6(PC);				\
+	ADD	$1, I1;				\
+	MOVUB	(I1), O1;			\
+	ADD	$1, L6;				\
+	MOVUB	O1, (L6);			\
+	JMP	-6(PC);				\
+	/* call function */			\
+	MOVD	f+8(FP), CTXT;			\
+	MOVD	(CTXT), I3;			\
+	PCDATA	$PCDATA_StackMapIndex, $0;	\
+	CALL	(I3);				\
+	/* copy return values back */		\
+	MOVD	arg+16(FP), I1;			\
+	MOVUW	n+24(FP), I4;			\
+	MOVUW	retoffset+28(FP), O1;		\
+	MOVD	BSP, L6;			\
+	ADD	O1, L6; 			\
+	ADD	O1, I1;				\
+	SUB	O1, I4;				\
+	ADD	$(FIXED_FRAME-1), L6;		\
+	SUB	$1, I1;				\
+	ADD	L6, I4;				\
+loop:						\
+	CMP	L6, I4;				\
+	BED	end;				\
+	ADD	$1, L6;				\
+	MOVUB	(L6), O1;			\
+	ADD	$1, I1;				\
+	MOVUB	O1, (I1);			\
+	JMP	loop;				\
+end:						\
+	/* execute write barrier updates */	\
+	MOVD	argtype+0(FP), O0;		\
+	MOVD	arg+16(FP), I1;			\
+	MOVUW	n+24(FP), I4;			\
+	MOVUW	retoffset+28(FP), O1;		\
+	MOVD	O0, (FIXED_FRAME+0)(BSP);	\
+	MOVD	I1, (FIXED_FRAME+8)(BSP);	\
+	MOVD	I4, (FIXED_FRAME+16)(BSP);	\
+	MOVD	O1, (FIXED_FRAME+24)(BSP);	\
+	CALL	runtime·callwritebarrier(SB);	\
+	RET
+
+CALLFN(·call32, 32)
+CALLFN(·call64, 64)
+CALLFN(·call128, 128)
+CALLFN(·call256, 256)
+CALLFN(·call512, 512)
+CALLFN(·call1024, 1024)
+CALLFN(·call2048, 2048)
+CALLFN(·call4096, 4096)
+CALLFN(·call8192, 8192)
+CALLFN(·call16384, 16384)
+CALLFN(·call32768, 32768)
+CALLFN(·call65536, 65536)
+CALLFN(·call131072, 131072)
+CALLFN(·call262144, 262144)
+CALLFN(·call524288, 524288)
+CALLFN(·call1048576, 1048576)
+CALLFN(·call2097152, 2097152)
+CALLFN(·call4194304, 4194304)
+CALLFN(·call8388608, 8388608)
+CALLFN(·call16777216, 16777216)
+CALLFN(·call33554432, 33554432)
+CALLFN(·call67108864, 67108864)
+CALLFN(·call134217728, 134217728)
+CALLFN(·call268435456, 268435456)
+CALLFN(·call536870912, 536870912)
+CALLFN(·call1073741824, 1073741824)
+
+
+// AES hashing not implemented for SPARC64.
+TEXT runtime·aeshash(SB),NOSPLIT|NOFRAME,$0-0
+	MOVW	(ZR), I3
+TEXT runtime·aeshash32(SB),NOSPLIT|NOFRAME,$0-0
+	MOVW	(ZR), I3
+TEXT runtime·aeshash64(SB),NOSPLIT|NOFRAME,$0-0
+	MOVW	(ZR), I3
+TEXT runtime·aeshashstr(SB),NOSPLIT|NOFRAME,$0-0
+	MOVW	(ZR), I3
+	
+TEXT runtime·procyield(SB),NOSPLIT,$0-0
+	RD	CCR, I5
+	RET
+
+// void jmpdefer(fv, sp);
+// called from deferreturn.
+// 1. grab stored LR for caller
+// 2. sub 8 bytes to get back to BL deferreturn
+// 3. BR to fn
+TEXT runtime·jmpdefer(SB), NOSPLIT|NOFRAME, $0-16
+	// We're in the same stack frame as our caller, deferreturn,
+	// so this retrieves the return address to deferreturn's caller.
+	// 
+	// We need to subtract -8 from this value, because the deferred
+	// functions return to $8(OLR).
+	MOVD	120(BSP), L1
+	SUB	$8, L1
+	// Preserve deferreturn's caller's return address before resetting
+	// BSP/BFP.
+	MOVD	120(BFP), L2
+
+	// fv is the deferred function.
+	MOVD	fv+0(FP), CTXT
+
+	// retrieve BSP, we reuse deferreturn's frame,
+	// so BFP is our caller's BSP
+	MOVD	BFP, RT1
+	// retrieve RFP
+	MOVD	112(BFP), RT2
+	// set BSP
+	MOVD	RT1, BSP
+	// set BFP
+	ADD	$STACK_BIAS, RT2, RT1
+	// set BFP/*LR *after* switching stacks to avoid spills to original
+	// stack; then manually spill to new stack to ensure Go itself can
+	// read the new values
+	MOVD	RT1, BFP
+	MOVD	RT2, 112(BSP)
+	MOVD	L2, ILR
+	MOVD	ILR, 120(BSP)
+	// return to deferreturn's return address
+	MOVD	L1, OLR
+
+	// call deferred function
+	MOVD	0(CTXT), TMP
+	JMPL	TMP, L7
+
+// Save state of caller into g->sched.
+TEXT gosave<>(SB),NOSPLIT|NOFRAME,$0
+	MOVD	$8(OLR), (g_sched+gobuf_pc)(g)
+	MOVD	BSP, TMP
+	MOVD	TMP, (g_sched+gobuf_sp)(g)
+	MOVD	ZR, (g_sched+gobuf_lr)(g)
+	MOVD	ZR, (g_sched+gobuf_ret)(g)
+	MOVD	ZR, (g_sched+gobuf_ctxt)(g)
+	MOVD	BFP, TMP
+	MOVD	TMP, (g_sched+gobuf_bp)(g)
+	RET
+
+#define CGOFRAMESZ 192
+
+// func asmcgocall(fn, arg unsafe.Pointer) int32
+TEXT ·asmcgocall(SB),NOSPLIT,$0-20
+	MOVD	fn+0(FP), O1
+	MOVD	arg+8(FP), O0
+
+	// save original stack pointer
+	MOVD	BSP, O2
+	MOVD	O2, O4
+
+	// Figure out if we need to switch to m->g0 stack.
+	// We get called to create new OS threads too, and those
+	// come in on the m->g0 stack already.
+	MOVD	g, O3
+	CMP	O3, ZR
+	BED	nosave
+
+	// These cases use the noswitch path instead of the nosave path, as
+	// stack might be copied during a callback.
+	MOVD	g_m(O3), O5
+	MOVD	m_gsignal(O5), O5
+	CMP	g, O5
+	BED	noswitch
+
+	MOVD	g_m(O3), O5
+	MOVD	m_g0(O5), O5
+	CMP	g, O5
+	BED	noswitch
+
+	// Switch to g0 and system stack
+	CALL	gosave<>(SB)
+	MOVD	O5, g
+	CALL	runtime·save_g(SB)
+	MOVD	(g_sched+gobuf_sp)(g), O4
+
+noswitch:
+	SUB	$(CGOFRAMESZ), O4, O5
+	MOVD	O5, BSP
+	// set BFP *after* switching stacks to avoid spills to original
+	// stack; then manually spill to new stack to ensure Go itself can
+	// read the new values
+	MOVD	O4, BFP
+	SUB	$STACK_BIAS, O4
+	MOVD	O4, 112(BSP)
+	MOVD	ILR, 120(BSP)
+
+	// Now on a scheduling stack (a pthread-created stack).
+	// save old g on stack
+	MOVD	O3, (CGOFRAMESZ-8)(BSP)
+	// save depth in old g stack, can't just save SP, as stack
+	// might be copied during a callback
+	MOVD	(g_stack+stack_hi)(O3), O3
+	SUB	O2, O3
+	MOVD	O3, (CGOFRAMESZ-16)(BSP)
+
+	// call target function
+	CALL	(O1)
+
+	// Restore g
+	MOVD	(CGOFRAMESZ-8)(BSP), g
+	CALL	runtime·save_g(SB)
+	// Retrieve stack pointer
+	MOVD	(g_stack+stack_hi)(g), O2
+	MOVD	(CGOFRAMESZ-16)(BSP), O3
+	SUB	O3, O2
+	// Retrieve frame pointer and return address.
+	MOVD	112(O2), O3
+	ADD	$STACK_BIAS, O3, O4
+	MOVD	120(O2), O5
+	// Restore stack pointer
+	MOVD	O2, BSP
+	// set BFP/ILR *after* switching stacks to avoid spills to original
+	// stack; then manually spill to new stack to ensure Go itself can
+	// read the new values
+	MOVD	O4, BFP
+	MOVD	O3, 112(BSP)
+	MOVD	O5, ILR
+	MOVD	ILR, 120(BSP)
+
+	MOVW	O0, ret+16(FP)
+	RET
+
+nosave:
+	// Running on a system stack, perhaps even without a g.
+	// Having no g can happen during thread creation or thread teardown
+	// (see needm/dropm on Solaris, for example).
+	// This code is like the above sequence but without saving/restoring g
+	// and without worrying about the stack moving out from under us
+	// (because we're on a system stack, not a goroutine stack).
+	SUB	$(CGOFRAMESZ), O4, O5
+	MOVD	O5, BSP
+	// set BFP/ILR *after* switching stacks to avoid spills to original
+	// stack; then manually spill to new stack to ensure Go itself can
+	// read the new values
+	MOVD	O4, BFP				// previous %sp becomes %fp
+	SUB	$STACK_BIAS, O4
+	MOVD	O4, 112(BSP)
+	MOVD	ILR, 120(BSP)
+
+	MOVD	ZR, (CGOFRAMESZ-8)(BSP)		// where above code stores g; in case someone looks during debugging
+
+	MOVD	O2, (CGOFRAMESZ-16)(BSP)	// save original stack pointer
+
+	// call target function
+	CALL	(O1)
+
+	// Retrieve stack pointer
+	MOVD	(CGOFRAMESZ-16)(BSP), O2
+	// Retrieve frame pointer and return address.
+	MOVD	112(O2), O3
+	ADD	$STACK_BIAS, O3, O4
+	MOVD	120(O2), O5
+	// Restore stack pointer
+	MOVD	O2, BSP
+	// set BFP/ILR *after* switching stacks to avoid spills to original
+	// stack; then manually spill to new stack to ensure Go itself can
+	// read the new values
+	MOVD	O4, BFP
+	MOVD	O3, 112(BSP)
+	MOVD	O5, ILR
+	MOVD	ILR, 120(BSP)
+
+	MOVW	O0, ret+16(FP)
+	RET
+
+
+// cgocallback(void (*fn)(void*), void *frame, uintptr framesize, uintptr ctxt)
+// Turn the fn into a Go func (by taking its address) and call
+// cgocallback_gofunc.
+TEXT runtime·cgocallback(SB),NOSPLIT,$32-32
+	MOVD	$fn+0(FP), TMP
+	MOVD	TMP, (FIXED_FRAME+0)(BSP)
+	MOVD	frame+8(FP), TMP
+	MOVD	TMP, (FIXED_FRAME+8)(BSP)
+	MOVD	framesize+16(FP), TMP
+	MOVD	TMP, (FIXED_FRAME+16)(BSP)
+	MOVD	ctxt+24(FP), TMP
+	MOVD	TMP, FIXED_FRAME+24(BSP)
+	MOVD	$runtime·cgocallback_gofunc(SB), RT1
+	CALL	RT1
+	RET
+
+// cgocallback_gofunc(FuncVal*, void *frame, uintptr framesize, uintptr ctxt)
+// See cgocall.go for more details.
+TEXT ·cgocallback_gofunc(SB),NOSPLIT,$16-32
+	NO_LOCAL_POINTERS
+
+	// Load m and g from thread-local storage.
+	// On Solaris we always use TLS, even without cgo.
+#ifndef GOOS_solaris
+	MOVB	runtime·iscgo(SB), RT1
+	CMP	RT1, ZR
+	BED	nocgo
+#endif
+	CALL	runtime·load_g(SB)
+nocgo:
+
+	// If g is nil, Go did not create the current thread.
+	// Call needm to obtain one for temporary use.
+	// In this case, we're running on the thread stack, so there's
+	// lots of space, but the linker doesn't know. Hide the call from
+	// the linker analysis by using an indirect call.
+	CMP	g, ZR
+	BED	needm
+
+	MOVD	g_m(g), O0
+	MOVD	O0, savedm-8(SP)
+	JMP	havem
+
+needm:
+	MOVD	g, savedm-8(SP) // g is zero, so is m.
+	MOVD	$runtime·needm(SB), RT1
+	CALL	(RT1)
+
+	// Set m->sched.sp = SP, so that if a panic happens
+	// during the function we are about to execute, it will
+	// have a valid SP to run on the g0 stack.
+	// The next few lines (after the havem label)
+	// will save this SP onto the stack and then write
+	// the same SP back to m->sched.sp. That seems redundant,
+	// but if an unrecovered panic happens, unwindm will
+	// restore the g->sched.sp from the stack location
+	// and then systemstack will try to use it. If we don't set it here,
+	// that restored SP will be uninitialized (typically 0) and
+	// will not be usable.
+	MOVD	g_m(g), O0
+	MOVD	m_g0(O0), O1
+	MOVD	BSP, TMP
+	MOVD	TMP, (g_sched+gobuf_sp)(O1)
+	MOVD	BFP, TMP
+	MOVD	TMP, (g_sched+gobuf_bp)(O1)
+
+havem:
+	// Now there's a valid m, and we're running on its m->g0.
+	// Save current m->g0->sched.sp on stack and then set it to SP.
+	// Save current sp in m->g0->sched.sp in preparation for
+	// switch back to m->curg stack.
+	// NOTE: unwindm knows that the saved g->sched.sp is at 8(I3) aka savedsp-16(SP).
+	MOVD	m_g0(O0), O1
+	MOVD	(g_sched+gobuf_sp)(O1), TMP
+	MOVD	(g_sched+gobuf_bp)(O1), RT1
+	MOVD	TMP, savedsp-16(SP)
+	MOVD	BSP, TMP
+	MOVD	TMP, (g_sched+gobuf_sp)(O1)
+	MOVD	BFP, TMP
+	MOVD	TMP, (g_sched+gobuf_bp)(O1)
+
+	// Switch to m->curg stack and call runtime.cgocallbackg.
+	// Because we are taking over the execution of m->curg
+	// but *not* resuming what had been running, we need to
+	// save that information (m->curg->sched) so we can restore it.
+	// We can restore m->curg->sched.sp easily, because calling
+	// runtime.cgocallbackg leaves SP unchanged upon return.
+	// To save m->curg->sched.pc, we push it onto the stack.
+	// This has the added benefit that it looks to the traceback
+	// routine like cgocallbackg is going to return to that
+	// PC (because the frame we allocate below has the same
+	// size as cgocallback_gofunc's frame declared above)
+	// so that the traceback will seamlessly trace back into
+	// the earlier calls.
+	//
+	// In the new goroutine, -8(SP) is unused.
+	MOVD	m_curg(O0), g
+	CALL	runtime·save_g(SB)
+
+	MOVD	(g_sched+gobuf_sp)(g), O1 // prepare stack as O1
+	MOVD	(g_sched+gobuf_pc)(g), O2
+	MOVD	O2, -(FIXED_FRAME+16)(O1)
+	MOVD	$-(FIXED_FRAME+16)(O1), TMP
+	MOVD	ctxt+24(FP), O3
+	MOVD	ZR, -8(O1)
+	MOVD	O3, -16(O1)
+	MOVD	TMP, BSP
+	MOVD	O1, BFP			// sched's %sp becomes %fp
+	SUB	$STACK_BIAS, O1
+	MOVD	O1, 112(TMP)
+	MOVD	O2, ILR
+	MOVD	ILR, 120(TMP)		// set return address for traceback
+	CALL	runtime·cgocallbackg(SB)
+
+	// Restore g->sched (== m->curg->sched) from saved values.
+	MOVD	0(BSP), TMP
+	MOVD	TMP, (g_sched+gobuf_pc)(g)
+	MOVD	$(FIXED_FRAME+16)(BSP), TMP
+	MOVD	TMP, (g_sched+gobuf_sp)(g)
+
+	// Switch back to m->g0's stack and restore m->g0->sched.sp.
+	// (Unlike m->curg, the g0 goroutine never uses sched.pc,
+	// so we do not have to restore it.)
+	MOVD	g_m(g), O0
+	MOVD	m_g0(O0), g
+	CALL	runtime·save_g(SB)
+
+	// retrieve BFP/ILR *before* switching stacks since a spill may
+	// overwrite the saved value
+	MOVD	(g_sched+gobuf_bp)(g), RT1
+	MOVD	(g_sched+gobuf_sp)(g), O0
+	MOVD	120(O0), O1
+	MOVD	O0, BSP
+	// set BFP/ILR *after* switching stacks to avoid spills to original
+	// stack; then manually spill to new stack to ensure Go itself can
+	// read the new values
+	MOVD	RT1, BFP
+	SUB	$STACK_BIAS, RT1
+	MOVD	RT1, 112(BSP)
+	MOVD	O1, ILR
+	MOVD	ILR, 120(BSP)
+	MOVD	savedsp-16(SP), TMP
+	MOVD	TMP, (g_sched+gobuf_sp)(g)
+
+	// If the m on entry was nil, we called needm above to borrow an m
+	// for the duration of the call. Since the call is over, return it with dropm.
+	MOVD	savedm-8(SP), TMP
+	CMP	TMP, ZR
+	BNED	droppedm
+	MOVD	$runtime·dropm(SB), RT1
+	CALL	(RT1)
+droppedm:
+
+	// Done!
+	RET
+
+// Called from cgo wrappers, this function returns g->m->curg.stack.hi.
+// Must obey the gcc calling convention.
+TEXT _cgo_topofstack(SB),NOSPLIT|REGWIN,$32
+	// g and RT1 might be clobbered by load_g. They
+	// are callee-save in the gcc calling convention, so save them.
+	MOVD	RT1, savedRT1-8(SP)
+	MOVD	g, saveG-16(SP)
+
+	CALL	runtime·load_g(SB)
+	MOVD	g_m(g), I0
+	MOVD	m_curg(I0), I0
+	MOVD	(g_stack+stack_hi)(I0), I0
+
+	MOVD	saveG-16(SP), g
+	MOVD	savedRT1-8(SP), RT1
+	RET
+
+// void setg(G*); set g. for use by needm.
+TEXT runtime·setg(SB), NOSPLIT, $0-8
+	MOVD	gg+0(FP), g
+	// This only happens if iscgo, so jump straight to save_g
+	CALL	runtime·save_g(SB)
+	RET
+
+// check that SP is in range [g->stack.lo, g->stack.hi)
+TEXT runtime·stackcheck(SB), NOSPLIT, $0
+	MOVD	BSP, I4
+	MOVD	(g_stack+stack_hi)(g), I3
+	CMP	I4, I3
+	BGED	2(PC);
+	UNDEF
+
+	MOVD	(g_stack+stack_lo)(g), I3
+	CMP	I3, I4
+	BGD	2(PC);
+	UNDEF
+
+	RET
+
+TEXT runtime·getcallerpc(SB),NOSPLIT,$16-16
+	MOVD	(8*15+FIXED_FRAME+16)(BSP), I1		// LR saved by caller
+	MOVD	runtime·stackBarrierPC(SB), I4
+	CMP	I1, I4
+	BNED	nobar
+	// Get original return PC.
+	CALL	runtime·nextBarrierPC(SB)
+	MOVD	FIXED_FRAME+0(BSP), I1
+nobar:
+	MOVD	I1, ret+8(FP)
+	RET
+
+TEXT runtime·setcallerpc(SB),NOSPLIT,$16-16
+	MOVD	pc+8(FP), I1
+	MOVD	(8*15+FIXED_FRAME+16)(BSP), I4
+	MOVD	runtime·stackBarrierPC(SB), L6
+	CMP	I4, L6
+	BED	setbar
+	MOVD	I1, (8*15+FIXED_FRAME+16)(BSP)		// set LR in caller
+	RET
+setbar:
+	// Set the stack barrier return PC.
+	MOVD	I1, FIXED_FRAME+0(BSP)
+	CALL	runtime·setNextBarrierPC(SB)
+	RET
+
+TEXT runtime·getcallersp(SB),NOSPLIT,$0-16
+	MOVD	argp+0(FP), I3
+	SUB	$FIXED_FRAME, I3
+	MOVD	I3, ret+8(FP)
+	RET
+
+TEXT runtime·abort(SB),NOSPLIT|NOFRAME,$0-0
+	JMPL	ZR, ZR
+	UNDEF
+
+// func cputicks() int64
+TEXT runtime·cputicks(SB),NOSPLIT,$0-0
+	RD	TICK, I3
+	MOVD	I3, ret+0(FP)
+	RET
+
+// memhash_varlen(p unsafe.Pointer, h seed) uintptr
+// redirects to memhash(p, h, size) using the size
+// stored in the closure.
+TEXT runtime·memhash_varlen(SB),NOSPLIT,$48-24
+	GO_ARGS
+	NO_LOCAL_POINTERS
+	MOVD	p+0(FP), I1
+	MOVD	h+8(FP), I4
+	MOVD	8(CTXT), L6
+	MOVD	I1, FIXED_FRAME+0(BSP)
+	MOVD	I4, FIXED_FRAME+8(BSP)
+	MOVD	L6, FIXED_FRAME+16(BSP)
+	CALL	runtime·memhash(SB)
+	MOVD	FIXED_FRAME+24(BSP), I1
+	MOVD	I1, ret+16(FP)
+	RET
+
+// memequal(p, q unsafe.Pointer, size uintptr) bool
+TEXT runtime·memequal(SB),NOSPLIT|NOFRAME,$0-25
+	MOVD	a+0(FP), I3
+	MOVD	b+8(FP), I5
+	MOVD	size+16(FP), I1
+	ADD	I3, I1, O1
+	MOVD	$1, TMP
+	MOVB	TMP, ret+24(FP)
+	CMP	I3, I5
+	BED	done
+loop:
+	CMP	I3, O1
+	BED	done
+	MOVUB	(I3), I4
+	ADD	$1, I3
+	MOVUB	(I5), L6
+	ADD	$1, I5
+	CMP	I4, L6
+	BED	loop
+
+	MOVB	ZR, ret+24(FP)
+done:
+	RET
+
+// memequal_varlen(a, b unsafe.Pointer) bool
+TEXT runtime·memequal_varlen(SB),NOSPLIT,$48-17
+	MOVD	a+0(FP), I1
+	MOVD	b+8(FP), I4
+	CMP	I1, I4
+	BED	eq
+	MOVD	8(CTXT), L6    // compiler stores size at offset 8 in the closure
+	MOVD	I1, FIXED_FRAME+0(BSP)
+	MOVD	I4, FIXED_FRAME+8(BSP)
+	MOVD	L6, FIXED_FRAME+16(BSP)
+	CALL	runtime·memequal(SB)
+	MOVD	$FIXED_FRAME+24(BSP), I1
+	MOVUB	(I1), I1
+	MOVB	I1, ret+16(FP)
+	RET
+eq:
+	MOVD	$1, I1
+	MOVB	I1, ret+16(FP)
+	RET
+
+//
+// functions for other packages
+//
+TEXT bytes·IndexByte(SB),NOSPLIT,$0-40
+	MOVD	s+0(FP), I1
+	MOVD	s_len+8(FP), I4
+	MOVUB	c+24(FP), L6	// byte to find
+	MOVD	I1, O1		// store base for later
+	SUB	$1, I1
+	ADD	I1, I4		// end-1
+
+loop:
+	CMP	I1, I4
+	BED	notfound
+	ADD	$1, I1
+	MOVUB	(I1), O0
+	CMP	L6, O0
+	BNEW	loop
+
+	SUB	O1, I1		// remove base
+	MOVD	I1, ret+32(FP)
+	RET
+
+notfound:
+	MOVD	$-1, I1
+	MOVD	I1, ret+32(FP)
+	RET
+
+TEXT strings·IndexByte(SB),NOSPLIT,$0-32
+	MOVD	p+0(FP), I1
+	MOVD	b_len+8(FP), I4
+	MOVUB	c+16(FP), L6	// byte to find
+	MOVD	I1, O1		// store base for later
+	SUB	$1, I1
+	ADD	I1, I4		// end-1
+
+loop:
+	CMP	I1, I4
+	BED	notfound
+	ADD	$1, I1
+	MOVUB	(I1), O0
+	CMP	L6, O0
+	BNEW	loop
+
+	SUB	O1, I1		// remove base
+	MOVD	I1, ret+24(FP)
+	RET
+
+notfound:
+	MOVD	$-1, I1
+	MOVD	I1, ret+24(FP)
+	RET
+
+TEXT runtime·fastrand1(SB),NOSPLIT|NOFRAME,$0-4
+	MOVD	g_m(g), I4
+	MOVUW	m_fastrand(I4), I1
+	ADD	I1, I1
+	CMP	ZR, I1
+	BGEW	2(PC)
+	XOR	$0x88888eef, I1
+	MOVW	I1, m_fastrand(I4)
+	MOVW	I1, ret+0(FP)
+	RET
+
+TEXT runtime·return0(SB), NOSPLIT, $0
+	MOVW	ZR, RT1
+	RET
+
+// The top-most function running on a goroutine
+// returns to goexit+PCQuantum.
+TEXT runtime·goexit(SB),NOSPLIT|NOFRAME,$0-0
+	MOVD	I3, I3	// NOP
+	CALL	runtime·goexit1(SB)	// does not return
+
+// TODO(aram):
+TEXT runtime·prefetcht0(SB),NOSPLIT|NOFRAME,$0-8
+	RET
+
+TEXT runtime·prefetcht1(SB),NOSPLIT|NOFRAME,$0-8
+	RET
+
+TEXT runtime·prefetcht2(SB),NOSPLIT|NOFRAME,$0-8
+	RET
+
+TEXT runtime·prefetchnta(SB),NOSPLIT|NOFRAME,$0-8
+	RET
+
+TEXT runtime·sigreturn(SB),NOSPLIT|NOFRAME,$0-8
+	RET
+
+// This is called from .init_array and follows the platform, not Go, ABI.
+TEXT runtime·addmoduledata(SB),NOSPLIT,$0-0
+	MOVD	runtime·lastmoduledatap(SB), I3
+	MOVD	O0, moduledata_next(I3)
+	MOVD	O0, runtime·lastmoduledatap(SB)
+	RET
+
+TEXT ·checkASM(SB),NOSPLIT,$0-1
+	OR	$1, ZR, I1
+	MOVB	I1, ret+0(FP)
+	RET
diff --git a/src/runtime/atomic_sparc64.s b/src/runtime/atomic_sparc64.s
new file mode 100644
index 0000000..598a0c8
--- /dev/null
+++ b/src/runtime/atomic_sparc64.s
@@ -0,0 +1,10 @@
+// Copyright 2016 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+#include "textflag.h"
+
+TEXT ·publicationBarrier(SB),NOSPLIT|NOFRAME,$0-0
+	// #MemIssue|#Sync|#LoadLoad|#StoreLoad|#LoadStore|#StoreStore
+	MEMBAR	$0x6f
+	RET
diff --git a/src/runtime/bytes_sparc64.go b/src/runtime/bytes_sparc64.go
new file mode 100644
index 0000000..fd4e646
--- /dev/null
+++ b/src/runtime/bytes_sparc64.go
@@ -0,0 +1,39 @@
+// Copyright 2016 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package runtime
+
+import "unsafe"
+
+var _ = unsafe.Pointer(uintptr(42))
+
+//go:linkname eqstring_go runtime.eqstring
+//go:nosplit
+func eqstring_go(s1, s2 string) bool {
+	if len(s1) != len(s2) {
+		return false
+	}
+	// optimization in assembly versions:
+	// if s1.str == s2.str { return true }
+	for i := 0; i < len(s1); i++ {
+		if s1[i] != s2[i] {
+			return false
+		}
+	}
+	return true
+}
+
+//go:linkname equal_bytes bytes.Equal
+//go:nosplit
+func equal_bytes(a, b []byte) bool {
+	if len(a) != len(b) {
+		return false
+	}
+	for i, c := range a {
+		if c != b[i] {
+			return false
+		}
+	}
+	return true
+}
diff --git a/src/runtime/cgo/asm_sparc64.s b/src/runtime/cgo/asm_sparc64.s
new file mode 100644
index 0000000..62b665b
--- /dev/null
+++ b/src/runtime/cgo/asm_sparc64.s
@@ -0,0 +1,38 @@
+// Copyright 2014 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// +build sparc64
+
+#include "textflag.h"
+#include "asm_sparc64.h"
+
+// Called by C code generated by cmd/cgo.
+// func crosscall2(fn func(a unsafe.Pointer, n int32, ctxt uintptr), a unsafe.Pointer, n int32, ctxt uintptr)
+// Saves C callee-saved registers and calls fn with three arguments.
+
+#define CROSS_FRAME 48
+
+// TODO(shawn): this probably has to be NOFRAME so it won't appear in
+// stack? Ideally, should really use register windows if possible.
+TEXT crosscall2(SB),NOSPLIT|REGWIN,$CROSS_FRAME
+	// Start with standard C stack frame layout and linkage
+
+	// Save %G2-%G3; the register window saved the rest.
+	MOVD	CTXT, (-8-0*8+CROSS_FRAME+FIXED_FRAME)(BSP)
+	MOVD	g, (-8-1*8+CROSS_FRAME+FIXED_FRAME)(BSP)
+
+	// Initialize Go ABI environment
+	CALL	runtime·reginit(SB)
+	CALL	runtime·load_g(SB)
+
+	MOVD	I1, FIXED_FRAME+0(BSP)	/* arg */
+	MOVD	I2, FIXED_FRAME+8(BSP)	/* argsize (includes padding) */
+	MOVD	I3, FIXED_FRAME+16(BSP)	/* ctxt */
+	CALL	(I0)
+
+	// Restore %G2-%G3; the rest will be restored at return.
+	MOVD	(-8-1*8+CROSS_FRAME+FIXED_FRAME)(BSP), g
+	MOVD	(-8-0*8+CROSS_FRAME+FIXED_FRAME)(BSP), CTXT
+
+	RET
diff --git a/src/runtime/cgo/gcc_solaris_sparc64.c b/src/runtime/cgo/gcc_solaris_sparc64.c
new file mode 100644
index 0000000..353b8a0
--- /dev/null
+++ b/src/runtime/cgo/gcc_solaris_sparc64.c
@@ -0,0 +1,79 @@
+// Copyright 2016 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// +build cgo
+
+#include <pthread.h>
+#include <string.h>
+#include <signal.h>
+#include <ucontext.h>
+#include "libcgo.h"
+
+static void* threadentry(void*);
+static void (*setg_gcc)(void*);
+
+void
+x_cgo_init(G *g, void (*setg)(void*))
+{
+	ucontext_t ctx;
+
+	setg_gcc = setg;
+	if (getcontext(&ctx) != 0)
+		perror("runtime/cgo: getcontext failed");
+	g->stacklo = (uintptr_t)ctx.uc_stack.ss_sp;
+
+	// Solaris processes report a tiny stack when run with "ulimit -s unlimited".
+	// Correct that as best we can: assume it's at least 1 MB.
+	// See golang.org/issue/12210.
+	if(ctx.uc_stack.ss_size < 1024*1024)
+		g->stacklo -= 1024*1024 - ctx.uc_stack.ss_size;
+}
+
+void
+_cgo_sys_thread_start(ThreadStart *ts)
+{
+	pthread_attr_t attr;
+	sigset_t ign, oset;
+	pthread_t p;
+	void *base;
+	size_t size;
+	int err;
+
+	sigfillset(&ign);
+	pthread_sigmask(SIG_SETMASK, &ign, &oset);
+
+	pthread_attr_init(&attr);
+
+	if (pthread_attr_getstack(&attr, &base, &size) != 0)
+		perror("runtime/cgo: pthread_attr_getstack failed");
+	if (size == 0) {
+		ts->g->stackhi = 2 << 20;
+		if (pthread_attr_setstack(&attr, NULL, ts->g->stackhi) != 0)
+			perror("runtime/cgo: pthread_attr_setstack failed");
+	} else {
+		ts->g->stackhi = size;
+	}
+	pthread_attr_setdetachstate(&attr, PTHREAD_CREATE_DETACHED);
+	err = pthread_create(&p, &attr, threadentry, ts);
+
+	pthread_sigmask(SIG_SETMASK, &oset, nil);
+
+	if (err != 0) {
+		fprintf(stderr, "runtime/cgo: pthread_create failed: %s\n", strerror(err));
+		abort();
+	}
+}
+
+extern void crosscall1(void (*fn)(void), void (*setg_gcc)(void*), void *g);
+static void*
+threadentry(void *v)
+{
+	ThreadStart ts;
+
+	ts = *(ThreadStart*)v;
+	free(v);
+
+	crosscall1(ts.fn, setg_gcc, (void*)ts.g);
+	return nil;
+}
diff --git a/src/runtime/cgo/gcc_sparc64.S b/src/runtime/cgo/gcc_sparc64.S
new file mode 100644
index 0000000..b09646c
--- /dev/null
+++ b/src/runtime/cgo/gcc_sparc64.S
@@ -0,0 +1,46 @@
+// Copyright 2016 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+#define STACK_BIAS 0x7ff
+
+/*
+ * Apple still insists on underscore prefixes for C function names.
+ */
+#if defined(__APPLE__)
+#define EXT(s) _##s
+#else
+#define EXT(s) s
+#endif
+
+// Solaris wants 4-byte alignment for SPARC code sections.
+.align	4
+
+/*
+ * void crosscall1(void (*fn)(void), void (*setg_gcc)(void *g), void *g)
+ *
+ * Calling into the gc tool chain, where all registers are caller save.
+ * Called from standard SPARC ABI, where r16-r29 are callee-save, so they
+ * must be saved explicitly.
+ */
+.globl EXT(crosscall1)
+EXT(crosscall1):
+	save	%sp, -304, %sp
+	flushw
+
+	mov	%i0, %l0
+	mov	%i1, %l1
+	mov	%i2, %o0
+
+	call %l1
+	nop
+	call %l0
+	nop
+
+	ret
+	restore
+#ifdef __ELF__
+.size	crosscall1,.-crosscall1
+.type	crosscall1,#function
+.section .note.GNU-stack,"",%progbits
+#endif
diff --git a/src/runtime/cgocall.go b/src/runtime/cgocall.go
index f8d6930..9b3d6da 100644
--- a/src/runtime/cgocall.go
+++ b/src/runtime/cgocall.go
@@ -251,9 +251,10 @@ func cgocallbackg1(ctxt uintptr) {
 	case "386":
 		// On 386, stack frame is three words, plus caller PC.
 		cb = (*args)(unsafe.Pointer(sp + 4*sys.PtrSize))
-	case "ppc64", "ppc64le", "s390x":
-		// On ppc64 and s390x, the callback arguments are in the arguments area of
-		// cgocallback's stack frame. The stack looks like this:
+	case "ppc64", "ppc64le", "s390x", "sparc64":
+		// On ppc64, s390x, and sparc64, the callback arguments are in
+		// the arguments area of cgocallback's stack frame. The stack
+		// looks like this:
 		// +--------------------+------------------------------+
 		// |                    | ...                          |
 		// | cgoexp_$fn         +------------------------------+
@@ -309,7 +310,7 @@ func unwindm(restore *bool) {
 	switch GOARCH {
 	default:
 		throw("unwindm not implemented")
-	case "386", "amd64", "arm", "ppc64", "ppc64le", "mips64", "mips64le", "s390x":
+	case "386", "amd64", "arm", "ppc64", "ppc64le", "mips64", "mips64le", "s390x", "sparc64":
 		sched.sp = *(*uintptr)(unsafe.Pointer(sched.sp + sys.MinFrameSize))
 	case "arm64":
 		sched.sp = *(*uintptr)(unsafe.Pointer(sched.sp + 16))
diff --git a/src/runtime/defs1_solaris_amd64.go b/src/runtime/defs1_solaris_amd64.go
index 3bb6f69..85a7b40 100644
--- a/src/runtime/defs1_solaris_amd64.go
+++ b/src/runtime/defs1_solaris_amd64.go
@@ -78,6 +78,7 @@ const (
 	_ITIMER_VIRTUAL = 0x1
 	_ITIMER_PROF    = 0x2
 
+	__SC_PAGESIZE         = 0xb
 	__SC_NPROCESSORS_ONLN = 0xf
 
 	_PTHREAD_CREATE_DETACHED = 0x40
diff --git a/src/runtime/defs1_solaris_sparc64.go b/src/runtime/defs1_solaris_sparc64.go
new file mode 100644
index 0000000..81fb74a
--- /dev/null
+++ b/src/runtime/defs1_solaris_sparc64.go
@@ -0,0 +1,248 @@
+// created by cgo -godefs and then manually edited.
+// CC=sparcv9-solaris2.12-gcc GOARCH=sparc64 GOOS=solaris go tool cgo -godefs defs_solaris.go defs_solaris_sparc64.go
+
+package runtime
+
+const (
+	_EINTR       = 0x4
+	_EBADF       = 0x9
+	_EFAULT      = 0xe
+	_EAGAIN      = 0xb
+	_ETIMEDOUT   = 0x91
+	_EWOULDBLOCK = 0xb
+	_EINPROGRESS = 0x96
+
+	_PROT_NONE  = 0x0
+	_PROT_READ  = 0x1
+	_PROT_WRITE = 0x2
+	_PROT_EXEC  = 0x4
+
+	_MAP_ANON    = 0x100
+	_MAP_PRIVATE = 0x2
+	_MAP_FIXED   = 0x10
+
+	_MADV_FREE = 0x5
+
+	_SA_SIGINFO = 0x8
+	_SA_RESTART = 0x4
+	_SA_ONSTACK = 0x1
+
+	_SIGHUP    = 0x1
+	_SIGINT    = 0x2
+	_SIGQUIT   = 0x3
+	_SIGILL    = 0x4
+	_SIGTRAP   = 0x5
+	_SIGABRT   = 0x6
+	_SIGEMT    = 0x7
+	_SIGFPE    = 0x8
+	_SIGKILL   = 0x9
+	_SIGBUS    = 0xa
+	_SIGSEGV   = 0xb
+	_SIGSYS    = 0xc
+	_SIGPIPE   = 0xd
+	_SIGALRM   = 0xe
+	_SIGTERM   = 0xf
+	_SIGURG    = 0x15
+	_SIGSTOP   = 0x17
+	_SIGTSTP   = 0x18
+	_SIGCONT   = 0x19
+	_SIGCHLD   = 0x12
+	_SIGTTIN   = 0x1a
+	_SIGTTOU   = 0x1b
+	_SIGIO     = 0x16
+	_SIGXCPU   = 0x1e
+	_SIGXFSZ   = 0x1f
+	_SIGVTALRM = 0x1c
+	_SIGPROF   = 0x1d
+	_SIGWINCH  = 0x14
+	_SIGUSR1   = 0x10
+	_SIGUSR2   = 0x11
+
+	_FPE_INTDIV = 0x1
+	_FPE_INTOVF = 0x2
+	_FPE_FLTDIV = 0x3
+	_FPE_FLTOVF = 0x4
+	_FPE_FLTUND = 0x5
+	_FPE_FLTRES = 0x6
+	_FPE_FLTINV = 0x7
+	_FPE_FLTSUB = 0x8
+
+	_BUS_ADRALN = 0x1
+	_BUS_ADRERR = 0x2
+	_BUS_OBJERR = 0x3
+
+	_SEGV_MAPERR = 0x1
+	_SEGV_ACCERR = 0x2
+
+	_ITIMER_REAL    = 0x0
+	_ITIMER_VIRTUAL = 0x1
+	_ITIMER_PROF    = 0x2
+
+	__SC_PAGESIZE         = 0xb
+	__SC_NPROCESSORS_ONLN = 0xf
+
+	_PTHREAD_CREATE_DETACHED = 0x40
+
+	_FORK_NOSIGCHLD = 0x1
+	_FORK_WAITPID   = 0x2
+
+	_MAXHOSTNAMELEN = 0x100
+
+	_O_NONBLOCK = 0x80
+	_FD_CLOEXEC = 0x1
+	_F_GETFL    = 0x3
+	_F_SETFL    = 0x4
+	_F_SETFD    = 0x2
+
+	_POLLIN  = 0x1
+	_POLLOUT = 0x4
+	_POLLHUP = 0x10
+	_POLLERR = 0x8
+
+	_PORT_SOURCE_FD = 0x4
+)
+
+type semt struct {
+	sem_count uint32
+	sem_type  uint16
+	sem_magic uint16
+	sem_pad1  [3]uint64
+	sem_pad2  [2]uint64
+}
+
+type sigaltstackt struct {
+	ss_sp     *byte
+	ss_size   uint64
+	ss_flags  int32
+	pad_cgo_0 [4]byte
+}
+
+type sigset struct {
+	__sigbits [4]uint32
+}
+
+type stackt struct {
+	ss_sp     *byte
+	ss_size   uint64
+	ss_flags  int32
+	pad_cgo_0 [4]byte
+}
+
+type siginfo struct {
+	si_signo int32
+	si_code  int32
+	si_errno int32
+	si_pad   int32
+	__data   [240]byte
+}
+
+type sigactiont struct {
+	sa_flags  int32
+	pad_cgo_0 [4]byte
+	_funcptr  [8]byte
+	sa_mask   sigset
+}
+
+type fpregset struct {
+	fp_reg_set [288]byte // lots of stuff here, but we treat it as opaque.
+}
+
+type mcontext struct {
+	gregs  [21]int64
+	gwins  *gwindows
+	fpregs fpregset
+	// additional stuff follows, but for our purpose, this struct is opaque.
+}
+
+type ucontext struct {
+	uc_flags    uint32
+	pad_cgo_0   [4]byte
+	uc_link     *ucontext
+	uc_sigmask  sigset
+	uc_stack    stackt
+	pad_cgo_1   [8]byte
+	uc_mcontext mcontext
+	uc_filler   [4]int64
+}
+
+type gwindows struct {
+	wbcnt     int32
+	pad_cgo_0 [4]byte
+	spbuf     [31]*int64
+	wbuf      [31]rwindow
+}
+type rwindow struct {
+	local [8]int64
+	in    [8]int64
+}
+
+type timespec struct {
+	tv_sec  int64
+	tv_nsec int64
+}
+
+type timeval struct {
+	tv_sec  int64
+	tv_usec int64
+}
+
+func (tv *timeval) set_usec(x int32) {
+	tv.tv_usec = int64(x)
+}
+
+type itimerval struct {
+	it_interval timeval
+	it_value    timeval
+}
+
+type portevent struct {
+	portev_events int32
+	portev_source uint16
+	portev_pad    uint16
+	portev_object uint64
+	portev_user   *byte
+}
+
+type pthread uint32
+type pthreadattr struct {
+	__pthread_attrp *byte
+}
+
+type stat struct {
+	st_dev     uint64
+	st_ino     uint64
+	st_mode    uint32
+	st_nlink   uint32
+	st_uid     uint32
+	st_gid     uint32
+	st_rdev    uint64
+	st_size    int64
+	st_atim    timespec
+	st_mtim    timespec
+	st_ctim    timespec
+	st_blksize int32
+	pad_cgo_0  [4]byte
+	st_blocks  int64
+	st_fstype  [16]int8
+}
+
+const (
+	_REG_CCR = 0x0
+	_REG_PC  = 0x1
+	_REG_nPC = 0x2
+	_REG_G1  = 0x4
+	_REG_G2  = 0x5
+	_REG_G3  = 0x6
+	_REG_G4  = 0x7
+	_REG_G5  = 0x8
+	_REG_G6  = 0x9
+	_REG_G7  = 0xa
+	_REG_O0  = 0xb
+	_REG_O1  = 0xc
+	_REG_O2  = 0xd
+	_REG_O3  = 0xe
+	_REG_O4  = 0xf
+	_REG_O5  = 0x10
+	_REG_O6  = 0x11
+	_REG_O7  = 0x12
+)
diff --git a/src/runtime/defs_solaris.go b/src/runtime/defs_solaris.go
index ba44e5f..83e94f9 100644
--- a/src/runtime/defs_solaris.go
+++ b/src/runtime/defs_solaris.go
@@ -143,6 +143,8 @@ type Sigaction C.struct_sigaction
 type Fpregset C.fpregset_t
 type Mcontext C.mcontext_t
 type Ucontext C.ucontext_t
+type Gwindows C.gwindows_t
+type Rwindow C.struct_rwindow
 
 type Timespec C.struct_timespec
 type Timeval C.struct_timeval
diff --git a/src/runtime/defs_solaris_sparc64.go b/src/runtime/defs_solaris_sparc64.go
new file mode 100644
index 0000000..3107637
--- /dev/null
+++ b/src/runtime/defs_solaris_sparc64.go
@@ -0,0 +1,40 @@
+// Copyright 2016 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// +build ignore
+
+/*
+Input to cgo.
+
+GOARCH=sparc64 go tool cgo -godefs defs_solaris_sparc64.go >defs1_solaris_sparc64.go
+*/
+
+package runtime
+
+/*
+#include <sys/types.h>
+#include <sys/regset.h>
+*/
+import "C"
+
+const (
+	REG_CCR = C.REG_CCR
+	REG_PC  = C.REG_PC
+	REG_nPC = C.REG_nPC
+	REG_G1  = C.REG_G1
+	REG_G2  = C.REG_G2
+	REG_G3  = C.REG_G3
+	REG_G4  = C.REG_G4
+	REG_G5  = C.REG_G5
+	REG_G6  = C.REG_G6
+	REG_G7  = C.REG_G7
+	REG_O0  = C.REG_O0
+	REG_O1  = C.REG_O1
+	REG_O2  = C.REG_O2
+	REG_O3  = C.REG_O3
+	REG_O4  = C.REG_O4
+	REG_O5  = C.REG_O5
+	REG_O6  = C.REG_O6
+	REG_O7  = C.REG_O7
+)
diff --git a/src/runtime/die.awk b/src/runtime/die.awk
new file mode 100644
index 0000000..773d0e7
--- /dev/null
+++ b/src/runtime/die.awk
@@ -0,0 +1,19 @@
+#!/usr/bin/awk -f
+
+! /DIE/ {
+	printf("%s\n", $0)
+}
+
+/DIE/ {
+	code++
+	printf("	// TODO(aram):\n");
+	printf("	MOVD	$%d, TMP\n", code);
+	printf("	ADD	$'!', TMP, TMP\n");
+	printf("	MOVD	TMP, dbgbuf(SB)\n");
+	printf("	MOVD	$2, R8\n");
+	printf("	MOVD	$dbgbuf(SB), R9\n");
+	printf("	MOVD	$2, R10\n");
+	printf("	MOVD	$libc_exit(SB), TMP\n");
+	printf("	CALL	TMP\n")
+	printf("	UNDEF\n")
+}
diff --git a/src/runtime/duff_sparc64.s b/src/runtime/duff_sparc64.s
new file mode 100644
index 0000000..60fd447
--- /dev/null
+++ b/src/runtime/duff_sparc64.s
@@ -0,0 +1,266 @@
+// AUTO-GENERATED by mkduff.go
+// Run go generate from src/runtime to update.
+// See mkduff.go for comments.
+
+#include "textflag.h"
+
+TEXT runtime·duffzero(SB), NOSPLIT|NOFRAME, $0-0
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	MOVD	ZR, 8(RT1)
+	ADD	$8, RT1
+	RET
+
+// TODO: Implement runtime·duffcopy.
diff --git a/src/runtime/export_mmap_test.go b/src/runtime/export_mmap_test.go
index bc8191e..f569627 100644
--- a/src/runtime/export_mmap_test.go
+++ b/src/runtime/export_mmap_test.go
@@ -9,7 +9,13 @@
 package runtime
 
 var Mmap = mmap
+var Munmap = munmap
 
 const ENOMEM = _ENOMEM
 const MAP_ANON = _MAP_ANON
 const MAP_PRIVATE = _MAP_PRIVATE
+const MAP_FIXED = _MAP_FIXED
+
+func GetPhysPageSize() uintptr {
+	return physPageSize
+}
diff --git a/src/runtime/extern.go b/src/runtime/extern.go
index 441dcd9..e09c3d0 100644
--- a/src/runtime/extern.go
+++ b/src/runtime/extern.go
@@ -189,7 +189,8 @@ func Caller(skip int) (pc uintptr, file string, line int, ok bool) {
 	// All architectures turn faults into apparent calls to sigpanic.
 	// If we see a call to sigpanic, we do not back up the PC to find
 	// the line number of the call instruction, because there is no call.
-	if xpc > f.entry && (g == nil || g.entry != funcPC(sigpanic)) {
+	// SPARC64's PC holds the address of the *current* instruction.
+	if xpc > f.entry && (g == nil || g.entry != funcPC(sigpanic)) && sys.GoarchSparc64 == 0 {
 		xpc--
 	}
 	file, line32 := funcline(f, xpc)
diff --git a/src/runtime/gcinfo_test.go b/src/runtime/gcinfo_test.go
index 011f005..abaeb9a 100644
--- a/src/runtime/gcinfo_test.go
+++ b/src/runtime/gcinfo_test.go
@@ -147,7 +147,7 @@ func infoBigStruct() []byte {
 			typeScalar, typeScalar, typeScalar, typeScalar, // t int; y uint16; u uint64
 			typePointer, typeScalar, // i string
 		}
-	case "arm64", "amd64", "mips64", "mips64le", "ppc64", "ppc64le", "s390x":
+	case "arm64", "amd64", "mips64", "mips64le", "ppc64", "ppc64le", "s390x", "sparc64":
 		return []byte{
 			typePointer,                        // q *int
 			typeScalar, typeScalar, typeScalar, // w byte; e [17]byte
diff --git a/src/runtime/hash64.go b/src/runtime/hash64.go
index d61f114..338b1cd 100644
--- a/src/runtime/hash64.go
+++ b/src/runtime/hash64.go
@@ -6,7 +6,7 @@
 //   xxhash: https://code.google.com/p/xxhash/
 // cityhash: https://code.google.com/p/cityhash/
 
-// +build amd64 amd64p32 arm64 mips64 mips64le ppc64 ppc64le s390x
+// +build amd64 amd64p32 arm64 mips64 mips64le ppc64 ppc64le s390x sparc64
 
 package runtime
 
diff --git a/src/runtime/heapdump.go b/src/runtime/heapdump.go
index c317b5f..976d82b 100644
--- a/src/runtime/heapdump.go
+++ b/src/runtime/heapdump.go
@@ -249,7 +249,8 @@ func dumpframe(s *stkframe, arg unsafe.Pointer) bool {
 
 	// Figure out what we can about our stack map
 	pc := s.pc
-	if pc != f.entry {
+	// SPARC64's PC holds the address of the *current* instruction.
+	if pc != f.entry && sys.GoarchSparc64 == 0 {
 		pc--
 	}
 	pcdata := pcdatavalue(f, _PCDATA_StackMapIndex, pc, nil)
@@ -594,7 +595,8 @@ func dumpmemprof_callback(b *bucket, nstk uintptr, pstk *uintptr, size, allocs,
 			dumpint(0)
 		} else {
 			dumpstr(funcname(f))
-			if i > 0 && pc > f.entry {
+			// SPARC64's PC holds the address of the *current* instruction.
+			if i > 0 && pc > f.entry && sys.GoarchSparc64 == 0 {
 				pc--
 			}
 			file, line := funcline(f, pc)
diff --git a/src/runtime/internal/atomic/asm_sparc64.s b/src/runtime/internal/atomic/asm_sparc64.s
new file mode 100644
index 0000000..1b2b215
--- /dev/null
+++ b/src/runtime/internal/atomic/asm_sparc64.s
@@ -0,0 +1,165 @@
+// Copyright 2016 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+#include "textflag.h"
+#include "asm_sparc64.h"
+
+// bool cas(uint32 *ptr, uint32 old, uint32 new)
+// Atomically:
+//	if(*val == old){
+//		*val = new;
+//		return 1;
+//	} else
+//		return 0;
+TEXT runtime∕internal∕atomic·Cas(SB), NOSPLIT, $0-17
+	MOVD	ptr+0(FP), I1
+	MOVUW	old+8(FP), I3
+	MOVUW	new+12(FP), I5
+	MEM_SYNC
+	CASW	(I1), I3, I5
+	CMP	I5, I3
+	MOVD	$0, I3
+	MOVE	ICC, $1, I3
+	MEM_SYNC
+	MOVB	I3, ret+16(FP)
+	RET
+
+// bool	runtime∕internal∕atomic·Cas64(uint64 *ptr, uint64 old, uint64 new)
+// Atomically:
+//	if(*val == *old){
+//		*val = new;
+//		return 1;
+//	} else {
+//		return 0;
+//	}
+TEXT runtime∕internal∕atomic·Cas64(SB), NOSPLIT, $0-25
+	MOVD	ptr+0(FP), I1
+	MOVD	old+8(FP), I3
+	MOVD	new+16(FP), I5
+	MEM_SYNC
+	CASD	(I1), I3, I5
+	CMP	I5, I3
+	MOVD	$0, I3
+	MOVE	XCC, $1, I3
+	MEM_SYNC
+	MOVB	I3, ret+24(FP)
+	RET
+
+TEXT runtime∕internal∕atomic·Casuintptr(SB), NOSPLIT|NOFRAME, $0-25
+	JMP	runtime∕internal∕atomic·Cas64(SB)
+
+TEXT runtime∕internal∕atomic·Loaduintptr(SB), NOSPLIT|NOFRAME, $0-16
+	JMP	runtime∕internal∕atomic·Load64(SB)
+
+TEXT runtime∕internal∕atomic·Loaduint(SB), NOSPLIT|NOFRAME, $0-16
+	JMP	runtime∕internal∕atomic·Load64(SB)
+
+TEXT runtime∕internal∕atomic·Storeuintptr(SB), NOSPLIT|NOFRAME, $0-16
+	JMP	runtime∕internal∕atomic·Store64(SB)
+
+TEXT runtime∕internal∕atomic·Xadduintptr(SB), NOSPLIT|NOFRAME, $0-24
+	JMP	runtime∕internal∕atomic·Xadd64(SB)
+
+TEXT runtime∕internal∕atomic·Loadint64(SB), NOSPLIT|NOFRAME, $0-16
+	JMP	runtime∕internal∕atomic·Load64(SB)
+
+TEXT runtime∕internal∕atomic·Xaddint64(SB), NOSPLIT|NOFRAME, $0-24
+	JMP	runtime∕internal∕atomic·Xadd64(SB)
+
+// bool casp(void **val, void *old, void *new)
+// Atomically:
+//	if(*val == old){
+//		*val = new;
+//		return 1;
+//	} else
+//		return 0;
+TEXT runtime∕internal∕atomic·Casp1(SB), NOSPLIT|NOFRAME, $0-25
+	JMP runtime∕internal∕atomic·Cas64(SB)
+
+// uint32 xadd(uint32 volatile *ptr, int32 delta)
+// Atomically:
+//	*val += delta;
+//	return *val;
+TEXT runtime∕internal∕atomic·Xadd(SB), NOSPLIT, $0-20
+	MOVD	ptr+0(FP), I4
+	MOVUW	delta+8(FP), I3
+	MOVUW	(I4), I1
+	MEM_SYNC
+retry:
+	ADD	I1, I3, I5
+	CASW	(I4), I1, I5
+	CMP	I1, I5
+	MOVNE	ICC, I5, I1
+	BNEW	retry
+	ADD	I1, I3, I5
+	MEM_SYNC
+	MOVUW	I5, ret+16(FP)
+	RET
+
+TEXT runtime∕internal∕atomic·Xadd64(SB), NOSPLIT, $0-24
+	MOVD	ptr+0(FP), I4
+	MOVD	delta+8(FP), I3
+	MEM_SYNC
+	MOVD	(I4), I1
+retry:
+	ADD	I1, I3, I5
+	CASD	(I4), I1, I5
+	CMP	I1, I5
+	MOVNE	XCC, I5, I1
+	BNED	retry
+	ADD	I1, I3, I5
+	MEM_SYNC
+	MOVD	I5, ret+16(FP)
+	RET
+
+TEXT runtime∕internal∕atomic·Xchg(SB), NOSPLIT, $0-20
+	MOVD	ptr+0(FP), I3
+	MOVUW	new+8(FP), I1
+again:
+	MEM_SYNC
+	MOVUW	(I3), I5
+	CASW	(I3), I5, I1
+	CMP	I1, I5
+	BNEW	again
+	MEM_SYNC
+	MOVUW	I5, ret+16(FP)
+	RET
+
+TEXT runtime∕internal∕atomic·Xchg64(SB), NOSPLIT, $0-24
+	MOVD	ptr+0(FP), I3
+	MOVD	new+8(FP), I1
+again:
+	MEM_SYNC
+	MOVD	(I3), I5
+	CASD	(I3), I5, I1
+	CMP	I1, I5
+	BNED	again
+	MEM_SYNC
+	MOVD	I5, ret+16(FP)
+	RET
+
+TEXT runtime∕internal∕atomic·Xchguintptr(SB), NOSPLIT|NOFRAME, $0-24
+	JMP	runtime∕internal∕atomic·Xchg64(SB)
+
+
+// TODO(shawn): verify this is performed without a write barrier;
+// see #15270.
+TEXT runtime∕internal∕atomic·StorepNoWB(SB), NOSPLIT|NOFRAME, $0-16
+	JMP	runtime∕internal∕atomic·Store64(SB)
+
+TEXT runtime∕internal∕atomic·Store(SB), NOSPLIT, $0-12
+	MOVD	ptr+0(FP), I3
+	MOVUW	val+8(FP), I5
+	MEM_SYNC
+	STW	I5, (I3)
+	MEM_SYNC
+	RET
+
+TEXT runtime∕internal∕atomic·Store64(SB), NOSPLIT, $0-16
+	MOVD	ptr+0(FP), I3
+	MOVD	val+8(FP), I5
+	MEM_SYNC
+	STD	I5, (I3)
+	MEM_SYNC
+	RET
diff --git a/src/runtime/internal/atomic/atomic_sparc64.go b/src/runtime/internal/atomic/atomic_sparc64.go
new file mode 100644
index 0000000..cf27e73
--- /dev/null
+++ b/src/runtime/internal/atomic/atomic_sparc64.go
@@ -0,0 +1,80 @@
+// Copyright 2016 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package atomic
+
+import "unsafe"
+
+//go:noescape
+func Xadd(ptr *uint32, delta int32) uint32
+
+//go:noescape
+func Xadd64(ptr *uint64, delta int64) uint64
+
+//go:noescape
+func Xadduintptr(ptr *uintptr, delta uintptr) uintptr
+
+//go:noescape
+func Xchg(ptr *uint32, new uint32) uint32
+
+//go:noescape
+func Xchg64(ptr *uint64, new uint64) uint64
+
+//go:noescape
+func Xchguintptr(ptr *uintptr, new uintptr) uintptr
+
+//go:noescape
+func Load(ptr *uint32) uint32
+
+//go:noescape
+func Load64(ptr *uint64) uint64
+
+//go:noescape
+func Loadp(ptr unsafe.Pointer) unsafe.Pointer
+
+//go:nosplit
+func And8(addr *uint8, v uint8) {
+	// TODO(aram) implement this in asm.
+	// Align down to 4 bytes and use 32-bit CAS.
+	uaddr := uintptr(unsafe.Pointer(addr))
+	addr32 := (*uint32)(unsafe.Pointer(uaddr &^ 3))
+	word := (uint32(v) << 24) >> ((uaddr & 3) * 8)    // big endian
+	mask := (uint32(0xFF) << 24) >> ((uaddr & 3) * 8) // big endian
+	word |= ^mask
+	for {
+		old := *addr32
+		if Cas(addr32, old, old&word) {
+			return
+		}
+	}
+}
+
+//go:nosplit
+func Or8(addr *uint8, v uint8) {
+	// TODO(aram) implement this in asm.
+	// Align down to 4 bytes and use 32-bit CAS.
+	uaddr := uintptr(unsafe.Pointer(addr))
+	addr32 := (*uint32)(unsafe.Pointer(uaddr &^ 3))
+	word := (uint32(v) << 24) >> ((uaddr & 3) * 8) // big endian
+	for {
+		old := *addr32
+		if Cas(addr32, old, old|word) {
+			return
+		}
+	}
+}
+
+// NOTE: Do not add atomicxor8 (XOR is not idempotent).
+
+//go:noescape
+func Cas64(ptr *uint64, old, new uint64) bool
+
+//go:noescape
+func Store(ptr *uint32, val uint32)
+
+//go:noescape
+func Store64(ptr *uint64, val uint64)
+
+// NO go:noescape annotation; see atomic_pointer.go.
+func StorepNoWB(ptr unsafe.Pointer, val unsafe.Pointer)
diff --git a/src/runtime/internal/atomic/atomic_sparc64.s b/src/runtime/internal/atomic/atomic_sparc64.s
new file mode 100644
index 0000000..e362a9e
--- /dev/null
+++ b/src/runtime/internal/atomic/atomic_sparc64.s
@@ -0,0 +1,28 @@
+// Copyright 2016 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+#include "textflag.h"
+#include "asm_sparc64.h"
+
+// uint32 runtime∕internal∕atomic·Load(uint32 volatile* addr)
+TEXT ·Load(SB),NOSPLIT,$0-12
+	MOVD	ptr+0(FP), I1
+	MEM_SYNC
+	LDUW	(I1), I1
+	MEM_SYNC
+	MOVUW	I1, ret+8(FP)
+	RET
+
+// uint64 runtime∕internal∕atomic·Load64(uint64 volatile* addr)
+TEXT ·Load64(SB),NOSPLIT,$0-16
+	MOVD	ptr+0(FP), I1
+	MEM_SYNC
+	LDD	(I1), I1
+	MEM_SYNC
+	MOVD	I1, ret+8(FP)
+	RET
+
+// void *runtime∕internal∕atomic·Loadp(void *volatile *addr)
+TEXT ·Loadp(SB),NOSPLIT|NOFRAME,$0-16
+	JMP	runtime∕internal∕atomic·Load64(SB)
diff --git a/src/runtime/internal/sys/arch.go b/src/runtime/internal/sys/arch.go
index c175704..8920645 100644
--- a/src/runtime/internal/sys/arch.go
+++ b/src/runtime/internal/sys/arch.go
@@ -14,4 +14,5 @@ const (
 	MIPS64
 	PPC64
 	S390X
+	SPARC64
 )
diff --git a/src/runtime/internal/sys/arch_386.go b/src/runtime/internal/sys/arch_386.go
index 48c42f7..33f4eaf 100644
--- a/src/runtime/internal/sys/arch_386.go
+++ b/src/runtime/internal/sys/arch_386.go
@@ -5,14 +5,15 @@
 package sys
 
 const (
-	ArchFamily    = I386
-	BigEndian     = 0
-	CacheLineSize = 64
-	PhysPageSize  = GoosNacl*65536 + (1-GoosNacl)*4096 // 4k normally; 64k on NaCl
-	PCQuantum     = 1
-	Int64Align    = 4
-	HugePageSize  = 1 << 21
-	MinFrameSize  = 0
+	ArchFamily          = I386
+	BigEndian           = 0
+	CacheLineSize       = 64
+	DefaultPhysPageSize = GoosNacl*65536 + (1-GoosNacl)*4096 // 4k normally; 64k on NaCl
+	PCQuantum           = 1
+	Int64Align          = 4
+	HugePageSize        = 1 << 21
+	MinFrameSize        = 0
+	SpAlign             = 1
 )
 
 type Uintreg uint32
diff --git a/src/runtime/internal/sys/arch_amd64.go b/src/runtime/internal/sys/arch_amd64.go
index 1bbdb99..c8be77a 100644
--- a/src/runtime/internal/sys/arch_amd64.go
+++ b/src/runtime/internal/sys/arch_amd64.go
@@ -5,14 +5,15 @@
 package sys
 
 const (
-	ArchFamily    = AMD64
-	BigEndian     = 0
-	CacheLineSize = 64
-	PhysPageSize  = 4096
-	PCQuantum     = 1
-	Int64Align    = 8
-	HugePageSize  = 1 << 21
-	MinFrameSize  = 0
+	ArchFamily          = AMD64
+	BigEndian           = 0
+	CacheLineSize       = 64
+	DefaultPhysPageSize = 4096
+	PCQuantum           = 1
+	Int64Align          = 8
+	HugePageSize        = 1 << 21
+	MinFrameSize        = 0
+	SpAlign             = 1
 )
 
 type Uintreg uint64
diff --git a/src/runtime/internal/sys/arch_amd64p32.go b/src/runtime/internal/sys/arch_amd64p32.go
index b7011a4..a7e2d49 100644
--- a/src/runtime/internal/sys/arch_amd64p32.go
+++ b/src/runtime/internal/sys/arch_amd64p32.go
@@ -5,14 +5,15 @@
 package sys
 
 const (
-	ArchFamily    = AMD64
-	BigEndian     = 0
-	CacheLineSize = 64
-	PhysPageSize  = 65536*GoosNacl + 4096*(1-GoosNacl)
-	PCQuantum     = 1
-	Int64Align    = 8
-	HugePageSize  = 1 << 21
-	MinFrameSize  = 0
+	ArchFamily          = AMD64
+	BigEndian           = 0
+	CacheLineSize       = 64
+	DefaultPhysPageSize = 65536*GoosNacl + 4096*(1-GoosNacl)
+	PCQuantum           = 1
+	Int64Align          = 8
+	HugePageSize        = 1 << 21
+	MinFrameSize        = 0
+	SpAlign             = 1
 )
 
 type Uintreg uint64
diff --git a/src/runtime/internal/sys/arch_arm.go b/src/runtime/internal/sys/arch_arm.go
index f90f52d..f85b593 100644
--- a/src/runtime/internal/sys/arch_arm.go
+++ b/src/runtime/internal/sys/arch_arm.go
@@ -5,14 +5,15 @@
 package sys
 
 const (
-	ArchFamily    = ARM
-	BigEndian     = 0
-	CacheLineSize = 32
-	PhysPageSize  = 65536*GoosNacl + 4096*(1-GoosNacl)
-	PCQuantum     = 4
-	Int64Align    = 4
-	HugePageSize  = 0
-	MinFrameSize  = 4
+	ArchFamily          = ARM
+	BigEndian           = 0
+	CacheLineSize       = 32
+	DefaultPhysPageSize = 65536
+	PCQuantum           = 4
+	Int64Align          = 4
+	HugePageSize        = 0
+	MinFrameSize        = 4
+	SpAlign             = 1
 )
 
 type Uintreg uint32
diff --git a/src/runtime/internal/sys/arch_arm64.go b/src/runtime/internal/sys/arch_arm64.go
index aaaa4b0..61aa363 100644
--- a/src/runtime/internal/sys/arch_arm64.go
+++ b/src/runtime/internal/sys/arch_arm64.go
@@ -5,14 +5,15 @@
 package sys
 
 const (
-	ArchFamily    = ARM64
-	BigEndian     = 0
-	CacheLineSize = 32
-	PhysPageSize  = 65536
-	PCQuantum     = 4
-	Int64Align    = 8
-	HugePageSize  = 0
-	MinFrameSize  = 8
+	ArchFamily          = ARM64
+	BigEndian           = 0
+	CacheLineSize       = 32
+	DefaultPhysPageSize = 65536
+	PCQuantum           = 4
+	Int64Align          = 8
+	HugePageSize        = 0
+	MinFrameSize        = 8
+	SpAlign             = 1
 )
 
 type Uintreg uint64
diff --git a/src/runtime/internal/sys/arch_mips64.go b/src/runtime/internal/sys/arch_mips64.go
index d567259..6b7dd3b 100644
--- a/src/runtime/internal/sys/arch_mips64.go
+++ b/src/runtime/internal/sys/arch_mips64.go
@@ -5,14 +5,15 @@
 package sys
 
 const (
-	ArchFamily    = MIPS64
-	BigEndian     = 1
-	CacheLineSize = 32
-	PhysPageSize  = 16384
-	PCQuantum     = 4
-	Int64Align    = 8
-	HugePageSize  = 0
-	MinFrameSize  = 8
+	ArchFamily          = MIPS64
+	BigEndian           = 1
+	CacheLineSize       = 32
+	DefaultPhysPageSize = 16384
+	PCQuantum           = 4
+	Int64Align          = 8
+	HugePageSize        = 0
+	MinFrameSize        = 8
+	SpAlign             = 1
 )
 
 type Uintreg uint64
diff --git a/src/runtime/internal/sys/arch_mips64le.go b/src/runtime/internal/sys/arch_mips64le.go
index f8cdf2b..7decad5 100644
--- a/src/runtime/internal/sys/arch_mips64le.go
+++ b/src/runtime/internal/sys/arch_mips64le.go
@@ -5,14 +5,15 @@
 package sys
 
 const (
-	ArchFamily    = MIPS64
-	BigEndian     = 0
-	CacheLineSize = 32
-	PhysPageSize  = 16384
-	PCQuantum     = 4
-	Int64Align    = 8
-	HugePageSize  = 0
-	MinFrameSize  = 8
+	ArchFamily          = MIPS64
+	BigEndian           = 0
+	CacheLineSize       = 32
+	DefaultPhysPageSize = 16384
+	PCQuantum           = 4
+	Int64Align          = 8
+	HugePageSize        = 0
+	MinFrameSize        = 8
+	SpAlign             = 1
 )
 
 type Uintreg uint64
diff --git a/src/runtime/internal/sys/arch_ppc64.go b/src/runtime/internal/sys/arch_ppc64.go
index cdec63f..5007352 100644
--- a/src/runtime/internal/sys/arch_ppc64.go
+++ b/src/runtime/internal/sys/arch_ppc64.go
@@ -5,14 +5,15 @@
 package sys
 
 const (
-	ArchFamily    = PPC64
-	BigEndian     = 1
-	CacheLineSize = 64
-	PhysPageSize  = 65536
-	PCQuantum     = 4
-	Int64Align    = 8
-	HugePageSize  = 0
-	MinFrameSize  = 32
+	ArchFamily          = PPC64
+	BigEndian           = 1
+	CacheLineSize       = 128
+	DefaultPhysPageSize = 65536
+	PCQuantum           = 4
+	Int64Align          = 8
+	HugePageSize        = 0
+	MinFrameSize        = 32
+	SpAlign             = 1
 )
 
 type Uintreg uint64
diff --git a/src/runtime/internal/sys/arch_ppc64le.go b/src/runtime/internal/sys/arch_ppc64le.go
index 4fd68f9..67c78b4 100644
--- a/src/runtime/internal/sys/arch_ppc64le.go
+++ b/src/runtime/internal/sys/arch_ppc64le.go
@@ -5,14 +5,15 @@
 package sys
 
 const (
-	ArchFamily    = PPC64
-	BigEndian     = 0
-	CacheLineSize = 64
-	PhysPageSize  = 65536
-	PCQuantum     = 4
-	Int64Align    = 8
-	HugePageSize  = 0
-	MinFrameSize  = 32
+	ArchFamily          = PPC64
+	BigEndian           = 0
+	CacheLineSize       = 128
+	DefaultPhysPageSize = 65536
+	PCQuantum           = 4
+	Int64Align          = 8
+	HugePageSize        = 0
+	MinFrameSize        = 32
+	SpAlign             = 1
 )
 
 type Uintreg uint64
diff --git a/src/runtime/internal/sys/arch_s390x.go b/src/runtime/internal/sys/arch_s390x.go
index ca1cb86..fae7ec4 100644
--- a/src/runtime/internal/sys/arch_s390x.go
+++ b/src/runtime/internal/sys/arch_s390x.go
@@ -5,14 +5,15 @@
 package sys
 
 const (
-	ArchFamily    = S390X
-	BigEndian     = 1
-	CacheLineSize = 256
-	PhysPageSize  = 4096
-	PCQuantum     = 2
-	Int64Align    = 8
-	HugePageSize  = 0
-	MinFrameSize  = 8
+	ArchFamily          = S390X
+	BigEndian           = 1
+	CacheLineSize       = 256
+	DefaultPhysPageSize = 4096
+	PCQuantum           = 2
+	Int64Align          = 8
+	HugePageSize        = 0
+	MinFrameSize        = 8
+	SpAlign             = 1
 )
 
 type Uintreg uint64
diff --git a/src/runtime/internal/sys/arch_sparc64.go b/src/runtime/internal/sys/arch_sparc64.go
new file mode 100644
index 0000000..593bbfa
--- /dev/null
+++ b/src/runtime/internal/sys/arch_sparc64.go
@@ -0,0 +1,19 @@
+// Copyright 2016 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package sys
+
+const (
+	ArchFamily          = SPARC64
+	BigEndian           = 1
+	CacheLineSize       = 64
+	DefaultPhysPageSize = 8192
+	PCQuantum           = 4
+	Int64Align          = 8
+	HugePageSize        = 0
+	MinFrameSize        = 176
+	SpAlign             = 16
+)
+
+type Uintreg uint64
diff --git a/src/runtime/internal/sys/stubs.go b/src/runtime/internal/sys/stubs.go
index 0a94502..735a38a 100644
--- a/src/runtime/internal/sys/stubs.go
+++ b/src/runtime/internal/sys/stubs.go
@@ -6,6 +6,6 @@ package sys
 
 // Declarations for runtime services implemented in C or assembly.
 
-const PtrSize = 4 << (^uintptr(0) >> 63)           // unsafe.Sizeof(uintptr(0)) but an ideal const
-const RegSize = 4 << (^Uintreg(0) >> 63)           // unsafe.Sizeof(uintreg(0)) but an ideal const
-const SpAlign = 1*(1-GoarchArm64) + 16*GoarchArm64 // SP alignment: 1 normally, 16 for ARM64
+const PtrSize = 4 << (^uintptr(0) >> 63) // unsafe.Sizeof(uintptr(0)) but an ideal const
+const RegSize = 4 << (^Uintreg(0) >> 63) // unsafe.Sizeof(uintreg(0)) but an ideal const
+const StackBias = 0x7ff * GoarchSparc64  // 0 normally, 0x7ff for SPARC64
diff --git a/src/runtime/internal/sys/zgoarch_sparc64.go b/src/runtime/internal/sys/zgoarch_sparc64.go
new file mode 100644
index 0000000..9c64b31
--- /dev/null
+++ b/src/runtime/internal/sys/zgoarch_sparc64.go
@@ -0,0 +1,26 @@
+// generated by gengoos.go using 'go generate'
+
+package sys
+
+const GOARCH = `sparc64`
+
+const Goarch386 = 0
+const GoarchAmd64 = 0
+const GoarchAmd64p32 = 0
+const GoarchArm = 0
+const GoarchArmbe = 0
+const GoarchArm64 = 0
+const GoarchArm64be = 0
+const GoarchPpc64 = 0
+const GoarchPpc64le = 0
+const GoarchMips = 0
+const GoarchMipsle = 0
+const GoarchMips64 = 0
+const GoarchMips64le = 0
+const GoarchMips64p32 = 0
+const GoarchMips64p32le = 0
+const GoarchPpc = 0
+const GoarchS390 = 0
+const GoarchS390x = 0
+const GoarchSparc = 0
+const GoarchSparc64 = 1
diff --git a/src/runtime/lfstack_sparc64.go b/src/runtime/lfstack_sparc64.go
new file mode 100644
index 0000000..c796b10
--- /dev/null
+++ b/src/runtime/lfstack_sparc64.go
@@ -0,0 +1,24 @@
+// Copyright 2016 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// +build sparc64
+
+package runtime
+
+import "unsafe"
+
+// On SPARC64, virtual addresses are 48-bit numbers sign extended to 64.
+// We shift the address left 16 to eliminate the sign extended part and make
+// room in the bottom for the count.
+// In addition to the 16 bits taken from the top, we can take 3 from the
+// bottom, because node must be pointer-aligned, giving a total of 19 bits
+// of count.
+
+func lfstackPack(node *lfnode, cnt uintptr) uint64 {
+	return uint64(uintptr(unsafe.Pointer(node)))<<16 | uint64(cnt&(1<<19-1))
+}
+
+func lfstackUnpack(val uint64) *lfnode {
+	return (*lfnode)(unsafe.Pointer(uintptr(int64(val) >> 19 << 3)))
+}
diff --git a/src/runtime/malloc.go b/src/runtime/malloc.go
index b079a07..2de35c2 100644
--- a/src/runtime/malloc.go
+++ b/src/runtime/malloc.go
@@ -172,6 +172,14 @@ const (
 
 const _MaxArena32 = 1<<32 - 1
 
+// physPageSize is the size in bytes of the OS's physical pages.
+// Mapping and unmapping operations must be done at multiples of
+// physPageSize.
+//
+// This must be set by the OS init code (typically in osinit) before
+// mallocinit.
+var physPageSize uintptr
+
 // OS-defined helpers:
 //
 // sysAlloc obtains a large chunk of zeroed memory from the
@@ -217,6 +225,20 @@ func mallocinit() {
 		throw("bad TinySizeClass")
 	}
 
+	// Check physPageSize.
+	if physPageSize == 0 {
+		// The OS init code failed to fetch the physical page size.
+		throw("failed to get system page size")
+	}
+	if physPageSize < minPhysPageSize {
+		print("system page size (", physPageSize, ") is smaller than minimum page size (", minPhysPageSize, ")\n")
+		throw("bad system page size")
+	}
+	if physPageSize&(physPageSize-1) != 0 {
+		print("system page size (", physPageSize, ") must be a power of 2\n")
+		throw("bad system page size")
+	}
+
 	var p, bitmapSize, spansSize, pSize, limit uintptr
 	var reserved bool
 
@@ -612,11 +634,11 @@ func mallocgc(size uintptr, typ *_type, needzero bool) unsafe.Pointer {
 			// reduces heap size by ~20%.
 			off := c.tinyoffset
 			// Align tiny pointer for required (conservative) alignment.
-			if size&7 == 0 {
+			if size >= 8 {
 				off = round(off, 8)
-			} else if size&3 == 0 {
+			} else if size >= 4 {
 				off = round(off, 4)
-			} else if size&1 == 0 {
+			} else if size >= 2 {
 				off = round(off, 2)
 			}
 			if off+size <= maxTinySize && c.tiny != 0 {
@@ -921,8 +943,20 @@ func persistentalloc1(size, align uintptr, sysStat *uint64) unsafe.Pointer {
 		lock(&globalAlloc.mutex)
 		persistent = &globalAlloc.persistentAlloc
 	}
-	persistent.off = round(persistent.off, align)
-	if persistent.off+size > chunk || persistent.base == nil {
+
+	if persistent.base != nil && size > 1 {
+		off := persistent.off
+		if size >= 8 {
+			off = round(off, 8)
+		} else if size >= 4 {
+			off = round(off, 4)
+		} else if size >= 2 {
+			off = round(off, 2)
+		}
+		persistent.off = off
+	}
+
+	if persistent.base == nil || persistent.off+size > chunk {
 		persistent.base = sysAlloc(chunk, &memstats.other_sys)
 		if persistent.base == nil {
 			if persistent == &globalAlloc.persistentAlloc {
diff --git a/src/runtime/mbitmap.go b/src/runtime/mbitmap.go
index ccefbcd..0d74207 100644
--- a/src/runtime/mbitmap.go
+++ b/src/runtime/mbitmap.go
@@ -151,7 +151,7 @@ func (h *mheap) mapBits(arena_used uintptr) {
 
 	n := (arena_used - mheap_.arena_start) / heapBitmapScale
 	n = round(n, bitmapChunk)
-	n = round(n, sys.PhysPageSize)
+	n = round(n, physPageSize)
 	if h.bitmap_mapped >= n {
 		return
 	}
@@ -1918,7 +1918,8 @@ func getgcmask(ep interface{}) (mask []byte) {
 			if targetpc == 0 {
 				return
 			}
-			if targetpc != f.entry {
+			// SPARC64's PC holds the address of the *current* instruction.
+			if targetpc != f.entry && sys.GoarchSparc64 == 0 {
 				targetpc--
 			}
 			pcdata := pcdatavalue(f, _PCDATA_StackMapIndex, targetpc, nil)
diff --git a/src/runtime/mem_linux.go b/src/runtime/mem_linux.go
index cd0bf26..094658d 100644
--- a/src/runtime/mem_linux.go
+++ b/src/runtime/mem_linux.go
@@ -22,17 +22,14 @@ const (
 var addrspace_vec [1]byte
 
 func addrspace_free(v unsafe.Pointer, n uintptr) bool {
-	// Step by the minimum possible physical page size. This is
-	// safe even if we have the wrong physical page size; mincore
-	// will just return EINVAL for unaligned addresses.
-	for off := uintptr(0); off < n; off += minPhysPageSize {
+	for off := uintptr(0); off < n; off += physPageSize {
 		// Use a length of 1 byte, which the kernel will round
 		// up to one physical page regardless of the true
 		// physical page size.
 		errval := mincore(unsafe.Pointer(uintptr(v)+off), 1, &addrspace_vec[0])
 		if errval == -_EINVAL {
 			// Address is not a multiple of the physical
-			// page size. That's fine.
+			// page size. Shouldn't happen, but just ignore it.
 			continue
 		}
 		// ENOMEM means unmapped, which is what we want.
@@ -138,7 +135,7 @@ func sysUnused(v unsafe.Pointer, n uintptr) {
 		}
 	}
 
-	if uintptr(v)&(sys.PhysPageSize-1) != 0 || n&(sys.PhysPageSize-1) != 0 {
+	if uintptr(v)&(physPageSize-1) != 0 || n&(physPageSize-1) != 0 {
 		// madvise will round this to any physical page
 		// *covered* by this range, so an unaligned madvise
 		// will release more memory than intended.
diff --git a/src/runtime/memclr_sparc64.s b/src/runtime/memclr_sparc64.s
new file mode 100644
index 0000000..e33607c
--- /dev/null
+++ b/src/runtime/memclr_sparc64.s
@@ -0,0 +1,5 @@
+// Copyright 2016 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+#include "textflag.h"
diff --git a/src/runtime/memmove_sparc64.go b/src/runtime/memmove_sparc64.go
new file mode 100644
index 0000000..38e9263
--- /dev/null
+++ b/src/runtime/memmove_sparc64.go
@@ -0,0 +1,38 @@
+// Copyright 2016 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package runtime
+
+import "unsafe"
+
+//go:linkname memmove_bytes runtime.memmove
+//go:nosplit
+func memmove_bytes(dst, src unsafe.Pointer, n uintptr) {
+	if n == 0 {
+		return
+	}
+	if uintptr(dst) == uintptr(src) {
+		return
+	}
+	if uintptr(src) < uintptr(dst) {
+		for i := int64(n) - 1; i >= 0; i-- {
+			b := *(*byte)(add(src, uintptr(i)))
+			*(*byte)(add(dst, uintptr(i))) = b
+		}
+		return
+	}
+	for i := int64(0); i < int64(n); i++ {
+		b := *(*byte)(add(src, uintptr(i)))
+		*(*byte)(add(dst, uintptr(i))) = b
+	}
+	return
+}
+
+//go:linkname memclr_bytes runtime.memclr
+//go:nosplit
+func memclr_bytes(ptr unsafe.Pointer, n uintptr) {
+	for i := uintptr(0); i < n; i++ {
+		*(*byte)(add(ptr, i)) = 0
+	}
+}
diff --git a/src/runtime/memmove_sparc64.s b/src/runtime/memmove_sparc64.s
new file mode 100644
index 0000000..e33607c
--- /dev/null
+++ b/src/runtime/memmove_sparc64.s
@@ -0,0 +1,5 @@
+// Copyright 2016 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+#include "textflag.h"
diff --git a/src/runtime/mgcmark.go b/src/runtime/mgcmark.go
index aa7f7a7..bae9d5c 100644
--- a/src/runtime/mgcmark.go
+++ b/src/runtime/mgcmark.go
@@ -801,7 +801,8 @@ func scanframeworker(frame *stkframe, cache *pcvalueCache, gcw *gcWork) {
 	if _DebugGC > 1 {
 		print("scanframe ", funcname(f), "\n")
 	}
-	if targetpc != f.entry {
+	// SPARC64's PC holds the address of the *current* instruction.
+	if targetpc != f.entry && sys.GoarchSparc64 == 0 {
 		targetpc--
 	}
 	pcdata := pcdatavalue(f, _PCDATA_StackMapIndex, targetpc, cache)
diff --git a/src/runtime/mheap.go b/src/runtime/mheap.go
index 8db2fcc..dfb484c 100644
--- a/src/runtime/mheap.go
+++ b/src/runtime/mheap.go
@@ -401,7 +401,7 @@ func (h *mheap) mapSpans(arena_used uintptr) {
 	n := arena_used
 	n -= h.arena_start
 	n = n / _PageSize * sys.PtrSize
-	n = round(n, sys.PhysPageSize)
+	n = round(n, physPageSize)
 	if h.spans_mapped >= n {
 		return
 	}
@@ -909,14 +909,14 @@ func scavengelist(list *mSpanList, now, limit uint64) uintptr {
 		if (now-uint64(s.unusedsince)) > limit && s.npreleased != s.npages {
 			start := s.base()
 			end := start + s.npages<<_PageShift
-			if sys.PhysPageSize > _PageSize {
+			if physPageSize > _PageSize {
 				// We can only release pages in
-				// PhysPageSize blocks, so round start
+				// physPageSize blocks, so round start
 				// and end in. (Otherwise, madvise
 				// will round them *out* and release
 				// more memory than we want.)
-				start = (start + sys.PhysPageSize - 1) &^ (sys.PhysPageSize - 1)
-				end &^= sys.PhysPageSize - 1
+				start = (start + physPageSize - 1) &^ (physPageSize - 1)
+				end &^= physPageSize - 1
 				if end <= start {
 					// start and end don't span a
 					// whole physical page.
@@ -926,7 +926,7 @@ func scavengelist(list *mSpanList, now, limit uint64) uintptr {
 			len := end - start
 
 			released := len - (s.npreleased << _PageShift)
-			if sys.PhysPageSize > _PageSize && released == 0 {
+			if physPageSize > _PageSize && released == 0 {
 				continue
 			}
 			memstats.heap_released += uint64(released)
diff --git a/src/runtime/mkduff.go b/src/runtime/mkduff.go
index 0e7cc66..c7f9829 100644
--- a/src/runtime/mkduff.go
+++ b/src/runtime/mkduff.go
@@ -38,6 +38,7 @@ func main() {
 	gen("arm64", notags, zeroARM64, copyARM64)
 	gen("ppc64x", tagsPPC64x, zeroPPC64x, copyPPC64x)
 	gen("mips64x", tagsMIPS64x, zeroMIPS64x, copyMIPS64x)
+	gen("sparc64", notags, zeroSPARC64, copySPARC64)
 }
 
 func gen(arch string, tags, zero, copy func(io.Writer)) {
@@ -185,6 +186,22 @@ func copyPPC64x(w io.Writer) {
 	fmt.Fprintln(w, "// TODO: Implement runtime·duffcopy.")
 }
 
+func zeroSPARC64(w io.Writer) {
+	// ZR (aka G0): always zero
+	// RT1 (aka G1): ptr to memory to be zeroed - 8
+	// On return, RT1 points to the last zeroed dword.
+	fmt.Fprintln(w, "TEXT runtime·duffzero(SB), NOSPLIT|NOFRAME, $0-0")
+	for i := 0; i < 128; i++ {
+		fmt.Fprintln(w, "\tMOVD\tZR, 8(RT1)")
+		fmt.Fprintln(w, "\tADD\t$8, RT1")
+	}
+	fmt.Fprintln(w, "\tRET")
+}
+
+func copySPARC64(w io.Writer) {
+	fmt.Fprintln(w, "// TODO: Implement runtime·duffcopy.")
+}
+
 func tagsMIPS64x(w io.Writer) {
 	fmt.Fprintln(w)
 	fmt.Fprintln(w, "// +build mips64 mips64le")
diff --git a/src/runtime/mstkbar.go b/src/runtime/mstkbar.go
index 1bf9d57..d8a1bed 100644
--- a/src/runtime/mstkbar.go
+++ b/src/runtime/mstkbar.go
@@ -192,7 +192,7 @@ func gcInstallStackBarrier(gp *g, frame *stkframe) bool {
 	// Save the return PC and overwrite it with stackBarrier.
 	var lrUintptr uintptr
 	if usesLR {
-		lrUintptr = frame.sp
+		lrUintptr = frame.sp + returnAddrOffset
 	} else {
 		lrUintptr = frame.fp - sys.RegSize
 	}
diff --git a/src/runtime/noasm.go b/src/runtime/noasm.go
index 0a8f9e6..fdb7509 100644
--- a/src/runtime/noasm.go
+++ b/src/runtime/noasm.go
@@ -4,7 +4,7 @@
 
 // Routines that are implemented in assembly in asm_{amd64,386,arm,arm64,ppc64x,s390x}.s
 
-// +build mips64 mips64le
+// +build mips64 mips64le sparc64
 
 package runtime
 
diff --git a/src/runtime/os3_solaris.go b/src/runtime/os3_solaris.go
index 9368e0d..9e01b92 100644
--- a/src/runtime/os3_solaris.go
+++ b/src/runtime/os3_solaris.go
@@ -127,8 +127,17 @@ func getncpu() int32 {
 	return n
 }
 
+func getPageSize() uintptr {
+	n := int32(sysconf(__SC_PAGESIZE))
+	if n <= 0 {
+		return 0
+	}
+	return uintptr(n)
+}
+
 func osinit() {
 	ncpu = getncpu()
+	physPageSize = getPageSize()
 }
 
 func tstart_sysvicall()
@@ -288,7 +297,6 @@ func setsig(i int32, fn uintptr, restart bool) {
 	var sa sigactiont
 
 	sa.sa_flags = _SA_SIGINFO | _SA_ONSTACK
-	sa.sa_flags = _SA_SIGINFO | _SA_ONSTACK
 	if restart {
 		sa.sa_flags |= _SA_RESTART
 	}
@@ -591,3 +599,11 @@ func osyield() {
 	}
 	osyield1()
 }
+
+//go:nosplit
+func threaddump() {
+	g := getg()
+	print("threaddump: g=", g, ", g.m=", g.m, ", g.m.g0=", g.m.g0, ", g.m.gsignal=", g.m.gsignal, "\n")
+	print("	g.stack.hi=", hex(g.stack.hi), ", g.stack.lo", hex(g.stack.lo), ", g.stackguard0=", hex(g.stackguard0), ", g.stackguard1=", hex(g.stackguard1), "\n")
+	print("	callersp=", hex(getcallersp(unsafe.Pointer(&g))), "\n")
+}
diff --git a/src/runtime/os_darwin.go b/src/runtime/os_darwin.go
index a0e3d8e..d2f865c 100644
--- a/src/runtime/os_darwin.go
+++ b/src/runtime/os_darwin.go
@@ -50,11 +50,19 @@ func osinit() {
 	// can look at the environment first.
 
 	ncpu = getncpu()
+
+	physPageSize = getPageSize()
 }
 
+const (
+	_CTL_HW      = 6
+	_HW_NCPU     = 3
+	_HW_PAGESIZE = 7
+)
+
 func getncpu() int32 {
 	// Use sysctl to fetch hw.ncpu.
-	mib := [2]uint32{6, 3}
+	mib := [2]uint32{_CTL_HW, _HW_NCPU}
 	out := uint32(0)
 	nout := unsafe.Sizeof(out)
 	ret := sysctl(&mib[0], 2, (*byte)(unsafe.Pointer(&out)), &nout, nil, 0)
@@ -64,6 +72,18 @@ func getncpu() int32 {
 	return 1
 }
 
+func getPageSize() uintptr {
+	// Use sysctl to fetch hw.pagesize.
+	mib := [2]uint32{_CTL_HW, _HW_PAGESIZE}
+	out := uint32(0)
+	nout := unsafe.Sizeof(out)
+	ret := sysctl(&mib[0], 2, (*byte)(unsafe.Pointer(&out)), &nout, nil, 0)
+	if ret >= 0 && int32(out) > 0 {
+		return uintptr(out)
+	}
+	return 0
+}
+
 var urandom_dev = []byte("/dev/urandom\x00")
 
 //go:nosplit
diff --git a/src/runtime/os_dragonfly.go b/src/runtime/os_dragonfly.go
index 85d4aad..edf7581 100644
--- a/src/runtime/os_dragonfly.go
+++ b/src/runtime/os_dragonfly.go
@@ -54,8 +54,9 @@ const stackSystem = 0
 
 // From DragonFly's <sys/sysctl.h>
 const (
-	_CTL_HW  = 6
-	_HW_NCPU = 3
+	_CTL_HW      = 6
+	_HW_NCPU     = 3
+	_HW_PAGESIZE = 7
 )
 
 var sigset_all = sigset{[4]uint32{^uint32(0), ^uint32(0), ^uint32(0), ^uint32(0)}}
@@ -71,6 +72,17 @@ func getncpu() int32 {
 	return 1
 }
 
+func getPageSize() uintptr {
+	mib := [2]uint32{_CTL_HW, _HW_PAGESIZE}
+	out := uint32(0)
+	nout := unsafe.Sizeof(out)
+	ret := sysctl(&mib[0], 2, (*byte)(unsafe.Pointer(&out)), &nout, nil, 0)
+	if ret >= 0 {
+		return uintptr(out)
+	}
+	return 0
+}
+
 //go:nosplit
 func futexsleep(addr *uint32, val uint32, ns int64) {
 	systemstack(func() {
@@ -141,6 +153,7 @@ func newosproc(mp *m, stk unsafe.Pointer) {
 
 func osinit() {
 	ncpu = getncpu()
+	physPageSize = getPageSize()
 }
 
 var urandom_dev = []byte("/dev/urandom\x00")
diff --git a/src/runtime/os_freebsd.go b/src/runtime/os_freebsd.go
index 0e09c60..7134998 100644
--- a/src/runtime/os_freebsd.go
+++ b/src/runtime/os_freebsd.go
@@ -41,8 +41,9 @@ func osyield()
 
 // From FreeBSD's <sys/sysctl.h>
 const (
-	_CTL_HW  = 6
-	_HW_NCPU = 3
+	_CTL_HW      = 6
+	_HW_NCPU     = 3
+	_HW_PAGESIZE = 7
 )
 
 var sigset_all = sigset{[4]uint32{^uint32(0), ^uint32(0), ^uint32(0), ^uint32(0)}}
@@ -58,6 +59,17 @@ func getncpu() int32 {
 	return 1
 }
 
+func getPageSize() uintptr {
+	mib := [2]uint32{_CTL_HW, _HW_PAGESIZE}
+	out := uint32(0)
+	nout := unsafe.Sizeof(out)
+	ret := sysctl(&mib[0], 2, (*byte)(unsafe.Pointer(&out)), &nout, nil, 0)
+	if ret >= 0 {
+		return uintptr(out)
+	}
+	return 0
+}
+
 // FreeBSD's umtx_op syscall is effectively the same as Linux's futex, and
 // thus the code is largely similar. See Linux implementation
 // and lock_futex.go for comments.
@@ -128,6 +140,7 @@ func newosproc(mp *m, stk unsafe.Pointer) {
 
 func osinit() {
 	ncpu = getncpu()
+	physPageSize = getPageSize()
 }
 
 var urandom_dev = []byte("/dev/urandom\x00")
diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 542f214..aea1703 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -207,17 +207,7 @@ func sysargs(argc int32, argv **byte) {
 			startupRandomData = (*[16]byte)(unsafe.Pointer(val))[:]
 
 		case _AT_PAGESZ:
-			// Check that the true physical page size is
-			// compatible with the runtime's assumed
-			// physical page size.
-			if sys.PhysPageSize < val {
-				print("runtime: kernel page size (", val, ") is larger than runtime page size (", sys.PhysPageSize, ")\n")
-				exit(1)
-			}
-			if sys.PhysPageSize%val != 0 {
-				print("runtime: runtime page size (", sys.PhysPageSize, ") is not a multiple of kernel page size (", val, ")\n")
-				exit(1)
-			}
+			physPageSize = val
 		}
 
 		archauxv(tag, val)
diff --git a/src/runtime/os_nacl.go b/src/runtime/os_nacl.go
index 1dacc1a..c968b1a 100644
--- a/src/runtime/os_nacl.go
+++ b/src/runtime/os_nacl.go
@@ -116,6 +116,7 @@ func osinit() {
 	ncpu = 1
 	getg().m.procid = 2
 	//nacl_exception_handler(funcPC(sigtramp), nil);
+	physPageSize = 65536
 }
 
 func signame(sig uint32) string {
diff --git a/src/runtime/os_netbsd.go b/src/runtime/os_netbsd.go
index 4c44b2b..3e94c3b 100644
--- a/src/runtime/os_netbsd.go
+++ b/src/runtime/os_netbsd.go
@@ -79,8 +79,9 @@ var sigset_all = sigset{[4]uint32{^uint32(0), ^uint32(0), ^uint32(0), ^uint32(0)
 
 // From NetBSD's <sys/sysctl.h>
 const (
-	_CTL_HW  = 6
-	_HW_NCPU = 3
+	_CTL_HW      = 6
+	_HW_NCPU     = 3
+	_HW_PAGESIZE = 7
 )
 
 func getncpu() int32 {
@@ -94,6 +95,17 @@ func getncpu() int32 {
 	return 1
 }
 
+func getPageSize() uintptr {
+	mib := [2]uint32{_CTL_HW, _HW_PAGESIZE}
+	out := uint32(0)
+	nout := unsafe.Sizeof(out)
+	ret := sysctl(&mib[0], 2, (*byte)(unsafe.Pointer(&out)), &nout, nil, 0)
+	if ret >= 0 {
+		return uintptr(out)
+	}
+	return 0
+}
+
 //go:nosplit
 func semacreate(mp *m) {
 }
@@ -186,6 +198,7 @@ func netbsdMstart() {
 
 func osinit() {
 	ncpu = getncpu()
+	physPageSize = getPageSize()
 }
 
 var urandom_dev = []byte("/dev/urandom\x00")
diff --git a/src/runtime/os_openbsd.go b/src/runtime/os_openbsd.go
index 9a5c53e..c2b3b97 100644
--- a/src/runtime/os_openbsd.go
+++ b/src/runtime/os_openbsd.go
@@ -64,8 +64,9 @@ const (
 
 // From OpenBSD's <sys/sysctl.h>
 const (
-	_CTL_HW  = 6
-	_HW_NCPU = 3
+	_CTL_HW      = 6
+	_HW_NCPU     = 3
+	_HW_PAGESIZE = 7
 )
 
 func getncpu() int32 {
@@ -81,6 +82,17 @@ func getncpu() int32 {
 	return 1
 }
 
+func getPageSize() uintptr {
+	mib := [2]uint32{_CTL_HW, _HW_PAGESIZE}
+	out := uint32(0)
+	nout := unsafe.Sizeof(out)
+	ret := sysctl(&mib[0], 2, (*byte)(unsafe.Pointer(&out)), &nout, nil, 0)
+	if ret >= 0 {
+		return uintptr(out)
+	}
+	return 0
+}
+
 //go:nosplit
 func semacreate(mp *m) {
 }
@@ -163,6 +175,7 @@ func newosproc(mp *m, stk unsafe.Pointer) {
 
 func osinit() {
 	ncpu = getncpu()
+	physPageSize = getPageSize()
 }
 
 var urandom_dev = []byte("/dev/urandom\x00")
diff --git a/src/runtime/os_plan9.go b/src/runtime/os_plan9.go
index 2f3a0d1..333f222 100644
--- a/src/runtime/os_plan9.go
+++ b/src/runtime/os_plan9.go
@@ -217,6 +217,55 @@ func getproccount() int32 {
 	return ncpu
 }
 
+var devswap = []byte("/dev/swap\x00")
+var pagesize = []byte(" pagesize\n")
+
+func getPageSize() uintptr {
+	var buf [2048]byte
+	var pos int
+	fd := open(&devswap[0], _OREAD, 0)
+	if fd < 0 {
+		// There's not much we can do if /dev/swap doesn't
+		// exist. However, nothing in the memory manager uses
+		// this on Plan 9, so it also doesn't really matter.
+		return minPhysPageSize
+	}
+	for pos < len(buf) {
+		n := read(fd, unsafe.Pointer(&buf[pos]), int32(len(buf)-pos))
+		if n <= 0 {
+			break
+		}
+		pos += int(n)
+	}
+	closefd(fd)
+	text := buf[:pos]
+	// Find "<n> pagesize" line.
+	bol := 0
+	for i, c := range text {
+		if c == '\n' {
+			bol = i + 1
+		}
+		if bytesHasPrefix(text[i:], pagesize) {
+			// Parse number at the beginning of this line.
+			return uintptr(_atoi(text[bol:]))
+		}
+	}
+	// Again, the page size doesn't really matter, so use a fallback.
+	return minPhysPageSize
+}
+
+func bytesHasPrefix(s, prefix []byte) bool {
+	if len(s) < len(prefix) {
+		return false
+	}
+	for i, p := range prefix {
+		if s[i] != p {
+			return false
+		}
+	}
+	return true
+}
+
 var pid = []byte("#c/pid\x00")
 
 func getpid() uint64 {
@@ -236,6 +285,7 @@ func getpid() uint64 {
 func osinit() {
 	initBloc()
 	ncpu = getproccount()
+	physPageSize = getPageSize()
 	getg().m.procid = getpid()
 	notify(unsafe.Pointer(funcPC(sigtramp)))
 }
diff --git a/src/runtime/os_windows.go b/src/runtime/os_windows.go
index 9147091..558a670 100644
--- a/src/runtime/os_windows.go
+++ b/src/runtime/os_windows.go
@@ -205,6 +205,12 @@ func getproccount() int32 {
 	return int32(info.dwnumberofprocessors)
 }
 
+func getPageSize() uintptr {
+	var info systeminfo
+	stdcall1(_GetSystemInfo, uintptr(unsafe.Pointer(&info)))
+	return uintptr(info.dwpagesize)
+}
+
 const (
 	currentProcess = ^uintptr(0) // -1 = current process
 	currentThread  = ^uintptr(1) // -2 = current thread
@@ -256,6 +262,8 @@ func osinit() {
 
 	ncpu = getproccount()
 
+	physPageSize = getPageSize()
+
 	// Windows dynamic priority boosting assumes that a process has different types
 	// of dedicated threads -- GUI, IO, computational, etc. Go processes use
 	// equivalent threads that all do a mix of GUI, IO, computations, etc.
diff --git a/src/runtime/panic.go b/src/runtime/panic.go
index 60b277d..a07e3f6 100644
--- a/src/runtime/panic.go
+++ b/src/runtime/panic.go
@@ -6,6 +6,7 @@ package runtime
 
 import (
 	"runtime/internal/atomic"
+	"runtime/internal/sys"
 	"unsafe"
 )
 
@@ -95,6 +96,12 @@ func deferproc(siz int32, fn *funcval) { // arguments of fn follow fn
 		}
 		d.fn = fn
 		d.pc = callerpc
+		if sys.GoarchSparc64 == 1 {
+			// on SPARC64 the link register contains the address of the
+			// 4-byte CALL instruction, which is always follwed by a
+			// 4-byteNOP.
+			d.pc += 8
+		}
 		d.sp = sp
 		memmove(add(unsafe.Pointer(d), unsafe.Sizeof(*d)), unsafe.Pointer(argp), uintptr(siz))
 	})
@@ -590,6 +597,13 @@ func recovery(gp *g) {
 	gcUnwindBarriers(gp, sp)
 	gp.sched.sp = sp
 	gp.sched.pc = pc
+	if sys.GoarchSparc64 == 1 {
+		// Function prolog saves the FP here; sp already has
+		// bias applied, so we use it directly. The recovered
+		// fp doesn't have the bias applied. Since gogo expects
+		// biased input, we add the stack bias to the loaded fp.
+		gp.sched.bp = *((*uintptr)(unsafe.Pointer(sp+112))) + sys.StackBias
+	}
 	gp.sched.lr = 0
 	gp.sched.ret = 1
 	gogo(&gp.sched)
diff --git a/src/runtime/proc.go b/src/runtime/proc.go
index e693f7e..1cbac5d 100644
--- a/src/runtime/proc.go
+++ b/src/runtime/proc.go
@@ -2758,7 +2758,7 @@ func newproc1(fn *funcval, argp *uint8, narg int32, nret int32, callerpc uintptr
 	spArg := sp
 	if usesLR {
 		// caller's LR
-		*(*unsafe.Pointer)(unsafe.Pointer(sp)) = nil
+		*(*unsafe.Pointer)(unsafe.Pointer(sp + returnAddrOffset)) = nil
 		prepGoExitFrame(sp)
 		spArg += sys.MinFrameSize
 	}
diff --git a/src/runtime/rt0_solaris_sparc64.s b/src/runtime/rt0_solaris_sparc64.s
new file mode 100644
index 0000000..4b04afa
--- /dev/null
+++ b/src/runtime/rt0_solaris_sparc64.s
@@ -0,0 +1,18 @@
+// Copyright 2014 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+#include "textflag.h"
+#include "asm_sparc64.h"
+
+TEXT _rt0_sparc64_solaris(SB),NOSPLIT|NOFRAME,$0
+	MOVD	WINDOW_SIZE+0(BSP), O0 // argc
+	MOVD	$(WINDOW_SIZE+8)(BSP), O1 // argv
+	MOVD	$main(SB), O3
+	JMPL	O3, ZR
+
+TEXT main(SB),NOSPLIT|REGWIN,$0
+	MOVW	I0, O0
+	MOVD	I1, O1
+	CALL	runtime·rt0_go(SB)
+	RET
diff --git a/src/runtime/runtime-gdb_test.go b/src/runtime/runtime-gdb_test.go
index aabe52d..24e3449 100644
--- a/src/runtime/runtime-gdb_test.go
+++ b/src/runtime/runtime-gdb_test.go
@@ -23,6 +23,9 @@ func checkGdbEnvironment(t *testing.T) {
 	if runtime.GOOS == "darwin" {
 		t.Skip("gdb does not work on darwin")
 	}
+	if runtime.GOOS == "solaris" {
+		t.Skip("gdb Python integration often broken on solaris")
+	}
 	if final := os.Getenv("GOROOT_FINAL"); final != "" && runtime.GOROOT() != final {
 		t.Skip("gdb test can fail with GOROOT_FINAL pending")
 	}
diff --git a/src/runtime/runtime-lldb_test.go b/src/runtime/runtime-lldb_test.go
index 4c379b9..3c3248d 100644
--- a/src/runtime/runtime-lldb_test.go
+++ b/src/runtime/runtime-lldb_test.go
@@ -22,6 +22,9 @@ import (
 var lldbPath string
 
 func checkLldbPython(t *testing.T) {
+	if runtime.GOARCH == "sparc64" {
+		t.Skipf("lldb not supported on sparc64")
+	}
 	cmd := exec.Command("lldb", "-P")
 	out, err := cmd.CombinedOutput()
 	if err != nil {
@@ -185,6 +188,9 @@ func TestLldbPython(t *testing.T) {
 
 // Check that aranges are valid even when lldb isn't installed.
 func TestDwarfAranges(t *testing.T) {
+	if runtime.GOARCH == "sparc64" {
+		t.Skipf("dwarf not currently supported on sparc64")
+	}
 	testenv.MustHaveGoBuild(t)
 	dir, err := ioutil.TempDir("", "go-build")
 	if err != nil {
diff --git a/src/runtime/runtime.go b/src/runtime/runtime.go
index d9c26cc..d8fe2f4 100644
--- a/src/runtime/runtime.go
+++ b/src/runtime/runtime.go
@@ -52,5 +52,8 @@ var argslice []string
 //go:linkname syscall_runtime_envs syscall.runtime_envs
 func syscall_runtime_envs() []string { return append([]string{}, envs...) }
 
+//go:linkname syscall_Getpagesize syscall.Getpagesize
+func syscall_Getpagesize() int { return int(physPageSize) }
+
 //go:linkname os_runtime_args os.runtime_args
 func os_runtime_args() []string { return append([]string{}, argslice...) }
diff --git a/src/runtime/runtime_mmap_test.go b/src/runtime/runtime_mmap_test.go
index cf240c1..2eca6b9 100644
--- a/src/runtime/runtime_mmap_test.go
+++ b/src/runtime/runtime_mmap_test.go
@@ -8,15 +8,15 @@ package runtime_test
 
 import (
 	"runtime"
-	"runtime/internal/sys"
 	"testing"
+	"unsafe"
 )
 
 // Test that the error value returned by mmap is positive, as that is
 // what the code in mem_bsd.go, mem_darwin.go, and mem_linux.go expects.
 // See the uses of ENOMEM in sysMap in those files.
 func TestMmapErrorSign(t *testing.T) {
-	p := runtime.Mmap(nil, ^uintptr(0)&^(sys.PhysPageSize-1), 0, runtime.MAP_ANON|runtime.MAP_PRIVATE, -1, 0)
+	p := runtime.Mmap(nil, ^uintptr(0)&^(runtime.GetPhysPageSize()-1), 0, runtime.MAP_ANON|runtime.MAP_PRIVATE, -1, 0)
 
 	// The runtime.mmap function is nosplit, but t.Errorf is not.
 	// Reset the pointer so that we don't get an "invalid stack
@@ -28,3 +28,27 @@ func TestMmapErrorSign(t *testing.T) {
 		t.Errorf("mmap = %v, want %v", v, runtime.ENOMEM)
 	}
 }
+
+func TestPhysPageSize(t *testing.T) {
+	// Mmap fails if the address is not page aligned, so we can
+	// use this to test if the page size is the true page size.
+	ps := runtime.GetPhysPageSize()
+
+	// Get a region of memory to play with. This should be page-aligned.
+	b := uintptr(runtime.Mmap(nil, 2*ps, 0, runtime.MAP_ANON|runtime.MAP_PRIVATE, -1, 0))
+	if b < 4096 {
+		t.Fatalf("Mmap: %v", b)
+	}
+
+	// Mmap should fail at a half page into the buffer.
+	err := uintptr(runtime.Mmap(unsafe.Pointer(uintptr(b)+ps/2), ps, 0, runtime.MAP_ANON|runtime.MAP_PRIVATE|runtime.MAP_FIXED, -1, 0))
+	if err >= 4096 {
+		t.Errorf("Mmap should have failed with half-page alignment %d, but succeeded: %v", ps/2, err)
+	}
+
+	// Mmap should succeed at a full page into the buffer.
+	err = uintptr(runtime.Mmap(unsafe.Pointer(uintptr(b)+ps), ps, 0, runtime.MAP_ANON|runtime.MAP_PRIVATE|runtime.MAP_FIXED, -1, 0))
+	if err < 4096 {
+		t.Errorf("Mmap at full-page alignment %d failed: %v", ps, err)
+	}
+}
diff --git a/src/runtime/signal_solaris_sparc64.go b/src/runtime/signal_solaris_sparc64.go
new file mode 100644
index 0000000..d79620b
--- /dev/null
+++ b/src/runtime/signal_solaris_sparc64.go
@@ -0,0 +1,191 @@
+// Copyright 2016 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package runtime
+
+import (
+	"runtime/internal/sys"
+	"unsafe"
+)
+
+type sigctxt struct {
+	info *siginfo
+	ctxt unsafe.Pointer
+}
+
+func (c *sigctxt) regs() *mcontext {
+	return (*mcontext)(unsafe.Pointer(&(*ucontext)(c.ctxt).uc_mcontext))
+}
+
+func (c *sigctxt) r1() uint64  { return uint64(c.regs().gregs[_REG_G1]) }
+func (c *sigctxt) r2() uint64  { return uint64(c.regs().gregs[_REG_G2]) }
+func (c *sigctxt) r3() uint64  { return uint64(c.regs().gregs[_REG_G3]) }
+func (c *sigctxt) r4() uint64  { return uint64(c.regs().gregs[_REG_G4]) }
+func (c *sigctxt) r5() uint64  { return uint64(c.regs().gregs[_REG_G5]) }
+func (c *sigctxt) r6() uint64  { return uint64(c.regs().gregs[_REG_G6]) }
+func (c *sigctxt) r7() uint64  { return uint64(c.regs().gregs[_REG_G7]) }
+func (c *sigctxt) r8() uint64  { return uint64(c.regs().gregs[_REG_O0]) }
+func (c *sigctxt) r9() uint64  { return uint64(c.regs().gregs[_REG_O1]) }
+func (c *sigctxt) r10() uint64 { return uint64(c.regs().gregs[_REG_O2]) }
+func (c *sigctxt) r11() uint64 { return uint64(c.regs().gregs[_REG_O3]) }
+func (c *sigctxt) r12() uint64 { return uint64(c.regs().gregs[_REG_O4]) }
+func (c *sigctxt) r13() uint64 { return uint64(c.regs().gregs[_REG_O5]) }
+
+func (c *sigctxt) sp() uint64 { return uint64(c.regs().gregs[_REG_O6]) + sys.StackBias }
+func (c *sigctxt) lr() uint64 { return uint64(c.regs().gregs[_REG_O7]) }
+
+func (c *sigctxt) r16() uint64 {
+	if c.regs().gwins != nil {
+		cwp := int(c.regs().gregs[_REG_CCR] & 0x1f)
+		return uint64(c.regs().gwins.wbuf[cwp].local[0])
+	}
+	return *(*uint64)(unsafe.Pointer((uintptr)(c.regs().gregs[_REG_O6] + sys.StackBias + 0*8)))
+}
+
+func (c *sigctxt) r17() uint64 {
+	if c.regs().gwins != nil {
+		cwp := int(c.regs().gregs[_REG_CCR] & 0x1f)
+		return uint64(c.regs().gwins.wbuf[cwp].local[1])
+	}
+	return *(*uint64)(unsafe.Pointer((uintptr)(c.regs().gregs[_REG_O6] + sys.StackBias + 1*8)))
+}
+
+func (c *sigctxt) r18() uint64 {
+	if c.regs().gwins != nil {
+		cwp := int(c.regs().gregs[_REG_CCR] & 0x1f)
+		return uint64(c.regs().gwins.wbuf[cwp].local[2])
+	}
+	return *(*uint64)(unsafe.Pointer((uintptr)(c.regs().gregs[_REG_O6] + sys.StackBias + 2*8)))
+}
+
+func (c *sigctxt) r19() uint64 {
+	if c.regs().gwins != nil {
+		cwp := int(c.regs().gregs[_REG_CCR] & 0x1f)
+		return uint64(c.regs().gwins.wbuf[cwp].local[3])
+	}
+	return *(*uint64)(unsafe.Pointer((uintptr)(c.regs().gregs[_REG_O6] + sys.StackBias + 3*8)))
+}
+
+func (c *sigctxt) r20() uint64 {
+	if c.regs().gwins != nil {
+		cwp := int(c.regs().gregs[_REG_CCR] & 0x1f)
+		return uint64(c.regs().gwins.wbuf[cwp].local[4])
+	}
+	return *(*uint64)(unsafe.Pointer((uintptr)(c.regs().gregs[_REG_O6] + sys.StackBias + 4*8)))
+}
+
+func (c *sigctxt) r21() uint64 {
+	if c.regs().gwins != nil {
+		cwp := int(c.regs().gregs[_REG_CCR] & 0x1f)
+		return uint64(c.regs().gwins.wbuf[cwp].local[5])
+	}
+	return *(*uint64)(unsafe.Pointer((uintptr)(c.regs().gregs[_REG_O6] + sys.StackBias + 5*8)))
+}
+
+func (c *sigctxt) r22() uint64 {
+	if c.regs().gwins != nil {
+		cwp := int(c.regs().gregs[_REG_CCR] & 0x1f)
+		return uint64(c.regs().gwins.wbuf[cwp].local[6])
+	}
+	return *(*uint64)(unsafe.Pointer((uintptr)(c.regs().gregs[_REG_O6] + sys.StackBias + 6*8)))
+}
+
+func (c *sigctxt) r23() uint64 {
+	if c.regs().gwins != nil {
+		cwp := int(c.regs().gregs[_REG_CCR] & 0x1f)
+		return uint64(c.regs().gwins.wbuf[cwp].local[7])
+	}
+	return *(*uint64)(unsafe.Pointer((uintptr)(c.regs().gregs[_REG_O6] + sys.StackBias + 7*8)))
+}
+
+func (c *sigctxt) r24() uint64 {
+	if c.regs().gwins != nil {
+		cwp := int(c.regs().gregs[_REG_CCR] & 0x1f)
+		return uint64(c.regs().gwins.wbuf[cwp].in[0])
+	}
+	return *(*uint64)(unsafe.Pointer((uintptr)(c.regs().gregs[_REG_O6] + sys.StackBias + 8*8)))
+}
+
+func (c *sigctxt) r25() uint64 {
+	if c.regs().gwins != nil {
+		cwp := int(c.regs().gregs[_REG_CCR] & 0x1f)
+		return uint64(c.regs().gwins.wbuf[cwp].in[1])
+	}
+	return *(*uint64)(unsafe.Pointer((uintptr)(c.regs().gregs[_REG_O6] + sys.StackBias + 9*8)))
+}
+
+func (c *sigctxt) r26() uint64 {
+	if c.regs().gwins != nil {
+		cwp := int(c.regs().gregs[_REG_CCR] & 0x1f)
+		return uint64(c.regs().gwins.wbuf[cwp].in[2])
+	}
+	return *(*uint64)(unsafe.Pointer((uintptr)(c.regs().gregs[_REG_O6] + sys.StackBias + 10*8)))
+}
+
+func (c *sigctxt) r27() uint64 {
+	if c.regs().gwins != nil {
+		cwp := int(c.regs().gregs[_REG_CCR] & 0x1f)
+		return uint64(c.regs().gwins.wbuf[cwp].in[3])
+	}
+	return *(*uint64)(unsafe.Pointer((uintptr)(c.regs().gregs[_REG_O6] + sys.StackBias + 11*8)))
+}
+
+func (c *sigctxt) r28() uint64 {
+	if c.regs().gwins != nil {
+		cwp := int(c.regs().gregs[_REG_CCR] & 0x1f)
+		return uint64(c.regs().gwins.wbuf[cwp].in[4])
+	}
+	return *(*uint64)(unsafe.Pointer((uintptr)(c.regs().gregs[_REG_O6] + sys.StackBias + 12*8)))
+}
+
+func (c *sigctxt) r29() uint64 {
+	if c.regs().gwins != nil {
+		cwp := int(c.regs().gregs[_REG_CCR] & 0x1f)
+		return uint64(c.regs().gwins.wbuf[cwp].in[5])
+	}
+	return *(*uint64)(unsafe.Pointer((uintptr)(c.regs().gregs[_REG_O6] + sys.StackBias + 13*8)))
+}
+
+func (c *sigctxt) fp() uint64 {
+	if c.regs().gwins != nil {
+		cwp := int(c.regs().gregs[_REG_CCR] & 0x1f)
+		return uint64(c.regs().gwins.wbuf[cwp].in[6] + sys.StackBias)
+	}
+	return *(*uint64)(unsafe.Pointer((uintptr)(c.regs().gregs[_REG_O6] + sys.StackBias + 14*8))) + sys.StackBias
+}
+
+func (c *sigctxt) r31() uint64 {
+	if c.regs().gwins != nil {
+		cwp := int(c.regs().gregs[_REG_CCR] & 0x1f)
+		return uint64(c.regs().gwins.wbuf[cwp].in[7])
+	}
+	return *(*uint64)(unsafe.Pointer((uintptr)(c.regs().gregs[_REG_O6] + sys.StackBias + 15*8)))
+}
+
+func (c *sigctxt) pc() uint64     { return uint64(c.regs().gregs[_REG_PC]) }
+func (c *sigctxt) npc() uint64     { return uint64(c.regs().gregs[_REG_nPC]) }
+func (c *sigctxt) tstate() uint64 { return uint64(c.regs().gregs[_REG_CCR]) }
+
+func (c *sigctxt) sigcode() uint64 { return uint64(c.info.si_code) }
+func (c *sigctxt) fault() uint64   { return *(*uint64)(unsafe.Pointer(&c.info.__data[0])) }
+
+func (c *sigctxt) set_r3(x uint64) { c.regs().gregs[_REG_G3] = int64(x) }
+
+func (c *sigctxt) set_pc(x uint64) { c.regs().gregs[_REG_PC] = int64(x) }
+func (c *sigctxt) set_npc(x uint64) { c.regs().gregs[_REG_nPC] = int64(x) }
+func (c *sigctxt) set_sp(x uint64) { c.regs().gregs[_REG_O6] = int64(x - sys.StackBias) }
+func (c *sigctxt) set_lr(x uint64) { c.regs().gregs[_REG_O7] = int64(x) }
+
+func (c *sigctxt) set_fp(x uint64) {
+	if c.regs().gwins != nil {
+		cwp := int(c.regs().gregs[_REG_CCR] & 0x1f)
+		c.regs().gwins.wbuf[cwp].in[6] = int64(x-sys.StackBias)
+	}
+	*(*uint64)(unsafe.Pointer((uintptr)(c.regs().gregs[_REG_O6] + sys.StackBias + 14*8))) = x - sys.StackBias
+}
+
+func (c *sigctxt) set_sigcode(x uint64) { c.info.si_code = int32(x) }
+func (c *sigctxt) set_sigaddr(x uint64) {
+	*(*uintptr)(unsafe.Pointer(&c.info.__data[0])) = uintptr(x)
+}
diff --git a/src/runtime/signal_sparc64.go b/src/runtime/signal_sparc64.go
new file mode 100644
index 0000000..d9af441
--- /dev/null
+++ b/src/runtime/signal_sparc64.go
@@ -0,0 +1,187 @@
+// Copyright 2016 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package runtime
+
+import (
+	"runtime/internal/sys"
+	"unsafe"
+)
+
+func dumpregs(c *sigctxt) {
+	print("r1      ", hex(c.r1()), "\n")
+	print("r2      ", hex(c.r2()), "\n")
+	print("r3      ", hex(c.r3()), "\n")
+	print("r4      ", hex(c.r4()), "\n")
+	print("r5      ", hex(c.r5()), "\n")
+	print("r6      ", hex(c.r6()), "\n")
+	print("r7      ", hex(c.r7()), "\n")
+	print("r8      ", hex(c.r8()), "\n")
+	print("r9      ", hex(c.r9()), "\n")
+	print("r10     ", hex(c.r10()), "\n")
+	print("r11     ", hex(c.r11()), "\n")
+	print("r12     ", hex(c.r12()), "\n")
+	print("r13     ", hex(c.r13()), "\n")
+	print("r16     ", hex(c.r16()), "\n")
+	print("r17     ", hex(c.r17()), "\n")
+	print("r18     ", hex(c.r18()), "\n")
+	print("r19     ", hex(c.r19()), "\n")
+	print("r20     ", hex(c.r20()), "\n")
+	print("r21     ", hex(c.r21()), "\n")
+	print("r22     ", hex(c.r22()), "\n")
+	print("r23     ", hex(c.r23()), "\n")
+	print("r24     ", hex(c.r24()), "\n")
+	print("r25     ", hex(c.r25()), "\n")
+	print("r26     ", hex(c.r26()), "\n")
+	print("r27     ", hex(c.r27()), "\n")
+	print("r28     ", hex(c.r28()), "\n")
+	print("r29     ", hex(c.r29()), "\n")
+	print("r31     ", hex(c.r31()), "\n")
+	print("lr      ", hex(c.lr()), "\n")
+	print("sp      ", hex(c.sp()), "\n")
+	print("fp      ", hex(c.fp()), "\n")
+	print("pc      ", hex(c.pc()), "\n")
+	print("npc     ", hex(c.npc()), "\n")
+	print("fault   ", hex(c.fault()), "\n")
+}
+
+var crashing int32
+
+// May run during STW, so write barriers are not allowed.
+//
+//go:nowritebarrierrec
+func sighandler(sig uint32, info *siginfo, ctxt unsafe.Pointer, gp *g) {
+	_g_ := getg()
+	c := &sigctxt{info, ctxt}
+
+	if sig == _SIGPROF {
+		sigprof(uintptr(c.pc()), uintptr(c.sp()), uintptr(c.lr()), gp, _g_.m)
+		return
+	}
+
+	flags := int32(_SigThrow)
+	if sig < uint32(len(sigtable)) {
+		flags = sigtable[sig].flags
+	}
+	if c.sigcode() != _SI_USER && flags&_SigPanic != 0 {
+		// Make it look like a call to the signal func.
+		// Have to pass arguments out of band since
+		// augmenting the stack frame would break
+		// the unwinding code.
+		gp.sig = sig
+		gp.sigcode0 = uintptr(c.sigcode())
+		gp.sigcode1 = uintptr(c.fault())
+		gp.sigpc = uintptr(c.pc())
+
+		// We arrange lr, and pc to pretend the panicking
+		// function calls sigpanic directly.
+		// Always save LR to stack so that panics in leaf
+		// functions are correctly handled. This smashes
+		// the stack frame but we're not going back there
+		// anyway.
+		fp := c.sp()
+		sp := c.sp() - sys.MinFrameSize
+		c.set_sp(sp)
+		c.set_fp(fp)
+		*(*uint64)(unsafe.Pointer(uintptr(sp + 120))) = c.lr()
+
+		pc := uintptr(gp.sigpc)
+
+		// If we don't recognize the PC as code
+		// but we do recognize the link register as code,
+		// then assume this was a call to non-code and treat like
+		// pc == 0, to make unwinding show the context.
+		if pc != 0 && findfunc(pc) == nil && findfunc(uintptr(c.lr())) != nil {
+			pc = 0
+		}
+
+		// Don't bother saving PC if it's zero, which is
+		// probably a call to a nil func: the old link register
+		// is more useful in the stack trace.
+		if pc != 0 {
+			c.set_lr(uint64(pc))
+		}
+
+		// In case we are panicking from external C code
+		c.set_r3(uint64(uintptr(unsafe.Pointer(gp))))
+		c.set_npc(uint64(funcPC(sigpanic)))
+		c.set_pc(uint64(funcPC(sigpanic)))
+		return
+	}
+
+	if c.sigcode() == _SI_USER || flags&_SigNotify != 0 {
+		if sigsend(sig) {
+			return
+		}
+	}
+
+	if c.sigcode() == _SI_USER && signal_ignored(sig) {
+		return
+	}
+
+	if flags&_SigKill != 0 {
+		dieFromSignal(int32(sig))
+	}
+
+	if flags&_SigThrow == 0 {
+		return
+	}
+
+	_g_.m.throwing = 1
+	_g_.m.caughtsig.set(gp)
+
+	if crashing == 0 {
+		startpanic()
+	}
+
+	if sig < uint32(len(sigtable)) {
+		print(sigtable[sig].name, "\n")
+	} else {
+		print("Signal ", sig, "\n")
+	}
+
+	print("PC=", hex(c.pc()), " m=", _g_.m.id, "\n")
+	if _g_.m.lockedg != nil && _g_.m.ncgo > 0 && gp == _g_.m.g0 {
+		print("signal arrived during cgo execution\n")
+		gp = _g_.m.lockedg
+	}
+	print("\n")
+
+	level, _, docrash := gotraceback()
+	if level > 0 {
+		goroutineheader(gp)
+		tracebacktrap(uintptr(c.pc()), uintptr(c.sp()), uintptr(c.lr()), gp)
+		if crashing > 0 && gp != _g_.m.curg && _g_.m.curg != nil && readgstatus(_g_.m.curg)&^_Gscan == _Grunning {
+			// tracebackothers on original m skipped this one; trace it now.
+			goroutineheader(_g_.m.curg)
+			traceback(^uintptr(0), ^uintptr(0), 0, gp)
+		} else if crashing == 0 {
+			tracebackothers(gp)
+			print("\n")
+		}
+		dumpregs(c)
+	}
+
+	if docrash {
+		crashing++
+		if crashing < sched.mcount {
+			// There are other m's that need to dump their stacks.
+			// Relay SIGQUIT to the next m by sending it to the current process.
+			// All m's that have already received SIGQUIT have signal masks blocking
+			// receipt of any signals, so the SIGQUIT will go to an m that hasn't seen it yet.
+			// When the last m receives the SIGQUIT, it will fall through to the call to
+			// crash below. Just in case the relaying gets botched, each m involved in
+			// the relay sleeps for 5 seconds and then does the crash/exit itself.
+			// In expected operation, the last m has received the SIGQUIT and run
+			// crash/exit and the process is gone, all long before any of the
+			// 5-second sleeps have finished.
+			print("\n-----\n\n")
+			raiseproc(_SIGQUIT)
+			usleep(5 * 1000 * 1000)
+		}
+		crash()
+	}
+
+	exit(2)
+}
diff --git a/src/runtime/slice.go b/src/runtime/slice.go
index e15e6c4..1d45d7a 100644
--- a/src/runtime/slice.go
+++ b/src/runtime/slice.go
@@ -130,7 +130,9 @@ func growslice(et *_type, old slice, cap int) slice {
 		// Note: can't use rawmem (which avoids zeroing of memory), because then GC can scan uninitialized memory.
 		p = mallocgc(capmem, et, true)
 		if !writeBarrier.enabled {
-			memmove(p, old.array, lenmem)
+			if lenmem > 0 {
+				memmove(p, old.array, lenmem)
+			}
 		} else {
 			for i := uintptr(0); i < lenmem; i += et.size {
 				typedmemmove(et, add(p, i), add(old.array, i))
diff --git a/src/runtime/stack.go b/src/runtime/stack.go
index 8398a10..7ac7487 100644
--- a/src/runtime/stack.go
+++ b/src/runtime/stack.go
@@ -67,8 +67,8 @@ const (
 	// and Darwin/ARM because they do not use a separate stack.
 	_StackSystem = sys.GoosWindows*512*sys.PtrSize + sys.GoosPlan9*512 + sys.GoosDarwin*sys.GoarchArm*1024
 
-	// The minimum size of stack used by Go code
-	_StackMin = 2048
+	// The minimum size of stack used by Go code.
+	_StackMin = 4096
 
 	// The minimum stack size to allocate.
 	// The hackery here rounds FixedStack0 up to a power of 2.
@@ -90,12 +90,12 @@ const (
 
 	// The stack guard is a pointer this many bytes above the
 	// bottom of the stack.
-	_StackGuard = 720*sys.StackGuardMultiplier + _StackSystem
+	_StackGuard = 2048*sys.StackGuardMultiplier + _StackSystem
 
 	// After a stack split check the SP is allowed to be this
 	// many bytes below the stack guard. This saves an instruction
 	// in the checking sequence for tiny frames.
-	_StackSmall = 128
+	_StackSmall = 256
 
 	// The maximum number of bytes that a chain of NOSPLIT
 	// functions can use.
@@ -337,12 +337,16 @@ func stackalloc(n uint32) (stack, []stkbar) {
 	nstkbar := unsafe.Sizeof(stkbar{}) * uintptr(maxstkbar)
 
 	if debug.efence != 0 || stackFromSystem != 0 {
-		v := sysAlloc(round(uintptr(n), _PageSize), &memstats.stacks_sys)
+		allocsz := round(uintptr(n), _PageSize)
+		v := sysAlloc(allocsz, &memstats.stacks_sys)
 		if v == nil {
 			throw("out of memory (stackalloc)")
 		}
 		top := uintptr(n) - nstkbar
 		stkbarSlice := slice{add(v, top), 0, maxstkbar}
+		if stackDebug >= 1 {
+			print("  allocated bytes ", allocsz, " lo ",  v, " hi ", hex(uintptr(v) + top), "\n")
+		}
 		return stack{uintptr(v), uintptr(v) + top}, *(*[]stkbar)(unsafe.Pointer(&stkbarSlice))
 	}
 
@@ -406,11 +410,11 @@ func stackalloc(n uint32) (stack, []stkbar) {
 	if msanenabled {
 		msanmalloc(v, uintptr(n))
 	}
-	if stackDebug >= 1 {
-		print("  allocated ", v, "\n")
-	}
 	top := uintptr(n) - nstkbar
 	stkbarSlice := slice{add(v, top), 0, maxstkbar}
+	if stackDebug >= 1 {
+		print("  allocated bytes ", n, " lo ",  v, " hi ", hex(uintptr(v) + top), "\n")
+	}
 	return stack{uintptr(v), uintptr(v) + top}, *(*[]stkbar)(unsafe.Pointer(&stkbarSlice))
 }
 
@@ -434,6 +438,10 @@ func stackfree(stk stack, n uintptr) {
 		memclr(v, n) // for testing, clobber stack data
 	}
 	if debug.efence != 0 || stackFromSystem != 0 {
+		// TODO(shawn): stackalloc uses rounded size, but doesn't
+		// return this to caller, to avoid leaking memory or not
+		// faulting, apply same rounding here; see issue #17289.
+		n = round(uintptr(n), _PageSize)
 		if debug.efence != 0 || stackFaultOnFree != 0 {
 			sysFault(v, n)
 		} else {
@@ -523,6 +531,31 @@ var ptrnames = []string{
 // +------------------+
 // |  return address  |
 // +------------------+ <- frame->sp
+//
+// (sparc64)
+//                 +------------------+
+//                 | args from caller |
+// RFP+BIAS+176 -> +------------------+ <- frame->argp
+//                 |    save area     |
+//                 +------------------+
+//                 | caller's retaddr |
+// RFP+BIAS+120 -> +------------------+
+//                 |   caller's RFP   |
+// RFP+BIAS+112 -> +------------------+
+//                 |     save area    |                 CALLER
+// --- RFP+BIAS -> +------------------+ <- frame->varp -------
+//                 |      locals      |                 CALLEE
+//                 +------------------+
+//                 |  args to callee  |
+// RSP+BIAS+176 -> +------------------+
+//                 |     save area    |
+//                 +------------------+
+//                 |  return address  | (not used in epilog, used by Go)
+// RSP+BIAS+120 -> +------------------+
+//                 |      our RFP     | (caller's RSP)
+// RFP+BIAS+112 -> +------------------+
+//                 |     save area    |
+//     RSP+BIAS -> +------------------+ <- frame->sp
 
 type adjustinfo struct {
 	old   stack
@@ -549,6 +582,28 @@ func adjustpointer(adjinfo *adjustinfo, vpp unsafe.Pointer) {
 	}
 }
 
+// Adjustrawpointer checks whether *vpp+StackBias is in the old stack described by adjinfo.
+// If so, it rewrites *vpp to point into the new stack.
+func adjustrawpointer(adjinfo *adjustinfo, vpp unsafe.Pointer) {
+	pp := (*uintptr)(vpp)
+	p := *pp + sys.StackBias
+	if stackDebug >= 4 {
+		print("        ", pp, ":", hex(p), "\n")
+	}
+	if adjinfo.old.lo <= p {
+		if p < adjinfo.old.hi {
+			*pp = p + adjinfo.delta - sys.StackBias
+			if stackDebug >= 3 {
+				print("        adjust bp ", pp, ":", hex(p-sys.StackBias), " -> ", hex(*pp), "\n")
+			}
+		} else if stackDebug >= 3 {
+				print("        >= old.hi; NOT adjusting bp ", pp, ":", hex(p-sys.StackBias), "\n")
+		}
+	} else if stackDebug >= 3 {
+		print("        > old.lo; NOT adjusting bp ", pp, ":", hex(p-sys.StackBias), "\n")
+	}
+}
+
 // Information from the compiler about the layout of stack frames.
 type bitvector struct {
 	n        int32 // # of bits
@@ -585,6 +640,7 @@ func adjustpointers(scanp unsafe.Pointer, cbv *bitvector, adjinfo *adjustinfo, f
 	// could race with adjusting those pointers. (The sent value
 	// itself can never contain stack pointers.)
 	useCAS := uintptr(scanp) < adjinfo.sghi
+	abortAdjust := false
 	for i := uintptr(0); i < num; i++ {
 		if stackDebug >= 4 {
 			print("        ", add(scanp, i*sys.PtrSize), ":", ptrnames[ptrbit(&bv, i)], ":", hex(*(*uintptr)(add(scanp, i*sys.PtrSize))), " # ", i, " ", bv.bytedata[i/8], "\n")
@@ -597,12 +653,13 @@ func adjustpointers(scanp unsafe.Pointer, cbv *bitvector, adjinfo *adjustinfo, f
 				// Looks like a junk value in a pointer slot.
 				// Live analysis wrong?
 				getg().m.traceback = 2
+				println("old -> hi delta ", hex(delta), "(", delta, ")")
 				print("runtime: bad pointer in frame ", funcname(f), " at ", pp, ": ", hex(p), "\n")
-				throw("invalid stack pointer")
+				abortAdjust = true
 			}
 			if minp <= p && p < maxp {
 				if stackDebug >= 3 {
-					print("adjust ptr ", p, " ", funcname(f), "\n")
+					print("adjust ptr ", hex(p), " ", funcname(f), "\n")
 				}
 				if useCAS {
 					ppu := (*unsafe.Pointer)(unsafe.Pointer(pp))
@@ -615,6 +672,9 @@ func adjustpointers(scanp unsafe.Pointer, cbv *bitvector, adjinfo *adjustinfo, f
 			}
 		}
 	}
+	if abortAdjust {
+		throw("invalid stack pointer(s)")
+	}
 }
 
 // Note: the argument/return area is adjusted by the callee.
@@ -635,7 +695,8 @@ func adjustframe(frame *stkframe, arg unsafe.Pointer) bool {
 		// have full GC info for it (because it is written in asm).
 		return true
 	}
-	if targetpc != f.entry {
+	// SPARC64's PC holds the address of the *current* instruction.
+	if targetpc != f.entry && sys.GoarchSparc64 == 0 {
 		targetpc--
 	}
 	pcdata := pcdatavalue(f, _PCDATA_StackMapIndex, targetpc, &adjinfo.cache)
@@ -652,6 +713,15 @@ func adjustframe(frame *stkframe, arg unsafe.Pointer) bool {
 	default:
 		minsize = sys.MinFrameSize
 	}
+
+	if sys.ArchFamily == sys.SPARC64 && size >= sys.MinFrameSize {
+		// framepointer is always available if there's a frame on sparc64
+		if stackDebug >= 3 {
+			print("      saved bp\n")
+		}
+		adjustrawpointer(adjinfo, unsafe.Pointer(frame.sp+uintptr(112)))
+	}
+
 	if size > minsize {
 		var bv bitvector
 		stackmap := (*stackmap)(funcdata(f, _FUNCDATA_LocalsPointerMaps))
@@ -673,7 +743,6 @@ func adjustframe(frame *stkframe, arg unsafe.Pointer) bool {
 		adjustpointers(unsafe.Pointer(frame.varp-size), &bv, adjinfo, f)
 	}
 
-	// Adjust saved base pointer if there is one.
 	if sys.ArchFamily == sys.AMD64 && frame.argp-frame.varp == 2*sys.RegSize {
 		if !framepointer_enabled {
 			print("runtime: found space for saved base pointer, but no framepointer experiment\n")
@@ -893,6 +962,9 @@ func copystack(gp *g, newsize uintptr, sync bool) {
 	gp.stack = new
 	gp.stackguard0 = new.lo + _StackGuard // NOTE: might clobber a preempt request
 	gp.sched.sp = new.hi - used
+	if sys.ArchFamily == sys.SPARC64 {
+		adjustpointer(&adjinfo, unsafe.Pointer(&gp.sched.bp))
+	}
 	oldsize := gp.stackAlloc
 	gp.stackAlloc = newsize
 	gp.stkbar = newstkbar
@@ -944,8 +1016,8 @@ func newstack() {
 		gp.syscallsp = morebuf.sp
 		gp.syscallpc = morebuf.pc
 		print("runtime: newstack sp=", hex(gp.sched.sp), " stack=[", hex(gp.stack.lo), ", ", hex(gp.stack.hi), "]\n",
-			"\tmorebuf={pc:", hex(morebuf.pc), " sp:", hex(morebuf.sp), " lr:", hex(morebuf.lr), "}\n",
-			"\tsched={pc:", hex(gp.sched.pc), " sp:", hex(gp.sched.sp), " lr:", hex(gp.sched.lr), " ctxt:", gp.sched.ctxt, "}\n")
+			"\tmorebuf={pc:", hex(morebuf.pc), " sp:", hex(morebuf.sp), " bp:", hex(morebuf.bp), " lr:", hex(morebuf.lr), " stackguard=", hex(morebuf.g.ptr().stackguard0), "}\n",
+			"\tsched={pc:", hex(gp.sched.pc), " sp:", hex(gp.sched.sp), " bp=", hex(gp.sched.bp), " lr:", hex(gp.sched.lr), " ctxt:", gp.sched.ctxt, "}\n")
 
 		traceback(morebuf.pc, morebuf.sp, morebuf.lr, gp)
 		throw("runtime: stack split at bad time")
@@ -955,6 +1027,7 @@ func newstack() {
 	morebuf := thisg.m.morebuf
 	thisg.m.morebuf.pc = 0
 	thisg.m.morebuf.lr = 0
+	thisg.m.morebuf.bp = 0
 	thisg.m.morebuf.sp = 0
 	thisg.m.morebuf.g = 0
 	rewindmorestack(&gp.sched)
@@ -977,6 +1050,11 @@ func newstack() {
 	// it needs a lock held by the goroutine), that small preemption turns
 	// into a real deadlock.
 	if preempt {
+		if stackDebug >= 1 {
+			print("runtime: preempt0, newstack sp=", hex(gp.sched.sp), " stack=[", hex(gp.stack.lo), ", ", hex(gp.stack.hi), "]\n",
+				"\tmorebuf={pc:", hex(morebuf.pc), " sp:", hex(morebuf.sp), " bp:", hex(morebuf.bp), " lr:", hex(morebuf.lr), " stackguard=", hex(morebuf.g.ptr().stackguard0), "}\n",
+				"\tsched={pc:", hex(gp.sched.pc), " sp:", hex(gp.sched.sp), " bp:", hex(gp.sched.bp), " lr:", hex(gp.sched.lr), " ctxt:", gp.sched.ctxt, "}\n\n")
+		}
 		if thisg.m.locks != 0 || thisg.m.mallocing != 0 || thisg.m.preemptoff != "" || thisg.m.p.ptr().status != _Prunning {
 			// Let the goroutine keep running for now.
 			// gp->preempt is set, so it will be preempted next time.
@@ -995,8 +1073,8 @@ func newstack() {
 	}
 	if stackDebug >= 1 || sp < gp.stack.lo {
 		print("runtime: newstack sp=", hex(sp), " stack=[", hex(gp.stack.lo), ", ", hex(gp.stack.hi), "]\n",
-			"\tmorebuf={pc:", hex(morebuf.pc), " sp:", hex(morebuf.sp), " lr:", hex(morebuf.lr), "}\n",
-			"\tsched={pc:", hex(gp.sched.pc), " sp:", hex(gp.sched.sp), " lr:", hex(gp.sched.lr), " ctxt:", gp.sched.ctxt, "}\n")
+			"\tmorebuf={pc:", hex(morebuf.pc), " sp:", hex(morebuf.sp), " bp:", hex(morebuf.bp), " lr:", hex(morebuf.lr), " stackguard=", hex(morebuf.g.ptr().stackguard0), "}\n",
+			"\tsched={pc:", hex(gp.sched.pc), " sp:", hex(gp.sched.sp), " bp:", hex(gp.sched.bp), " lr:", hex(gp.sched.lr), " ctxt:", gp.sched.ctxt, "}\n")
 	}
 	if sp < gp.stack.lo {
 		print("runtime: gp=", gp, ", gp->status=", hex(readgstatus(gp)), "\n ")
@@ -1013,6 +1091,11 @@ func newstack() {
 	}
 
 	if preempt {
+		if stackDebug >= 1 {
+			print("runtime: preempt1, newstack sp=", hex(gp.sched.sp), " stack=[", hex(gp.stack.lo), ", ", hex(gp.stack.hi), "]\n",
+				"\tmorebuf={pc:", hex(morebuf.pc), " sp:", hex(morebuf.sp), " bp:", hex(morebuf.bp), " lr:", hex(morebuf.lr), " stackguard=", hex(morebuf.g.ptr().stackguard0), "}\n",
+				"\tsched={pc:", hex(gp.sched.pc), " sp:", hex(gp.sched.sp), " bp:", hex(gp.sched.bp), " lr:", hex(gp.sched.lr), " ctxt:", gp.sched.ctxt, "}\n\n")
+		}
 		if gp == thisg.m.g0 {
 			throw("runtime: preempt g0")
 		}
@@ -1069,7 +1152,7 @@ func newstack() {
 	// the gp is in a Gcopystack status.
 	copystack(gp, uintptr(newsize), true)
 	if stackDebug >= 1 {
-		print("stack grow done\n")
+		print("stack grow done\n\n")
 	}
 	casgstatus(gp, _Gcopystack, _Grunning)
 	gogo(&gp.sched)
diff --git a/src/runtime/symtab.go b/src/runtime/symtab.go
index 4f6fae2..764f72a 100644
--- a/src/runtime/symtab.go
+++ b/src/runtime/symtab.go
@@ -85,7 +85,8 @@ func (ci *Frames) Next() (frame Frame, more bool) {
 
 	entry := f.Entry()
 	xpc := pc
-	if xpc > entry && !ci.wasPanic {
+	// SPARC64's PC holds the address of the *current* instruction.
+	if xpc > entry && !ci.wasPanic && sys.GoarchSparc64 == 0 {
 		xpc--
 	}
 	file, line := f.FileLine(xpc)
@@ -354,6 +355,36 @@ func findmoduledatap(pc uintptr) *moduledata {
 	return nil
 }
 
+func funcNameForPc(pc uintptr) string {
+	f := findfunc(pc)
+	if f == nil {
+		return "nil"
+	}
+	s := funcname(f)
+	if pc != f.entry {
+		s += "+" + itox(uint64(pc-f.entry))
+	}
+	return s
+}
+
+func itox(v uint64) string {
+	const dig = "0123456789abcdef"
+	buf := rawbyteslice(100)
+	i := len(buf)
+	for i--; i > 0; i-- {
+		buf[i] = dig[v%16]
+		if v < 16 {
+			break
+		}
+		v /= 16
+	}
+	i--
+	buf[i] = 'x'
+	i--
+	buf[i] = '0'
+	return slicebytetostringtmp(buf[i:])
+}
+
 func findfunc(pc uintptr) *_func {
 	datap := findmoduledatap(pc)
 	if datap == nil {
diff --git a/src/runtime/sys_darwin_amd64.s b/src/runtime/sys_darwin_amd64.s
index ea2cc06..81eb333 100644
--- a/src/runtime/sys_darwin_amd64.s
+++ b/src/runtime/sys_darwin_amd64.s
@@ -182,7 +182,7 @@ TEXT time·now(SB),NOSPLIT,$0-12
 	CALL	nanotime<>(SB)
 
 	// generated code for
-	//	func f(x uint64) (uint64, uint64) { return x/1000000000, x%100000000 }
+	//	func f(x uint64) (uint64, uint64) { return x/1000000000, x%1000000000 }
 	// adapted to reduce duplication
 	MOVQ	AX, CX
 	MOVQ	$1360296554856532783, AX
diff --git a/src/runtime/sys_plan9_amd64.s b/src/runtime/sys_plan9_amd64.s
index 1492ef2..78a843f 100644
--- a/src/runtime/sys_plan9_amd64.s
+++ b/src/runtime/sys_plan9_amd64.s
@@ -98,7 +98,7 @@ TEXT time·now(SB),NOSPLIT,$8-12
 	MOVQ	0(SP), AX
 
 	// generated code for
-	//	func f(x uint64) (uint64, uint64) { return x/1000000000, x%100000000 }
+	//	func f(x uint64) (uint64, uint64) { return x/1000000000, x%1000000000 }
 	// adapted to reduce duplication
 	MOVQ	AX, CX
 	MOVQ	$1360296554856532783, AX
diff --git a/src/runtime/sys_solaris_amd64.s b/src/runtime/sys_solaris_amd64.s
index 07a7ace..bf73f71 100644
--- a/src/runtime/sys_solaris_amd64.s
+++ b/src/runtime/sys_solaris_amd64.s
@@ -347,7 +347,7 @@ TEXT time·now(SB),NOSPLIT,$8-12
 	MOVQ	0(SP), AX
 
 	// generated code for
-	//	func f(x uint64) (uint64, uint64) { return x/1000000000, x%100000000 }
+	//	func f(x uint64) (uint64, uint64) { return x/1000000000, x%1000000000 }
 	// adapted to reduce duplication
 	MOVQ	AX, CX
 	MOVQ	$1360296554856532783, AX
diff --git a/src/runtime/sys_solaris_sparc64.s b/src/runtime/sys_solaris_sparc64.s
new file mode 100644
index 0000000..0b06d26
--- /dev/null
+++ b/src/runtime/sys_solaris_sparc64.s
@@ -0,0 +1,283 @@
+// Copyright 2016 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+//
+// System calls and other sys.stuff for SPARC64, Solaris.
+//
+
+#include "go_asm.h"
+#include "go_tls.h"
+#include "textflag.h"
+#include "asm_sparc64.h"
+
+// void libc_miniterrno(void *(*___errno)(void));
+//
+// Set the TLS errno pointer in M.
+//
+// Called using runtime·asmcgocall from os_solaris.c:/minit.
+// NOT USING GO CALLING CONVENTION.
+TEXT runtime·miniterrno(SB),NOSPLIT|REGWIN,$0
+	// asmcgocall will put first argument into I0.
+	CALL	I0	// SysV ABI so returns in O0
+	CALL	runtime·load_g(SB)
+	MOVD	g_m(g), I3
+	MOVD	O0,	(m_mOS+mOS_perrno)(I3)
+	RET
+
+// int64 runtime·nanotime1(void);
+//
+// clock_gettime(3c) wrapper because Timespec is too large for
+// runtime·nanotime stack.
+//
+// Called using runtime·sysvicall6 from os_solaris.c:/nanotime.
+// NOT USING GO CALLING CONVENTION.
+TEXT runtime·nanotime1(SB),NOSPLIT|REGWIN,$64
+	MOVW	$3, O0	// CLOCK_REALTIME from <sys/time_impl.h>
+	MOVD	$tv-16(SP), O1
+	MOVD	$libc_clock_gettime(SB), I3
+	CALL	I3
+	MOVD	tv_sec-16(SP), I3	// tv_sec from struct timespec
+	MOVD	$1000000000, I1
+	MULD	I1, I3	// multiply into nanoseconds
+	MOVD	tv_nsec-8(SP), I5	// tv_nsec, offset should be stable.
+	ADD	I5, I3, I0
+	RET
+
+// pipe(3c) wrapper that returns fds in AX, DX.
+// NOT USING GO CALLING CONVENTION.
+TEXT runtime·pipe1(SB),NOSPLIT|REGWIN,$16
+	MOVD	$FIXED_FRAME(BSP), O0
+	MOVD	$libc_pipe(SB), I3
+	CALL	I3
+	MOVW	(FIXED_FRAME+0)(BSP), I0
+	MOVW	(FIXED_FRAME+4)(BSP), I1
+	RET
+
+// Call a library function with SysV calling conventions.
+// The called function can take a maximum of 6 INTEGER class arguments,
+// see 
+// 	SYSTEM V APPLICATION BINARY INTERFACE
+// 	SPARC Version 9 Processor Supplement
+// section 3.2.2.
+//
+// Called by runtime·asmcgocall or runtime·cgocall.
+// NOT USING GO CALLING CONVENTION.
+TEXT runtime·asmsysvicall6(SB),NOSPLIT|REGWIN,$0
+	// asmcgocall will put first argument into I0.
+	MOVD	I0, L6
+	MOVD	libcall_fn(I0), I3
+	MOVD	libcall_args(I0), L1
+	MOVD	libcall_n(I0), L2
+
+	CMP	ZR, g
+	BED	skiperrno1
+	MOVD	g_m(g), I5
+	MOVD	(m_mOS+mOS_perrno)(I5), I1
+	CMP	I1, ZR
+	BED	skiperrno1
+	MOVW	ZR, (I1)
+
+skiperrno1:
+	CMP	L1, ZR
+	BED	skipargs
+	// Load 6 args into correspondent registers.
+	MOVD	0(L1), O0
+	MOVD	8(L1), O1
+	MOVD	16(L1), O2
+	MOVD	24(L1), O3
+	MOVD	32(L1), O4
+	MOVD	40(L1), O5
+skipargs:
+
+	MOVD	g, L1
+	// Call SysV function
+	CALL	I3
+	MOVD	L1, g
+
+	// Return result
+	MOVD	O0, libcall_r1(L6)
+	MOVD	O1, libcall_r2(L6)
+	MOVD	O0, I0
+	MOVD	O1, I1
+
+	CMP	g, ZR
+	BED	skiperrno2
+	MOVD	g_m(g), I5
+	MOVD	(m_mOS+mOS_perrno)(I5), I4
+	CMP	I4, ZR
+	BED	skiperrno2
+	MOVW	(I4), I4
+	MOVD	I4, libcall_err(L6)
+
+skiperrno2:	
+	RET
+
+// uint32 tstart_sysvicall(M *newm);
+TEXT runtime·tstart_sysvicall(SB),NOSPLIT|REGWIN,$0
+	// I0 contains first arg newm
+	MOVD	m_g0(I0), g		// g
+	MOVD	I0, g_m(g)
+
+	CALL	runtime·save_g(SB)
+
+	// Layout new m scheduler stack on os stack.
+	MOVD	BSP, I3
+	MOVD	I3, (g_stack+stack_hi)(g)
+	SUB	$(0x100000), I3		// stack size
+	MOVD	I3, (g_stack+stack_lo)(g)
+	ADD	$const__StackGuard, I3
+	MOVD	I3, g_stackguard0(g)
+	MOVD	I3, g_stackguard1(g)
+
+	// initialize essential registers
+	CALL	runtime·reginit(SB)
+
+	CALL	runtime·stackcheck(SB)
+	CALL	runtime·mstart(SB)
+
+	MOVW	ZR, ret+8(FP)
+	RET
+
+#define SIGTRAMP_FRAME 144
+
+// Careful, this is called by __sighndlr, a libc function.
+// We must preserve registers as per SPARC64 ABI.
+TEXT runtime·sigtramp(SB),NOSPLIT|REGWIN,$SIGTRAMP_FRAME
+	MOVD	g, L1
+	CALL	runtime·load_g(SB)
+	CMP	g, ZR
+	BNED	allgood
+	MOVD	L1, g
+	MOVD	I0, (8*0+FIXED_FRAME)(BSP)
+	MOVD	ZR, (8*1+FIXED_FRAME)(BSP)
+	MOVD	$runtime·badsignal(SB), L1
+	CALL	(L1)
+	JMP	exit
+
+allgood:
+	// initialize essential registers (just in case)
+	CALL	runtime·reginit(SB)
+
+	// save g
+	MOVD	g, (-8-0*8+SIGTRAMP_FRAME+FIXED_FRAME)(BSP)
+
+	// Save m->libcall and m->scratch. We need to do this because we
+	// might get interrupted by a signal in runtime·asmcgocall.
+
+	// save m->libcall 
+	MOVD	g_m(g), L1
+	MOVD	$m_libcall(L1), L2
+	MOVD	libcall_fn(L2), L3
+	MOVD	L3, (-8-1*8+SIGTRAMP_FRAME+FIXED_FRAME)(BSP)
+	MOVD	libcall_args(L2), L3
+	MOVD	L3, (-8-2*8+SIGTRAMP_FRAME+FIXED_FRAME)(BSP)
+	MOVD	libcall_n(L2), L3
+	MOVD	L3, (-8-3*8+SIGTRAMP_FRAME+FIXED_FRAME)(BSP)
+	MOVD	libcall_r1(L2), L3
+	MOVD	L3, (-8-4*8+SIGTRAMP_FRAME+FIXED_FRAME)(BSP)
+	MOVD	libcall_r2(L2), L3
+	MOVD	L3, (-8-5*8+SIGTRAMP_FRAME+FIXED_FRAME)(BSP)
+
+	// save m->scratch
+	MOVD	$(m_mOS+mOS_scratch)(L1), L2
+	MOVD	0(L2), L3
+	MOVD	L3, (-8-6*8+SIGTRAMP_FRAME+FIXED_FRAME)(BSP)
+	MOVD	8(L2), L3
+	MOVD	L3, (-8-7*8+SIGTRAMP_FRAME+FIXED_FRAME)(BSP)
+	MOVD	16(L2), L3
+	MOVD	L3, (-8-8*8+SIGTRAMP_FRAME+FIXED_FRAME)(BSP)
+	MOVD	24(L2), L3
+	MOVD	L3, (-8-9*8+SIGTRAMP_FRAME+FIXED_FRAME)(BSP)
+	MOVD	32(L2), L3
+	MOVD	L3, (-8-10*8+SIGTRAMP_FRAME+FIXED_FRAME)(BSP)
+	MOVD	40(L2), L3
+	MOVD	L3, (-8-11*8+SIGTRAMP_FRAME+FIXED_FRAME)(BSP)
+
+	// save errno, it might be EINTR; stuff we do here might reset it.
+	MOVD	(m_mOS+mOS_perrno)(L1), L2
+	MOVW	0(L2), L2
+	MOVD	L2, (-8-12*8+SIGTRAMP_FRAME+FIXED_FRAME)(BSP)
+
+	// prepare call
+	MOVW	I0, (8*0+FIXED_FRAME)(BSP)
+	MOVD	I1, (8*1+FIXED_FRAME)(BSP)
+	MOVD	I2, (8*2+FIXED_FRAME)(BSP)
+	MOVD	g, (8*3+FIXED_FRAME)(BSP)
+
+	// g = m->gsignal
+	MOVD	m_gsignal(L1), g
+	CALL	runtime·save_g(SB)
+
+	// TODO(shawn): If current SP is not in gsignal.stack, then assume
+	// non-Go code caused a signal and adjust gsignal.stack?
+	MOVD	(g_stack+stack_lo)(g), L2
+	MOVD	BSP, TMP
+	CMP	L2, TMP
+	BGED	checkhi
+	CALL	runtime·abort(SB)
+
+checkhi:
+	MOVD	(g_stack+stack_hi)(g), L2
+	MOVD	BSP, TMP
+	CMP	L2, TMP
+	BLD	handler
+	CALL	runtime·abort(SB)
+
+handler:
+	CALL	runtime·sighandler(SB)
+
+	MOVD	g_m(g), L1
+	// restore libcall
+	MOVD	$m_libcall(L1), L2
+	MOVD	(-8-1*8+SIGTRAMP_FRAME+FIXED_FRAME)(BSP), L3
+	MOVD	L3, libcall_fn(L2)
+	MOVD	(-8-2*8+SIGTRAMP_FRAME+FIXED_FRAME)(BSP), L3
+	MOVD	L3, libcall_args(L2)
+	MOVD	(-8-3*8+SIGTRAMP_FRAME+FIXED_FRAME)(BSP), L3	
+	MOVD	L3, libcall_n(L2)
+	MOVD	(-8-4*8+SIGTRAMP_FRAME+FIXED_FRAME)(BSP), L3
+	MOVD	L3, libcall_r1(L2)
+	MOVD	(-8-5*8+SIGTRAMP_FRAME+FIXED_FRAME)(BSP), L3
+	MOVD	L3, libcall_r2(L2)
+
+	// restore scratch
+	MOVD	$(m_mOS+mOS_scratch)(L1), L2
+	MOVD	(-8-6*8+SIGTRAMP_FRAME+FIXED_FRAME)(BSP), L3
+	MOVD	L3, 0(L2)
+	MOVD	(-8-7*8+SIGTRAMP_FRAME+FIXED_FRAME)(BSP), L3
+	MOVD	L3, 8(L2)
+	MOVD	(-8-8*8+SIGTRAMP_FRAME+FIXED_FRAME)(BSP), L3
+	MOVD	L3, 16(L2)
+	MOVD	(-8-9*8+SIGTRAMP_FRAME+FIXED_FRAME)(BSP), L3
+	MOVD	L3, 24(L2)
+	MOVD	(-8-10*8+SIGTRAMP_FRAME+FIXED_FRAME)(BSP), L3
+	MOVD	L3, 32(L2)
+	MOVD	(-8-11*8+SIGTRAMP_FRAME+FIXED_FRAME)(BSP), L3
+	MOVD	L3, 40(L2)
+
+	// restore errno
+	MOVD	(m_mOS+mOS_perrno)(L1), L2
+	MOVD	(-8-12*8+SIGTRAMP_FRAME+FIXED_FRAME)(BSP), L3
+	MOVW	L3, 0(L2)
+
+	// restore g
+	MOVD	(-8-0*8+SIGTRAMP_FRAME+FIXED_FRAME)(BSP), g
+	CALL	runtime·save_g(SB)
+
+exit:
+	// kernel will restore registers
+	RET
+
+
+// Runs on OS stack, called from runtime·usleep1_go.
+TEXT runtime·usleep2(SB),NOSPLIT|REGWIN,$0
+	MOVW	us+0(FP), O0
+	MOVD	$libc_usleep(SB), I3
+	CALL	I3
+	RET
+
+// Runs on OS stack, called from runtime·osyield.
+TEXT runtime·osyield1(SB),NOSPLIT|REGWIN,$0
+	MOVD	$libc_sched_yield(SB), I3
+	CALL	I3
+	RET
diff --git a/src/runtime/sys_sparc64.go b/src/runtime/sys_sparc64.go
new file mode 100644
index 0000000..32b761a
--- /dev/null
+++ b/src/runtime/sys_sparc64.go
@@ -0,0 +1,67 @@
+// Copyright 2016 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package runtime
+
+import "unsafe"
+
+// adjust Gobuf as if it executed a call to fn with context ctxt
+// and then did an immediate Gosave.
+func gostartcall(buf *gobuf, fn, ctxt unsafe.Pointer) {
+	if buf.lr != 0 {
+		throw("invalid use of gostartcall")
+	}
+	buf.lr = buf.pc
+	buf.pc = uintptr(fn)
+	buf.ctxt = ctxt
+}
+
+// Called to rewind context saved during morestack back to beginning of function.
+// To help us, the linker emits a call back to the beginning 8 bytes after the
+// call to morestack. We just have to decode and apply that jump.
+func rewindmorestack(buf *gobuf) {
+	var inst uint32
+	if buf.pc&3 == 0 && buf.pc != 0 {
+		// For sparc, the pc register holds the address of the
+		// *current* instruction, rather than the next instruction
+		// to execute, and CTIs are padded with a nop to avoid DCTI
+		// coupling, so we need to skip ahead to get the jump.
+		buf.pc += 8
+		inst = *(*uint32)(unsafe.Pointer(buf.pc))
+		// Extract opcode
+		op1 := inst >> 30
+		// call and link
+		mop1 := 1 << 30
+
+		call := uint32(mop1 >> 30)
+		if op1 == call {
+			// Extract pc-relative address (4*sign_ext(disp30))
+			idisp30 := 4 * (int32(inst<<2) >> 2)
+
+			//ipc := uintptr(unsafe.Pointer(buf.pc))
+			buf.pc += uintptr(idisp30)
+
+			//print("runtime: rewind pc=", hex(ipc), " to pc=", hex(buf.pc), "\n");
+			return
+		}
+	}
+	print("runtime: pc=", hex(buf.pc), " ", hex(inst), "\n")
+	throw("runtime: misuse of rewindmorestack")
+}
+
+func usleep2(us uint32)
+
+//go:linkname usleep1_go runtime.usleep1
+//go:nosplit
+func usleep1_go(µs uint32) {
+	_g_ := getg()
+
+	// Check the validity of m because we might be called in cgo callback
+	// path early enough where there isn't a m available yet.
+	if _g_ != nil && _g_.m != nil {
+		sysvicall1(&libc_usleep, uintptr(µs))
+		return
+	}
+	usleep2(µs)
+}
diff --git a/src/runtime/sys_windows_amd64.s b/src/runtime/sys_windows_amd64.s
index 9c19737..227252f 100644
--- a/src/runtime/sys_windows_amd64.s
+++ b/src/runtime/sys_windows_amd64.s
@@ -463,7 +463,7 @@ TEXT time·now(SB),NOSPLIT,$8-12
 	MOVQ	0(SP), AX
 
 	// generated code for
-	//	func f(x uint64) (uint64, uint64) { return x/1000000000, x%100000000 }
+	//	func f(x uint64) (uint64, uint64) { return x/1000000000, x%1000000000 }
 	// adapted to reduce duplication
 	MOVQ	AX, CX
 	MOVQ	$1360296554856532783, AX
diff --git a/src/runtime/textflag.h b/src/runtime/textflag.h
index 929e9b3..fb14e3c 100644
--- a/src/runtime/textflag.h
+++ b/src/runtime/textflag.h
@@ -32,3 +32,5 @@
 #define NOFRAME 512
 // Function can call reflect.Type.Method or reflect.Type.MethodByName.
 #define REFLECTMETHOD = 1024
+// This function switches the register window. Only for SPARC64.
+#define REGWIN 2048
diff --git a/src/runtime/time_sparc64.go b/src/runtime/time_sparc64.go
new file mode 100644
index 0000000..e9b62f6
--- /dev/null
+++ b/src/runtime/time_sparc64.go
@@ -0,0 +1,16 @@
+// Copyright 2016 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package runtime
+
+import "unsafe"
+
+var _ = unsafe.Sizeof(0)
+
+//go:nosplit
+//go:linkname time_now_sparc64 time.now
+func time_now_sparc64() (sec int64, nsec int32) {
+	ns := nanotime()
+	return ns / 1000000000, int32(ns % 1000000000)
+}
diff --git a/src/runtime/tls_sparc64.s b/src/runtime/tls_sparc64.s
new file mode 100644
index 0000000..483215d
--- /dev/null
+++ b/src/runtime/tls_sparc64.s
@@ -0,0 +1,75 @@
+// Copyright 2016 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+#include "go_asm.h"
+#include "funcdata.h"
+#include "textflag.h"
+#include "asm_sparc64.h"
+
+// save_g saves the g register into pthread-provided
+// thread-local memory, so that we can call externally compiled
+// sparc64 code that will overwrite this register.
+//
+// If !iscgo, this is a no-op.
+//
+// NOTE: setg_gcc<> assume this clobbers only RT1.
+TEXT runtime·save_g(SB),NOSPLIT|NOFRAME,$0-0
+// On Solaris we always use TLS, even without cgo.
+#ifndef GOOS_solaris
+	MOVB	runtime·iscgo(SB), RT1
+	CMP	RT1, ZR
+	BEW	nocgo
+#endif
+
+	MOVD	$runtime·tls_g(SB), RT1
+	MOVD	g, (RT1)
+nocgo:
+	RET
+
+// load_g loads the g register from pthread-provided
+// thread-local memory, for use after calling externally compiled
+// sparc64 code that overwrote those registers.
+//
+// This is never called directly from C code (it doesn't have to
+// follow the C ABI), but it may be called from a C context, where the
+// usual Go registers aren't set up.
+//
+// NOTE: _cgo_topofstack assumes this only clobbers g, and RT1.
+TEXT runtime·load_g(SB),NOSPLIT|NOFRAME,$0-0
+// On Solaris we always use TLS, even without cgo.
+#ifndef GOOS_solaris
+	MOVB	runtime·iscgo(SB), RT1
+	CMP	RT1, ZR
+	BEW	nocgo
+#endif
+
+	MOVD	$runtime·tls_g(SB), RT1
+	MOVD	(RT1), g
+nocgo:
+	RET
+
+TEXT runtime·do_cgo_init(SB),NOSPLIT,$0-0
+	// if there is a _cgo_init, call it using the gcc ABI.
+	MOVD	_cgo_init(SB), O4
+	CMP	ZR, O4
+	BED	nocgo
+
+	MOVD	TLS, O3			// arg 3: TLS base pointer
+	MOVD	$runtime·tls_g(SB), O2 	// arg 2: &tls_g
+	MOVD	$setg_gcc<>(SB), O1	// arg 1: setg
+	MOVD	g, O0			// arg 0: G
+	CALL	(O4)
+	MOVD	$runtime·g0(SB), g
+nocgo:
+	RET
+
+// void setg_gcc(G*); set g called from gcc
+TEXT setg_gcc<>(SB),NOSPLIT,$16
+	MOVD	O0, g
+	MOVD	RT1, savedRT1-8(SP)
+	CALL	runtime·save_g(SB)
+	MOVD	savedRT1-8(SP), RT1
+	RET
+
+GLOBL runtime·tls_g+0(SB), TLSBSS, $8
diff --git a/src/runtime/trace.go b/src/runtime/trace.go
index 092f941..a9dfc2a 100644
--- a/src/runtime/trace.go
+++ b/src/runtime/trace.go
@@ -790,7 +790,13 @@ func traceFrameForPC(buf *traceBuf, frames map[uintptr]traceFrame, pc uintptr) (
 		fn = fn[len(fn)-maxLen:]
 	}
 	frame.funcID, buf = traceString(buf, fn)
-	file, line := funcline(f, pc-sys.PCQuantum)
+	tracepc := pc
+	if sys.GoarchSparc64 != 1 {
+		// SPARC64's PC holds the address of the *current*
+		// instruction, so it doesn't need this.
+		tracepc -= sys.PCQuantum
+	}
+	file, line := funcline(f, tracepc)
 	frame.line = uint64(line)
 	if len(file) > maxLen {
 		file = file[len(file)-maxLen:]
@@ -899,8 +905,14 @@ func traceGCSweepDone() {
 func traceGoCreate(newg *g, pc uintptr) {
 	newg.traceseq = 0
 	newg.tracelastp = getg().m.p
-	// +PCQuantum because traceFrameForPC expects return PCs and subtracts PCQuantum.
-	id := trace.stackTab.put([]uintptr{pc + sys.PCQuantum})
+	tracepc := pc
+	if sys.GoarchSparc64 != 1 {
+		// +PCQuantum because traceFrameForPC expects return PCs and
+		// subtracts PCQuantum. SPARC64's PC holds the address of the
+		// *current* instruction, so it doesn't need this.
+		tracepc += sys.PCQuantum
+	}
+	id := trace.stackTab.put([]uintptr{tracepc})
 	traceEvent(traceEvGoCreate, 2, uint64(newg.goid), uint64(id))
 }
 
diff --git a/src/runtime/traceback.go b/src/runtime/traceback.go
index 80a5440..da458c0 100644
--- a/src/runtime/traceback.go
+++ b/src/runtime/traceback.go
@@ -35,6 +35,8 @@ import (
 
 const usesLR = sys.MinFrameSize > 0
 
+const returnAddrOffset = sys.GoarchSparc64 * 120
+
 var (
 	// initialized in tracebackinit
 	goexitPC             uintptr
@@ -184,7 +186,7 @@ func gentraceback(pc0, sp0, lr0 uintptr, gp *g, skip int, pcbuf *uintptr, max in
 	// Start in the caller's frame.
 	if frame.pc == 0 {
 		if usesLR {
-			frame.pc = *(*uintptr)(unsafe.Pointer(frame.sp))
+			frame.pc = *(*uintptr)(unsafe.Pointer(frame.sp + returnAddrOffset))
 			frame.lr = 0
 		} else {
 			frame.pc = uintptr(*(*sys.Uintreg)(unsafe.Pointer(frame.sp)))
@@ -286,7 +288,7 @@ func gentraceback(pc0, sp0, lr0 uintptr, gp *g, skip int, pcbuf *uintptr, max in
 			var lrPtr uintptr
 			if usesLR {
 				if n == 0 && frame.sp < frame.fp || frame.lr == 0 {
-					lrPtr = frame.sp
+					lrPtr = frame.sp + returnAddrOffset
 					frame.lr = *(*uintptr)(unsafe.Pointer(lrPtr))
 				}
 			} else {
@@ -386,7 +388,9 @@ func gentraceback(pc0, sp0, lr0 uintptr, gp *g, skip int, pcbuf *uintptr, max in
 				//		/home/rsc/go/src/runtime/x.go:23 +0xf
 				//
 				tracepc := frame.pc // back up to CALL instruction for funcline.
-				if (n > 0 || flags&_TraceTrap == 0) && frame.pc > f.entry && !waspanic {
+				// SPARC64's PC holds the address of the *current* 
+				// instruction, so it doesn't need this.
+				if (n > 0 || flags&_TraceTrap == 0) && frame.pc > f.entry && !waspanic && sys.GoarchSparc64 == 0 {
 					tracepc--
 				}
 				name := funcname(f)
@@ -451,7 +455,7 @@ func gentraceback(pc0, sp0, lr0 uintptr, gp *g, skip int, pcbuf *uintptr, max in
 		// On link register architectures, sighandler saves the LR on stack
 		// before faking a call to sigpanic.
 		if usesLR && waspanic {
-			x := *(*uintptr)(unsafe.Pointer(frame.sp))
+			x := *(*uintptr)(unsafe.Pointer(frame.sp + returnAddrOffset))
 			frame.sp += sys.MinFrameSize
 			if GOARCH == "arm64" {
 				// arm64 needs 16-byte aligned SP, always
@@ -606,7 +610,9 @@ func printcreatedby(gp *g) {
 	if f != nil && showframe(f, gp) && gp.goid != 1 {
 		print("created by ", funcname(f), "\n")
 		tracepc := pc // back up to CALL instruction for funcline.
-		if pc > f.entry {
+		// SPARC64's PC holds the address of the *current* 
+		// instruction, so it doesn't need this.
+		if pc > f.entry && sys.GoarchSparc64 == 0 {
 			tracepc -= sys.PCQuantum
 		}
 		file, line := funcline(f, tracepc)
diff --git a/src/runtime/unaligned2.go b/src/runtime/unaligned2.go
index fed3cca..d6ad7d0 100644
--- a/src/runtime/unaligned2.go
+++ b/src/runtime/unaligned2.go
@@ -2,7 +2,7 @@
 // Use of this source code is governed by a BSD-style
 // license that can be found in the LICENSE file.
 
-// +build arm mips64 mips64le
+// +build arm mips64 mips64le sparc64
 
 package runtime
 
diff --git a/src/sync/atomic/asm_sparc64.s b/src/sync/atomic/asm_sparc64.s
new file mode 100644
index 0000000..2f0615a
--- /dev/null
+++ b/src/sync/atomic/asm_sparc64.s
@@ -0,0 +1,170 @@
+// Copyright 2016 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+#include "textflag.h"
+#include "asm_sparc64.h"
+
+TEXT ·SwapInt32(SB),NOSPLIT|NOFRAME,$0-20
+	JMP	·SwapUint32(SB)
+
+TEXT ·SwapUint32(SB),NOSPLIT,$0-20
+	MOVD	addr+0(FP), I3
+	MOVUW	new+8(FP), I1
+again:
+	MEM_SYNC
+	MOVUW	(I3), I5
+	CASW	(I3), I5, I1
+	CMP	I1, I5
+	BNEW	again
+	MEM_SYNC
+	MOVUW	I5, old+16(FP)
+	RET
+
+TEXT ·SwapInt64(SB),NOSPLIT|NOFRAME,$0-24
+	JMP	·SwapUint64(SB)
+
+TEXT ·SwapUint64(SB),NOSPLIT,$0-24
+	MOVD	addr+0(FP), I3
+	MOVD	new+8(FP), I1
+again:
+	MEM_SYNC
+	MOVD	(I3), I5
+	CASD	(I3), I5, I1
+	CMP	I1, I5
+	BNED	again
+	MEM_SYNC
+	MOVD	I5, old+16(FP)
+	RET
+
+TEXT ·SwapUintptr(SB),NOSPLIT|NOFRAME,$0-24
+	JMP	·SwapUint64(SB)
+
+TEXT ·CompareAndSwapInt32(SB),NOSPLIT|NOFRAME,$0-17
+	JMP	·CompareAndSwapUint32(SB)
+
+TEXT ·CompareAndSwapUint32(SB),NOSPLIT,$0-17
+	MOVD	addr+0(FP), I1
+	MOVUW	old+8(FP), I3
+	MOVUW	new+12(FP), I5
+	MEM_SYNC
+	CASW	(I1), I3, I5
+	CMP	I5, I3
+	MOVD	$0, I3
+	MOVE	ICC, $1, I3
+	MEM_SYNC
+	MOVB	I3, swapped+16(FP)
+	RET
+
+TEXT ·CompareAndSwapUintptr(SB),NOSPLIT|NOFRAME,$0-25
+	JMP	·CompareAndSwapUint64(SB)
+
+TEXT ·CompareAndSwapInt64(SB),NOSPLIT|NOFRAME,$0-25
+	JMP	·CompareAndSwapUint64(SB)
+
+TEXT ·CompareAndSwapUint64(SB),NOSPLIT,$0-25
+	MOVD	addr+0(FP), I1
+	MOVD	old+8(FP), I3
+	MOVD	new+16(FP), I5
+	MEM_SYNC
+	CASD	(I1), I3, I5
+	CMP	I5, I3
+	MOVD	$0, I3
+	MOVE	XCC, $1, I3
+	MEM_SYNC
+	MOVB	I3, swapped+24(FP)
+	RET
+
+TEXT ·AddInt32(SB),NOSPLIT|NOFRAME,$0-20
+	JMP	·AddUint32(SB)
+
+TEXT ·AddUint32(SB),NOSPLIT,$0-20
+	MOVD	addr+0(FP), I4
+	MOVUW	delta+8(FP), I3
+	MOVUW	(I4), I1
+	MEM_SYNC
+retry:
+	ADD	I1, I3, I5
+	CASW	(I4), I1, I5
+	CMP	I1, I5
+	MOVNE	ICC, I5, I1
+	BNEW	retry
+	ADD	I1, I3, I5
+	MEM_SYNC
+	MOVUW	I5, new+16(FP)
+	RET
+
+TEXT ·AddUintptr(SB),NOSPLIT|NOFRAME,$0-24
+	JMP	·AddUint64(SB)
+
+TEXT ·AddInt64(SB),NOSPLIT|NOFRAME,$0-24
+	JMP	·AddUint64(SB)
+
+TEXT ·AddUint64(SB),NOSPLIT,$0-24
+	MOVD	addr+0(FP), I4
+	MOVD	delta+8(FP), I3
+	MEM_SYNC
+	MOVD	(I4), I1
+retry:
+	ADD	I1, I3, I5
+	CASD	(I4), I1, I5
+	CMP	I1, I5
+	MOVNE	XCC, I5, I1
+	BNED	retry
+	ADD	I1, I3, I5
+	MEM_SYNC
+	MOVD	I5, new+16(FP)
+	RET
+
+TEXT ·LoadInt32(SB),NOSPLIT|NOFRAME,$0-12
+	JMP	·LoadUint32(SB)
+
+TEXT ·LoadUint32(SB),NOSPLIT,$0-12
+	MOVD	addr+0(FP), I1
+	MEM_SYNC
+	LDUW	(I1), I1
+	MEM_SYNC
+	MOVUW	I1, val+8(FP)
+	RET
+
+TEXT ·LoadInt64(SB),NOSPLIT|NOFRAME,$0-16
+	JMP	·LoadUint64(SB)
+
+TEXT ·LoadUint64(SB),NOSPLIT,$0-16
+	MOVD	addr+0(FP), I1
+	MEM_SYNC
+	LDD	(I1), I1
+	MEM_SYNC
+	MOVD	I1, val+8(FP)
+	RET
+
+TEXT ·LoadUintptr(SB),NOSPLIT|NOFRAME,$0-16
+	JMP	·LoadPointer(SB)
+
+TEXT ·LoadPointer(SB),NOSPLIT|NOFRAME,$0-16
+	JMP	·LoadUint64(SB)
+
+TEXT ·StoreInt32(SB),NOSPLIT|NOFRAME,$0-12
+	JMP	·StoreUint32(SB)
+
+TEXT ·StoreUint32(SB),NOSPLIT,$0-12
+	MOVD	addr+0(FP), I3
+	MOVUW	val+8(FP), I5
+	MEM_SYNC
+	STW	I5, (I3)
+	MEM_SYNC
+	RET
+
+TEXT ·StoreInt64(SB),NOSPLIT|NOFRAME,$0-16
+	JMP	·StoreUint64(SB)
+
+TEXT ·StoreUint64(SB),NOSPLIT,$0-16
+	MOVD	addr+0(FP), I3
+	MOVD	val+8(FP), I5
+	MEM_SYNC
+	STD	I5, (I3)
+	MEM_SYNC
+	RET
+
+TEXT ·StoreUintptr(SB),NOSPLIT|NOFRAME,$0-16
+	JMP	·StoreUint64(SB)
diff --git a/src/syscall/asm_solaris_sparc64.s b/src/syscall/asm_solaris_sparc64.s
new file mode 100644
index 0000000..15c04a5
--- /dev/null
+++ b/src/syscall/asm_solaris_sparc64.s
@@ -0,0 +1,75 @@
+// Copyright 2016 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+#include "textflag.h"
+
+//
+// System calls for solaris/amd64 are implemented in ../runtime/syscall_solaris.go
+//
+
+TEXT ·sysvicall6(SB),NOSPLIT|NOFRAME,$0
+	JMP	runtime·syscall_sysvicall6(SB)
+
+TEXT ·rawSysvicall6(SB),NOSPLIT|NOFRAME,$0
+	JMP	runtime·syscall_rawsysvicall6(SB)
+
+TEXT ·chdir(SB),NOSPLIT|NOFRAME,$0
+	JMP	runtime·syscall_chdir(SB)
+
+TEXT ·chroot1(SB),NOSPLIT|NOFRAME,$0
+	JMP	runtime·syscall_chroot(SB)
+
+TEXT ·close(SB),NOSPLIT|NOFRAME,$0
+	JMP	runtime·syscall_close(SB)
+
+TEXT ·execve(SB),NOSPLIT|NOFRAME,$0
+	JMP	runtime·syscall_execve(SB)
+
+TEXT ·exit(SB),NOSPLIT|NOFRAME,$0
+	JMP	runtime·syscall_exit(SB)
+
+TEXT ·fcntl1(SB),NOSPLIT|NOFRAME,$0
+	JMP	runtime·syscall_fcntl(SB)
+
+TEXT ·forkx(SB),NOSPLIT|NOFRAME,$0
+	JMP	runtime·syscall_forkx(SB)
+
+TEXT ·gethostname(SB),NOSPLIT|NOFRAME,$0
+	JMP	runtime·syscall_gethostname(SB)
+
+TEXT ·getpid(SB),NOSPLIT|NOFRAME,$0
+	JMP	runtime·syscall_getpid(SB)
+
+TEXT ·ioctl(SB),NOSPLIT|NOFRAME,$0
+	JMP	runtime·syscall_ioctl(SB)
+
+TEXT ·pipe(SB),NOSPLIT|NOFRAME,$0
+	JMP	runtime·syscall_pipe(SB)
+
+TEXT ·RawSyscall(SB),NOSPLIT|NOFRAME,$0
+	JMP	runtime·syscall_rawsyscall(SB)
+
+TEXT ·setgid(SB),NOSPLIT|NOFRAME,$0
+	JMP	runtime·syscall_setgid(SB)
+
+TEXT ·setgroups1(SB),NOSPLIT|NOFRAME,$0
+	JMP	runtime·syscall_setgroups(SB)
+
+TEXT ·setsid(SB),NOSPLIT|NOFRAME,$0
+	JMP	runtime·syscall_setsid(SB)
+
+TEXT ·setuid(SB),NOSPLIT|NOFRAME,$0
+	JMP	runtime·syscall_setuid(SB)
+
+TEXT ·setpgid(SB),NOSPLIT|NOFRAME,$0
+	JMP	runtime·syscall_setpgid(SB)
+
+TEXT ·Syscall(SB),NOSPLIT|NOFRAME,$0
+	JMP	runtime·syscall_syscall(SB)
+
+TEXT ·wait4(SB),NOSPLIT|NOFRAME,$0
+	JMP	runtime·syscall_wait4(SB)
+
+TEXT ·write1(SB),NOSPLIT|NOFRAME,$0
+	JMP	runtime·syscall_write(SB)
diff --git a/src/syscall/exec_solaris.go b/src/syscall/exec_solaris.go
index fcb481c..a5597b7 100644
--- a/src/syscall/exec_solaris.go
+++ b/src/syscall/exec_solaris.go
@@ -111,14 +111,14 @@ func forkAndExecInChild(argv0 *byte, argv, envv []*byte, chroot, dir *byte, attr
 	}
 
 	if sys.Foreground {
-		pgrp := sys.Pgid
+		pgrp := int32(sys.Pgid)
 		if pgrp == 0 {
 			r1, err1 = getpid()
 			if err1 != 0 {
 				goto childerror
 			}
 
-			pgrp = int(r1)
+			pgrp = int32(r1)
 		}
 
 		// Place process group in foreground.
diff --git a/src/syscall/mkall.sh b/src/syscall/mkall.sh
index 6a9aacb..dd8226f 100755
--- a/src/syscall/mkall.sh
+++ b/src/syscall/mkall.sh
@@ -274,6 +274,12 @@ solaris_amd64)
 	mksysnum=
 	mktypes="GOARCH=$GOARCH go tool cgo -godefs"
 	;;
+solaris_sparc64)
+	mksyscall="./mksyscall_solaris.pl"
+	mkerrors="$mkerrors -m64"
+	mksysnum=
+	mktypes="GOARCH=$GOARCH go tool cgo -godefs"
+	;;
 windows_*)
 	echo 'run "go generate syscall_windows.go" instead' 1>&2
 	exit 1
diff --git a/src/syscall/mkerrors.sh b/src/syscall/mkerrors.sh
index b59a46b..590cdb8 100755
--- a/src/syscall/mkerrors.sh
+++ b/src/syscall/mkerrors.sh
@@ -19,6 +19,7 @@ if [[ "$GOOS" -eq "solaris" ]]; then
 fi
 
 uname=$(uname)
+UNAME=${UNAME:-$uname}
 
 includes_Darwin='
 #define _DARWIN_C_SOURCE
@@ -241,7 +242,7 @@ ccflags="$@"
 	echo package syscall
 	echo
 	echo '/*'
-	indirect="includes_$(uname)"
+	indirect="includes_$UNAME"
 	echo "${!indirect} $includes"
 	echo '*/'
 	echo 'import "C"'
diff --git a/src/syscall/syscall.go b/src/syscall/syscall.go
index bb102c6..14962fc 100644
--- a/src/syscall/syscall.go
+++ b/src/syscall/syscall.go
@@ -93,6 +93,10 @@ func (tv *Timeval) Nano() int64 {
 	return int64(tv.Sec)*1e9 + int64(tv.Usec)*1000
 }
 
+// Getpagesize is provided by the runtime.
+
+func Getpagesize() int
+
 // use is a no-op, but the compiler cannot see that it is.
 // Calling use(p) ensures that p is kept live until that point.
 // This was needed until Go 1.6 to call syscall.Syscall correctly.
diff --git a/src/syscall/syscall_darwin_386.go b/src/syscall/syscall_darwin_386.go
index f75de00..11a991e 100644
--- a/src/syscall/syscall_darwin_386.go
+++ b/src/syscall/syscall_darwin_386.go
@@ -6,8 +6,6 @@ package syscall
 
 import "unsafe"
 
-func Getpagesize() int { return 4096 }
-
 func TimespecToNsec(ts Timespec) int64 { return int64(ts.Sec)*1e9 + int64(ts.Nsec) }
 
 func NsecToTimespec(nsec int64) (ts Timespec) {
diff --git a/src/syscall/syscall_darwin_amd64.go b/src/syscall/syscall_darwin_amd64.go
index 7908311..d7951e8 100644
--- a/src/syscall/syscall_darwin_amd64.go
+++ b/src/syscall/syscall_darwin_amd64.go
@@ -6,8 +6,6 @@ package syscall
 
 import "unsafe"
 
-func Getpagesize() int { return 4096 }
-
 func TimespecToNsec(ts Timespec) int64 { return int64(ts.Sec)*1e9 + int64(ts.Nsec) }
 
 func NsecToTimespec(nsec int64) (ts Timespec) {
diff --git a/src/syscall/syscall_darwin_arm.go b/src/syscall/syscall_darwin_arm.go
index fe43103..830a2e0 100644
--- a/src/syscall/syscall_darwin_arm.go
+++ b/src/syscall/syscall_darwin_arm.go
@@ -6,8 +6,6 @@ package syscall
 
 import "unsafe"
 
-func Getpagesize() int { return 4096 }
-
 func TimespecToNsec(ts Timespec) int64 { return int64(ts.Sec)*1e9 + int64(ts.Nsec) }
 
 func NsecToTimespec(nsec int64) (ts Timespec) {
diff --git a/src/syscall/syscall_darwin_arm64.go b/src/syscall/syscall_darwin_arm64.go
index d396e25..02316a7 100644
--- a/src/syscall/syscall_darwin_arm64.go
+++ b/src/syscall/syscall_darwin_arm64.go
@@ -6,8 +6,6 @@ package syscall
 
 import "unsafe"
 
-func Getpagesize() int { return 16384 }
-
 func TimespecToNsec(ts Timespec) int64 { return int64(ts.Sec)*1e9 + int64(ts.Nsec) }
 
 func NsecToTimespec(nsec int64) (ts Timespec) {
diff --git a/src/syscall/syscall_dragonfly_amd64.go b/src/syscall/syscall_dragonfly_amd64.go
index 70c2ffb..7cad725 100644
--- a/src/syscall/syscall_dragonfly_amd64.go
+++ b/src/syscall/syscall_dragonfly_amd64.go
@@ -6,8 +6,6 @@ package syscall
 
 import "unsafe"
 
-func Getpagesize() int { return 4096 }
-
 func TimespecToNsec(ts Timespec) int64 { return int64(ts.Sec)*1e9 + int64(ts.Nsec) }
 
 func NsecToTimespec(nsec int64) (ts Timespec) {
diff --git a/src/syscall/syscall_freebsd_386.go b/src/syscall/syscall_freebsd_386.go
index ebd3d4c..7b53dc2 100644
--- a/src/syscall/syscall_freebsd_386.go
+++ b/src/syscall/syscall_freebsd_386.go
@@ -6,8 +6,6 @@ package syscall
 
 import "unsafe"
 
-func Getpagesize() int { return 4096 }
-
 func TimespecToNsec(ts Timespec) int64 { return int64(ts.Sec)*1e9 + int64(ts.Nsec) }
 
 func NsecToTimespec(nsec int64) (ts Timespec) {
diff --git a/src/syscall/syscall_freebsd_amd64.go b/src/syscall/syscall_freebsd_amd64.go
index 70c2ffb..7cad725 100644
--- a/src/syscall/syscall_freebsd_amd64.go
+++ b/src/syscall/syscall_freebsd_amd64.go
@@ -6,8 +6,6 @@ package syscall
 
 import "unsafe"
 
-func Getpagesize() int { return 4096 }
-
 func TimespecToNsec(ts Timespec) int64 { return int64(ts.Sec)*1e9 + int64(ts.Nsec) }
 
 func NsecToTimespec(nsec int64) (ts Timespec) {
diff --git a/src/syscall/syscall_freebsd_arm.go b/src/syscall/syscall_freebsd_arm.go
index ab72871..83fa1d7 100644
--- a/src/syscall/syscall_freebsd_arm.go
+++ b/src/syscall/syscall_freebsd_arm.go
@@ -6,8 +6,6 @@ package syscall
 
 import "unsafe"
 
-func Getpagesize() int { return 4096 }
-
 func TimespecToNsec(ts Timespec) int64 { return ts.Sec*1e9 + int64(ts.Nsec) }
 
 func NsecToTimespec(nsec int64) (ts Timespec) {
diff --git a/src/syscall/syscall_linux_386.go b/src/syscall/syscall_linux_386.go
index d9e0ed5..4f3c7c4 100644
--- a/src/syscall/syscall_linux_386.go
+++ b/src/syscall/syscall_linux_386.go
@@ -14,8 +14,6 @@ const (
 	_SYS_getdents = SYS_GETDENTS64
 )
 
-func Getpagesize() int { return 4096 }
-
 func TimespecToNsec(ts Timespec) int64 { return int64(ts.Sec)*1e9 + int64(ts.Nsec) }
 
 func NsecToTimespec(nsec int64) (ts Timespec) {
diff --git a/src/syscall/syscall_linux_amd64.go b/src/syscall/syscall_linux_amd64.go
index d1bda29..119204b 100644
--- a/src/syscall/syscall_linux_amd64.go
+++ b/src/syscall/syscall_linux_amd64.go
@@ -72,8 +72,6 @@ func Gettimeofday(tv *Timeval) (err error) {
 	return nil
 }
 
-func Getpagesize() int { return 4096 }
-
 func Time(t *Time_t) (tt Time_t, err error) {
 	var tv Timeval
 	errno := gettimeofday(&tv)
diff --git a/src/syscall/syscall_linux_arm.go b/src/syscall/syscall_linux_arm.go
index 7c78254..b38b7e5 100644
--- a/src/syscall/syscall_linux_arm.go
+++ b/src/syscall/syscall_linux_arm.go
@@ -11,8 +11,6 @@ const (
 	_SYS_getdents = SYS_GETDENTS64
 )
 
-func Getpagesize() int { return 4096 }
-
 func TimespecToNsec(ts Timespec) int64 { return int64(ts.Sec)*1e9 + int64(ts.Nsec) }
 
 func NsecToTimespec(nsec int64) (ts Timespec) {
diff --git a/src/syscall/syscall_linux_arm64.go b/src/syscall/syscall_linux_arm64.go
index 5f1478c..98681d3 100644
--- a/src/syscall/syscall_linux_arm64.go
+++ b/src/syscall/syscall_linux_arm64.go
@@ -68,8 +68,6 @@ func Lstat(path string, stat *Stat_t) (err error) {
 //sys	sendmsg(s int, msg *Msghdr, flags int) (n int, err error)
 //sys	mmap(addr uintptr, length uintptr, prot int, flags int, fd int, offset int64) (xaddr uintptr, err error)
 
-func Getpagesize() int { return 65536 }
-
 //sysnb	Gettimeofday(tv *Timeval) (err error)
 //sysnb	Time(t *Time_t) (tt Time_t, err error)
 
diff --git a/src/syscall/syscall_linux_mips64x.go b/src/syscall/syscall_linux_mips64x.go
index a14485a..a1331a8 100644
--- a/src/syscall/syscall_linux_mips64x.go
+++ b/src/syscall/syscall_linux_mips64x.go
@@ -65,8 +65,6 @@ const (
 //sys	sendmsg(s int, msg *Msghdr, flags int) (n int, err error)
 //sys	mmap(addr uintptr, length uintptr, prot int, flags int, fd int, offset int64) (xaddr uintptr, err error)
 
-func Getpagesize() int { return 65536 }
-
 //sysnb	Gettimeofday(tv *Timeval) (err error)
 
 func Time(t *Time_t) (tt Time_t, err error) {
diff --git a/src/syscall/syscall_linux_ppc64x.go b/src/syscall/syscall_linux_ppc64x.go
index 9f1c07e..e931ee4 100644
--- a/src/syscall/syscall_linux_ppc64x.go
+++ b/src/syscall/syscall_linux_ppc64x.go
@@ -64,8 +64,6 @@ const (
 //sys	sendmsg(s int, msg *Msghdr, flags int) (n int, err error)
 //sys	mmap(addr uintptr, length uintptr, prot int, flags int, fd int, offset int64) (xaddr uintptr, err error)
 
-func Getpagesize() int { return 65536 }
-
 //sysnb	Gettimeofday(tv *Timeval) (err error)
 //sysnb	Time(t *Time_t) (tt Time_t, err error)
 
diff --git a/src/syscall/syscall_linux_s390x.go b/src/syscall/syscall_linux_s390x.go
index d74277a..b67d7aa 100644
--- a/src/syscall/syscall_linux_s390x.go
+++ b/src/syscall/syscall_linux_s390x.go
@@ -44,8 +44,6 @@ const (
 //sysnb	getgroups(n int, list *_Gid_t) (nn int, err error)
 //sysnb	setgroups(n int, list *_Gid_t) (err error)
 
-func Getpagesize() int { return 4096 }
-
 //sysnb	Gettimeofday(tv *Timeval) (err error)
 
 func Time(t *Time_t) (tt Time_t, err error) {
diff --git a/src/syscall/syscall_nacl.go b/src/syscall/syscall_nacl.go
index ba6eafe..1bddb6b 100644
--- a/src/syscall/syscall_nacl.go
+++ b/src/syscall/syscall_nacl.go
@@ -292,7 +292,6 @@ func Getegid() int                      { return 1 }
 func Geteuid() int                      { return 1 }
 func Getgid() int                       { return 1 }
 func Getgroups() ([]int, error)         { return []int{1}, nil }
-func Getpagesize() int                  { return 65536 }
 func Getppid() int                      { return 2 }
 func Getpid() int                       { return 3 }
 func Getuid() int                       { return 1 }
diff --git a/src/syscall/syscall_netbsd_386.go b/src/syscall/syscall_netbsd_386.go
index 2dbff07..71c639c 100644
--- a/src/syscall/syscall_netbsd_386.go
+++ b/src/syscall/syscall_netbsd_386.go
@@ -4,8 +4,6 @@
 
 package syscall
 
-func Getpagesize() int { return 4096 }
-
 func TimespecToNsec(ts Timespec) int64 { return int64(ts.Sec)*1e9 + int64(ts.Nsec) }
 
 func NsecToTimespec(nsec int64) (ts Timespec) {
diff --git a/src/syscall/syscall_netbsd_amd64.go b/src/syscall/syscall_netbsd_amd64.go
index 5784db9..4762da3 100644
--- a/src/syscall/syscall_netbsd_amd64.go
+++ b/src/syscall/syscall_netbsd_amd64.go
@@ -4,8 +4,6 @@
 
 package syscall
 
-func Getpagesize() int { return 4096 }
-
 func TimespecToNsec(ts Timespec) int64 { return int64(ts.Sec)*1e9 + int64(ts.Nsec) }
 
 func NsecToTimespec(nsec int64) (ts Timespec) {
diff --git a/src/syscall/syscall_netbsd_arm.go b/src/syscall/syscall_netbsd_arm.go
index 659698a..0160252 100644
--- a/src/syscall/syscall_netbsd_arm.go
+++ b/src/syscall/syscall_netbsd_arm.go
@@ -4,8 +4,6 @@
 
 package syscall
 
-func Getpagesize() int { return 4096 }
-
 func TimespecToNsec(ts Timespec) int64 { return int64(ts.Sec)*1e9 + int64(ts.Nsec) }
 
 func NsecToTimespec(nsec int64) (ts Timespec) {
diff --git a/src/syscall/syscall_openbsd_386.go b/src/syscall/syscall_openbsd_386.go
index ad5ae14..c836a6f 100644
--- a/src/syscall/syscall_openbsd_386.go
+++ b/src/syscall/syscall_openbsd_386.go
@@ -4,8 +4,6 @@
 
 package syscall
 
-func Getpagesize() int { return 4096 }
-
 func TimespecToNsec(ts Timespec) int64 { return int64(ts.Sec)*1e9 + int64(ts.Nsec) }
 
 func NsecToTimespec(nsec int64) (ts Timespec) {
diff --git a/src/syscall/syscall_openbsd_amd64.go b/src/syscall/syscall_openbsd_amd64.go
index 6181344..ca8e7c0 100644
--- a/src/syscall/syscall_openbsd_amd64.go
+++ b/src/syscall/syscall_openbsd_amd64.go
@@ -4,8 +4,6 @@
 
 package syscall
 
-func Getpagesize() int { return 4096 }
-
 func TimespecToNsec(ts Timespec) int64 { return int64(ts.Sec)*1e9 + int64(ts.Nsec) }
 
 func NsecToTimespec(nsec int64) (ts Timespec) {
diff --git a/src/syscall/syscall_openbsd_arm.go b/src/syscall/syscall_openbsd_arm.go
index ad5ae14..c836a6f 100644
--- a/src/syscall/syscall_openbsd_arm.go
+++ b/src/syscall/syscall_openbsd_arm.go
@@ -4,8 +4,6 @@
 
 package syscall
 
-func Getpagesize() int { return 4096 }
-
 func TimespecToNsec(ts Timespec) int64 { return int64(ts.Sec)*1e9 + int64(ts.Nsec) }
 
 func NsecToTimespec(nsec int64) (ts Timespec) {
diff --git a/src/syscall/syscall_plan9.go b/src/syscall/syscall_plan9.go
index b511867..0691889 100644
--- a/src/syscall/syscall_plan9.go
+++ b/src/syscall/syscall_plan9.go
@@ -305,8 +305,6 @@ func Gettimeofday(tv *Timeval) error {
 	return nil
 }
 
-func Getpagesize() int { return 0x1000 }
-
 func Getegid() (egid int) { return -1 }
 func Geteuid() (euid int) { return -1 }
 func Getgid() (gid int)   { return -1 }
diff --git a/src/syscall/syscall_solaris_amd64.go b/src/syscall/syscall_solaris_amd64.go
index 67b8af1..5f918d3 100644
--- a/src/syscall/syscall_solaris_amd64.go
+++ b/src/syscall/syscall_solaris_amd64.go
@@ -4,8 +4,6 @@
 
 package syscall
 
-func Getpagesize() int { return 4096 }
-
 func TimespecToNsec(ts Timespec) int64 { return int64(ts.Sec)*1e9 + int64(ts.Nsec) }
 
 func NsecToTimespec(nsec int64) (ts Timespec) {
diff --git a/src/syscall/syscall_solaris_sparc64.go b/src/syscall/syscall_solaris_sparc64.go
new file mode 100644
index 0000000..0a59b0f
--- /dev/null
+++ b/src/syscall/syscall_solaris_sparc64.go
@@ -0,0 +1,30 @@
+// Copyright 2016 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package syscall
+
+func TimespecToNsec(ts Timespec) int64 { return int64(ts.Sec)*1e9 + int64(ts.Nsec) }
+
+func NsecToTimespec(nsec int64) (ts Timespec) {
+	ts.Sec = nsec / 1e9
+	ts.Nsec = nsec % 1e9
+	return
+}
+
+func TimevalToNsec(tv Timeval) int64 { return int64(tv.Sec)*1e9 + int64(tv.Usec)*1e3 }
+
+func NsecToTimeval(nsec int64) (tv Timeval) {
+	nsec += 999 // round up to microsecond
+	tv.Usec = nsec % 1e9 / 1e3
+	tv.Sec = int64(nsec / 1e9)
+	return
+}
+
+func (iov *Iovec) SetLen(length int) {
+	iov.Len = uint64(length)
+}
+
+func (cmsg *Cmsghdr) SetLen(length int) {
+	cmsg.Len = uint32(length)
+}
diff --git a/src/syscall/syscall_windows.go b/src/syscall/syscall_windows.go
index 703bb53..5569a77 100644
--- a/src/syscall/syscall_windows.go
+++ b/src/syscall/syscall_windows.go
@@ -76,8 +76,6 @@ func UTF16PtrFromString(s string) (*uint16, error) {
 	return &a[0], nil
 }
 
-func Getpagesize() int { return 4096 }
-
 // Errno is the Windows error number.
 type Errno uintptr
 
diff --git a/src/syscall/zerrors_solaris_sparc64.go b/src/syscall/zerrors_solaris_sparc64.go
new file mode 100644
index 0000000..99d2766
--- /dev/null
+++ b/src/syscall/zerrors_solaris_sparc64.go
@@ -0,0 +1,1447 @@
+// mkerrors.sh -m64
+// MACHINE GENERATED BY THE COMMAND ABOVE; DO NOT EDIT
+
+// Created by cgo -godefs - DO NOT EDIT
+// cgo -godefs -- -m64 _const.go
+
+package syscall
+
+const (
+	AF_802                        = 0x12
+	AF_APPLETALK                  = 0x10
+	AF_CCITT                      = 0xa
+	AF_CHAOS                      = 0x5
+	AF_DATAKIT                    = 0x9
+	AF_DECnet                     = 0xc
+	AF_DLI                        = 0xd
+	AF_ECMA                       = 0x8
+	AF_FILE                       = 0x1
+	AF_GOSIP                      = 0x16
+	AF_HYLINK                     = 0xf
+	AF_IMPLINK                    = 0x3
+	AF_INET                       = 0x2
+	AF_INET6                      = 0x1a
+	AF_INET_OFFLOAD               = 0x1e
+	AF_INET_SDP                   = 0x21
+	AF_IPX                        = 0x17
+	AF_KEY                        = 0x1b
+	AF_LAT                        = 0xe
+	AF_LINK                       = 0x19
+	AF_LOCAL                      = 0x1
+	AF_MAX                        = 0x21
+	AF_NBS                        = 0x7
+	AF_NIT                        = 0x11
+	AF_NS                         = 0x6
+	AF_OSI                        = 0x13
+	AF_OSINET                     = 0x15
+	AF_PACKET                     = 0x20
+	AF_POLICY                     = 0x1d
+	AF_PUP                        = 0x4
+	AF_ROUTE                      = 0x18
+	AF_SNA                        = 0xb
+	AF_TRILL                      = 0x1f
+	AF_UNIX                       = 0x1
+	AF_UNSPEC                     = 0x0
+	AF_X25                        = 0x14
+	ARPHRD_ARCNET                 = 0x7
+	ARPHRD_ATM                    = 0x10
+	ARPHRD_AX25                   = 0x3
+	ARPHRD_CHAOS                  = 0x5
+	ARPHRD_EETHER                 = 0x2
+	ARPHRD_ETHER                  = 0x1
+	ARPHRD_FC                     = 0x12
+	ARPHRD_FRAME                  = 0xf
+	ARPHRD_HDLC                   = 0x11
+	ARPHRD_IB                     = 0x20
+	ARPHRD_IEEE802                = 0x6
+	ARPHRD_IPATM                  = 0x13
+	ARPHRD_METRICOM               = 0x17
+	ARPHRD_TUNNEL                 = 0x1f
+	B0                            = 0x0
+	B110                          = 0x3
+	B115200                       = 0x12
+	B1200                         = 0x9
+	B134                          = 0x4
+	B150                          = 0x5
+	B153600                       = 0x13
+	B1800                         = 0xa
+	B19200                        = 0xe
+	B200                          = 0x6
+	B230400                       = 0x14
+	B2400                         = 0xb
+	B300                          = 0x7
+	B307200                       = 0x15
+	B38400                        = 0xf
+	B460800                       = 0x16
+	B4800                         = 0xc
+	B50                           = 0x1
+	B57600                        = 0x10
+	B600                          = 0x8
+	B75                           = 0x2
+	B76800                        = 0x11
+	B921600                       = 0x17
+	B9600                         = 0xd
+	BIOCFLUSH                     = 0x20004268
+	BIOCGBLEN                     = 0x40044266
+	BIOCGDLT                      = 0x4004426a
+	BIOCGDLTLIST                  = -0x3fefbd89
+	BIOCGDLTLIST32                = -0x3ff7bd89
+	BIOCGETIF                     = 0x4020426b
+	BIOCGETLIF                    = 0x4078426b
+	BIOCGHDRCMPLT                 = 0x40044274
+	BIOCGRTIMEOUT                 = 0x4010427b
+	BIOCGRTIMEOUT32               = 0x4008427b
+	BIOCGSEESENT                  = 0x40044278
+	BIOCGSTATS                    = 0x4080426f
+	BIOCGSTATSOLD                 = 0x4008426f
+	BIOCIMMEDIATE                 = -0x7ffbbd90
+	BIOCPROMISC                   = 0x20004269
+	BIOCSBLEN                     = -0x3ffbbd9a
+	BIOCSDLT                      = -0x7ffbbd8a
+	BIOCSETF                      = -0x7fefbd99
+	BIOCSETF32                    = -0x7ff7bd99
+	BIOCSETIF                     = -0x7fdfbd94
+	BIOCSETLIF                    = -0x7f87bd94
+	BIOCSHDRCMPLT                 = -0x7ffbbd8b
+	BIOCSRTIMEOUT                 = -0x7fefbd86
+	BIOCSRTIMEOUT32               = -0x7ff7bd86
+	BIOCSSEESENT                  = -0x7ffbbd87
+	BIOCSTCPF                     = -0x7fefbd8e
+	BIOCSUDPF                     = -0x7fefbd8d
+	BIOCVERSION                   = 0x40044271
+	BPF_A                         = 0x10
+	BPF_ABS                       = 0x20
+	BPF_ADD                       = 0x0
+	BPF_ALIGNMENT                 = 0x4
+	BPF_ALU                       = 0x4
+	BPF_AND                       = 0x50
+	BPF_B                         = 0x10
+	BPF_BUFSIZE                   = 0x8000
+	BPF_DFLTBUFSIZE               = 0x100000
+	BPF_DFLTMAXBUFSIZE            = 0x1000000
+	BPF_DIV                       = 0x30
+	BPF_H                         = 0x8
+	BPF_IMM                       = 0x0
+	BPF_IND                       = 0x40
+	BPF_JA                        = 0x0
+	BPF_JEQ                       = 0x10
+	BPF_JGE                       = 0x30
+	BPF_JGT                       = 0x20
+	BPF_JMP                       = 0x5
+	BPF_JSET                      = 0x40
+	BPF_K                         = 0x0
+	BPF_LD                        = 0x0
+	BPF_LDX                       = 0x1
+	BPF_LEN                       = 0x80
+	BPF_LSH                       = 0x60
+	BPF_MAJOR_VERSION             = 0x1
+	BPF_MAXBUFSIZE                = 0x7fffffff
+	BPF_MAXINSNS                  = 0x200
+	BPF_MEM                       = 0x60
+	BPF_MEMWORDS                  = 0x10
+	BPF_MINBUFSIZE                = 0x20
+	BPF_MINOR_VERSION             = 0x1
+	BPF_MISC                      = 0x7
+	BPF_MSH                       = 0xa0
+	BPF_MUL                       = 0x20
+	BPF_NEG                       = 0x80
+	BPF_OR                        = 0x40
+	BPF_RELEASE                   = 0x30bb6
+	BPF_RET                       = 0x6
+	BPF_RSH                       = 0x70
+	BPF_ST                        = 0x2
+	BPF_STX                       = 0x3
+	BPF_SUB                       = 0x10
+	BPF_TAX                       = 0x0
+	BPF_TXA                       = 0x80
+	BPF_W                         = 0x0
+	BPF_X                         = 0x8
+	BRKINT                        = 0x2
+	CFLUSH                        = 0xf
+	CLOCAL                        = 0x800
+	CREAD                         = 0x80
+	CS5                           = 0x0
+	CS6                           = 0x10
+	CS7                           = 0x20
+	CS8                           = 0x30
+	CSIZE                         = 0x30
+	CSTART                        = 0x11
+	CSTOP                         = 0x13
+	CSTOPB                        = 0x40
+	CSUSP                         = 0x1a
+	CSWTCH                        = 0x1a
+	DLT_AIRONET_HEADER            = 0x78
+	DLT_APPLE_IP_OVER_IEEE1394    = 0x8a
+	DLT_ARCNET                    = 0x7
+	DLT_ARCNET_LINUX              = 0x81
+	DLT_ATM_CLIP                  = 0x13
+	DLT_ATM_RFC1483               = 0xb
+	DLT_AURORA                    = 0x7e
+	DLT_AX25                      = 0x3
+	DLT_BACNET_MS_TP              = 0xa5
+	DLT_CHAOS                     = 0x5
+	DLT_CISCO_IOS                 = 0x76
+	DLT_C_HDLC                    = 0x68
+	DLT_DOCSIS                    = 0x8f
+	DLT_ECONET                    = 0x73
+	DLT_EN10MB                    = 0x1
+	DLT_EN3MB                     = 0x2
+	DLT_ENC                       = 0x6d
+	DLT_ERF_ETH                   = 0xaf
+	DLT_ERF_POS                   = 0xb0
+	DLT_FDDI                      = 0xa
+	DLT_FRELAY                    = 0x6b
+	DLT_GCOM_SERIAL               = 0xad
+	DLT_GCOM_T1E1                 = 0xac
+	DLT_GPF_F                     = 0xab
+	DLT_GPF_T                     = 0xaa
+	DLT_GPRS_LLC                  = 0xa9
+	DLT_HDLC                      = 0x10
+	DLT_HHDLC                     = 0x79
+	DLT_HIPPI                     = 0xf
+	DLT_IBM_SN                    = 0x92
+	DLT_IBM_SP                    = 0x91
+	DLT_IEEE802                   = 0x6
+	DLT_IEEE802_11                = 0x69
+	DLT_IEEE802_11_RADIO          = 0x7f
+	DLT_IEEE802_11_RADIO_AVS      = 0xa3
+	DLT_IPNET                     = 0xe2
+	DLT_IPOIB                     = 0xa2
+	DLT_IP_OVER_FC                = 0x7a
+	DLT_JUNIPER_ATM1              = 0x89
+	DLT_JUNIPER_ATM2              = 0x87
+	DLT_JUNIPER_CHDLC             = 0xb5
+	DLT_JUNIPER_ES                = 0x84
+	DLT_JUNIPER_ETHER             = 0xb2
+	DLT_JUNIPER_FRELAY            = 0xb4
+	DLT_JUNIPER_GGSN              = 0x85
+	DLT_JUNIPER_MFR               = 0x86
+	DLT_JUNIPER_MLFR              = 0x83
+	DLT_JUNIPER_MLPPP             = 0x82
+	DLT_JUNIPER_MONITOR           = 0xa4
+	DLT_JUNIPER_PIC_PEER          = 0xae
+	DLT_JUNIPER_PPP               = 0xb3
+	DLT_JUNIPER_PPPOE             = 0xa7
+	DLT_JUNIPER_PPPOE_ATM         = 0xa8
+	DLT_JUNIPER_SERVICES          = 0x88
+	DLT_LINUX_IRDA                = 0x90
+	DLT_LINUX_LAPD                = 0xb1
+	DLT_LINUX_SLL                 = 0x71
+	DLT_LOOP                      = 0x6c
+	DLT_LTALK                     = 0x72
+	DLT_MTP2                      = 0x8c
+	DLT_MTP2_WITH_PHDR            = 0x8b
+	DLT_MTP3                      = 0x8d
+	DLT_NULL                      = 0x0
+	DLT_PCI_EXP                   = 0x7d
+	DLT_PFLOG                     = 0x75
+	DLT_PFSYNC                    = 0x12
+	DLT_PPP                       = 0x9
+	DLT_PPP_BSDOS                 = 0xe
+	DLT_PPP_PPPD                  = 0xa6
+	DLT_PRISM_HEADER              = 0x77
+	DLT_PRONET                    = 0x4
+	DLT_RAW                       = 0xc
+	DLT_RAWAF_MASK                = 0x2240000
+	DLT_RIO                       = 0x7c
+	DLT_SCCP                      = 0x8e
+	DLT_SLIP                      = 0x8
+	DLT_SLIP_BSDOS                = 0xd
+	DLT_SUNATM                    = 0x7b
+	DLT_SYMANTEC_FIREWALL         = 0x63
+	DLT_TZSP                      = 0x80
+	ECHO                          = 0x8
+	ECHOCTL                       = 0x200
+	ECHOE                         = 0x10
+	ECHOK                         = 0x20
+	ECHOKE                        = 0x800
+	ECHONL                        = 0x40
+	ECHOPRT                       = 0x400
+	EMPTY_SET                     = 0x0
+	EMT_CPCOVF                    = 0x2
+	EMT_TAGOVF                    = 0x1
+	EQUALITY_CHECK                = 0x0
+	EXTA                          = 0xe
+	EXTB                          = 0xf
+	FD_CLOEXEC                    = 0x1
+	FD_CLOFORK                    = 0x2
+	FD_NFDBITS                    = 0x40
+	FD_SETSIZE                    = 0x10000
+	FLUSHALL                      = 0x1
+	FLUSHDATA                     = 0x0
+	FLUSHO                        = 0x2000
+	F_ALLOCSP                     = 0xa
+	F_ALLOCSP64                   = 0xa
+	F_BADFD                       = 0x2e
+	F_BLKSIZE                     = 0x13
+	F_BLOCKS                      = 0x12
+	F_CHKFL                       = 0x8
+	F_COMPAT                      = 0x8
+	F_DUP2FD                      = 0x9
+	F_DUP2FD_CLOEXEC              = 0x30
+	F_DUP2FD_CLOFORK              = 0x32
+	F_DUPFD                       = 0x0
+	F_DUPFD_CLOEXEC               = 0x2f
+	F_DUPFD_CLOFORK               = 0x31
+	F_FREESP                      = 0xb
+	F_FREESP64                    = 0xb
+	F_GETFD                       = 0x1
+	F_GETFL                       = 0x3
+	F_GETLK                       = 0xe
+	F_GETLK64                     = 0xe
+	F_GETOWN                      = 0x17
+	F_GETXFL                      = 0x2d
+	F_HASREMOTELOCKS              = 0x1a
+	F_ISSTREAM                    = 0xd
+	F_MANDDNY                     = 0x10
+	F_MDACC                       = 0x20
+	F_NODNY                       = 0x0
+	F_NPRIV                       = 0x10
+	F_PRIV                        = 0xf
+	F_QUOTACTL                    = 0x11
+	F_RDACC                       = 0x1
+	F_RDDNY                       = 0x1
+	F_RDLCK                       = 0x1
+	F_REVOKE                      = 0x19
+	F_RMACC                       = 0x4
+	F_RMDNY                       = 0x4
+	F_RWACC                       = 0x3
+	F_RWDNY                       = 0x3
+	F_SETFD                       = 0x2
+	F_SETFL                       = 0x4
+	F_SETLK                       = 0x6
+	F_SETLK64                     = 0x6
+	F_SETLK64_NBMAND              = 0x2a
+	F_SETLKW                      = 0x7
+	F_SETLKW64                    = 0x7
+	F_SETLK_NBMAND                = 0x2a
+	F_SETOWN                      = 0x18
+	F_SHARE                       = 0x28
+	F_SHARE_NBMAND                = 0x2b
+	F_UNLCK                       = 0x3
+	F_UNLKSYS                     = 0x4
+	F_UNSHARE                     = 0x29
+	F_WRACC                       = 0x2
+	F_WRDNY                       = 0x2
+	F_WRLCK                       = 0x2
+	HUPCL                         = 0x400
+	ICANON                        = 0x2
+	ICRNL                         = 0x100
+	IEXTEN                        = 0x8000
+	IFF_ADDRCONF                  = 0x80000
+	IFF_ALLMULTI                  = 0x200
+	IFF_ANYCAST                   = 0x400000
+	IFF_BROADCAST                 = 0x2
+	IFF_CANTCHANGE                = 0x36f203003b5a
+	IFF_COS_ENABLED               = 0x200000000
+	IFF_DEBUG                     = 0x4
+	IFF_DEPRECATED                = 0x40000
+	IFF_DHCPRUNNING               = 0x4000
+	IFF_DUPLICATE                 = 0x4000000000
+	IFF_FAILED                    = 0x10000000
+	IFF_FIXEDMTU                  = 0x1000000000
+	IFF_INACTIVE                  = 0x40000000
+	IFF_INTELLIGENT               = 0x400
+	IFF_IPMP                      = 0x8000000000
+	IFF_IPMP_CANTCHANGE           = 0x10000000
+	IFF_IPMP_INVALID              = 0x1e8200080
+	IFF_IPV4                      = 0x1000000
+	IFF_IPV6                      = 0x2000000
+	IFF_L3PROTECT                 = 0x40000000000
+	IFF_LOOPBACK                  = 0x8
+	IFF_MULTICAST                 = 0x800
+	IFF_MULTI_BCAST               = 0x1000
+	IFF_NOACCEPT                  = 0x4000000
+	IFF_NOARP                     = 0x80
+	IFF_NOFAILOVER                = 0x8000000
+	IFF_NOLINKLOCAL               = 0x20000000000
+	IFF_NOLOCAL                   = 0x20000
+	IFF_NONUD                     = 0x200000
+	IFF_NORTEXCH                  = 0x800000
+	IFF_NOTRAILERS                = 0x20
+	IFF_NOXMIT                    = 0x10000
+	IFF_OFFLINE                   = 0x80000000
+	IFF_PHYSRUNNING               = 0x100000000000
+	IFF_POINTOPOINT               = 0x10
+	IFF_PREFERRED                 = 0x400000000
+	IFF_PRIVATE                   = 0x8000
+	IFF_PROBER                    = 0x80000000000
+	IFF_PROMISC                   = 0x100
+	IFF_ROUTER                    = 0x100000
+	IFF_RUNNING                   = 0x40
+	IFF_STANDBY                   = 0x20000000
+	IFF_TEMPORARY                 = 0x800000000
+	IFF_UNNUMBERED                = 0x2000
+	IFF_UP                        = 0x1
+	IFF_VIRTUAL                   = 0x2000000000
+	IFF_VNI                       = 0x200000000000
+	IFF_VRRP                      = 0x10000000000
+	IFF_XRESOLV                   = 0x100000000
+	IFNAMSIZ                      = 0x10
+	IFT_1822                      = 0x2
+	IFT_6TO4                      = 0xca
+	IFT_AAL5                      = 0x31
+	IFT_ARCNET                    = 0x23
+	IFT_ARCNETPLUS                = 0x24
+	IFT_ATM                       = 0x25
+	IFT_CEPT                      = 0x13
+	IFT_DS3                       = 0x1e
+	IFT_EON                       = 0x19
+	IFT_ETHER                     = 0x6
+	IFT_FDDI                      = 0xf
+	IFT_FRELAY                    = 0x20
+	IFT_FRELAYDCE                 = 0x2c
+	IFT_HDH1822                   = 0x3
+	IFT_HIPPI                     = 0x2f
+	IFT_HSSI                      = 0x2e
+	IFT_HY                        = 0xe
+	IFT_IB                        = 0xc7
+	IFT_IPV4                      = 0xc8
+	IFT_IPV6                      = 0xc9
+	IFT_ISDNBASIC                 = 0x14
+	IFT_ISDNPRIMARY               = 0x15
+	IFT_ISO88022LLC               = 0x29
+	IFT_ISO88023                  = 0x7
+	IFT_ISO88024                  = 0x8
+	IFT_ISO88025                  = 0x9
+	IFT_ISO88026                  = 0xa
+	IFT_LAPB                      = 0x10
+	IFT_LOCALTALK                 = 0x2a
+	IFT_LOOP                      = 0x18
+	IFT_MIOX25                    = 0x26
+	IFT_MODEM                     = 0x30
+	IFT_NSIP                      = 0x1b
+	IFT_OTHER                     = 0x1
+	IFT_P10                       = 0xc
+	IFT_P80                       = 0xd
+	IFT_PARA                      = 0x22
+	IFT_PPP                       = 0x17
+	IFT_PROPMUX                   = 0x36
+	IFT_PROPVIRTUAL               = 0x35
+	IFT_PTPSERIAL                 = 0x16
+	IFT_RS232                     = 0x21
+	IFT_SDLC                      = 0x11
+	IFT_SIP                       = 0x1f
+	IFT_SLIP                      = 0x1c
+	IFT_SMDSDXI                   = 0x2b
+	IFT_SMDSICIP                  = 0x34
+	IFT_SONET                     = 0x27
+	IFT_SONETPATH                 = 0x32
+	IFT_SONETVT                   = 0x33
+	IFT_STARLAN                   = 0xb
+	IFT_T1                        = 0x12
+	IFT_ULTRA                     = 0x1d
+	IFT_V35                       = 0x2d
+	IFT_X25                       = 0x5
+	IFT_X25DDN                    = 0x4
+	IFT_X25PLE                    = 0x28
+	IFT_XETHER                    = 0x1a
+	IGNBRK                        = 0x1
+	IGNCR                         = 0x80
+	IGNPAR                        = 0x4
+	IMAXBEL                       = 0x2000
+	INLCR                         = 0x40
+	INPCK                         = 0x10
+	IN_AUTOCONF_MASK              = 0xffff0000
+	IN_AUTOCONF_NET               = 0xa9fe0000
+	IN_CLASSA_HOST                = 0xffffff
+	IN_CLASSA_MAX                 = 0x80
+	IN_CLASSA_NET                 = 0xff000000
+	IN_CLASSA_NSHIFT              = 0x18
+	IN_CLASSB_HOST                = 0xffff
+	IN_CLASSB_MAX                 = 0x10000
+	IN_CLASSB_NET                 = 0xffff0000
+	IN_CLASSB_NSHIFT              = 0x10
+	IN_CLASSC_HOST                = 0xff
+	IN_CLASSC_NET                 = 0xffffff00
+	IN_CLASSC_NSHIFT              = 0x8
+	IN_CLASSD_HOST                = 0xfffffff
+	IN_CLASSD_NET                 = 0xf0000000
+	IN_CLASSD_NSHIFT              = 0x1c
+	IN_CLASSE_NET                 = 0xffffffff
+	IN_LOOPBACKNET                = 0x7f
+	IN_PRIVATE12_MASK             = 0xfff00000
+	IN_PRIVATE12_NET              = 0xac100000
+	IN_PRIVATE16_MASK             = 0xffff0000
+	IN_PRIVATE16_NET              = 0xc0a80000
+	IN_PRIVATE8_MASK              = 0xff000000
+	IN_PRIVATE8_NET               = 0xa000000
+	IPPROTO_AH                    = 0x33
+	IPPROTO_DSTOPTS               = 0x3c
+	IPPROTO_EGP                   = 0x8
+	IPPROTO_ENCAP                 = 0x4
+	IPPROTO_EON                   = 0x50
+	IPPROTO_ESP                   = 0x32
+	IPPROTO_FRAGMENT              = 0x2c
+	IPPROTO_GGP                   = 0x3
+	IPPROTO_HELLO                 = 0x3f
+	IPPROTO_HOPOPTS               = 0x0
+	IPPROTO_ICMP                  = 0x1
+	IPPROTO_ICMPV6                = 0x3a
+	IPPROTO_IDP                   = 0x16
+	IPPROTO_IGMP                  = 0x2
+	IPPROTO_IP                    = 0x0
+	IPPROTO_IPV6                  = 0x29
+	IPPROTO_MAX                   = 0x100
+	IPPROTO_ND                    = 0x4d
+	IPPROTO_NONE                  = 0x3b
+	IPPROTO_OSPF                  = 0x59
+	IPPROTO_PIM                   = 0x67
+	IPPROTO_PUP                   = 0xc
+	IPPROTO_RAW                   = 0xff
+	IPPROTO_ROUTING               = 0x2b
+	IPPROTO_RSVP                  = 0x2e
+	IPPROTO_SCTP                  = 0x84
+	IPPROTO_TCP                   = 0x6
+	IPPROTO_UDP                   = 0x11
+	IPV6_ADD_MEMBERSHIP           = 0x9
+	IPV6_BOUND_IF                 = 0x41
+	IPV6_CHECKSUM                 = 0x18
+	IPV6_DONTFRAG                 = 0x21
+	IPV6_DROP_MEMBERSHIP          = 0xa
+	IPV6_DSTOPTS                  = 0xf
+	IPV6_HOPLIMIT                 = 0xc
+	IPV6_HOPOPTS                  = 0xe
+	IPV6_JOIN_GROUP               = 0x9
+	IPV6_LEAVE_GROUP              = 0xa
+	IPV6_MULTICAST_HOPS           = 0x7
+	IPV6_MULTICAST_IF             = 0x6
+	IPV6_MULTICAST_LOOP           = 0x8
+	IPV6_NEXTHOP                  = 0xd
+	IPV6_PAD1_OPT                 = 0x0
+	IPV6_PATHMTU                  = 0x25
+	IPV6_PKTINFO                  = 0xb
+	IPV6_PREFER_SRC_CGA           = 0x20
+	IPV6_PREFER_SRC_CGADEFAULT    = 0x10
+	IPV6_PREFER_SRC_CGAMASK       = 0x30
+	IPV6_PREFER_SRC_COA           = 0x2
+	IPV6_PREFER_SRC_DEFAULT       = 0x15
+	IPV6_PREFER_SRC_HOME          = 0x1
+	IPV6_PREFER_SRC_MASK          = 0x3f
+	IPV6_PREFER_SRC_MIPDEFAULT    = 0x1
+	IPV6_PREFER_SRC_MIPMASK       = 0x3
+	IPV6_PREFER_SRC_NONCGA        = 0x10
+	IPV6_PREFER_SRC_PUBLIC        = 0x4
+	IPV6_PREFER_SRC_TMP           = 0x8
+	IPV6_PREFER_SRC_TMPDEFAULT    = 0x4
+	IPV6_PREFER_SRC_TMPMASK       = 0xc
+	IPV6_RECVDSTOPTS              = 0x28
+	IPV6_RECVHOPLIMIT             = 0x13
+	IPV6_RECVHOPOPTS              = 0x14
+	IPV6_RECVPATHMTU              = 0x24
+	IPV6_RECVPKTINFO              = 0x12
+	IPV6_RECVRTHDR                = 0x16
+	IPV6_RECVRTHDRDSTOPTS         = 0x17
+	IPV6_RECVTCLASS               = 0x19
+	IPV6_RTHDR                    = 0x10
+	IPV6_RTHDRDSTOPTS             = 0x11
+	IPV6_RTHDR_TYPE_0             = 0x0
+	IPV6_SEC_OPT                  = 0x22
+	IPV6_SRC_PREFERENCES          = 0x23
+	IPV6_TCLASS                   = 0x26
+	IPV6_UNICAST_HOPS             = 0x5
+	IPV6_UNSPEC_SRC               = 0x42
+	IPV6_USE_MIN_MTU              = 0x20
+	IPV6_V6ONLY                   = 0x27
+	IP_ADD_MEMBERSHIP             = 0x13
+	IP_ADD_SOURCE_MEMBERSHIP      = 0x17
+	IP_BLOCK_SOURCE               = 0x15
+	IP_BOUND_IF                   = 0x41
+	IP_BROADCAST                  = 0x106
+	IP_BROADCAST_TTL              = 0x43
+	IP_DEFAULT_MULTICAST_LOOP     = 0x1
+	IP_DEFAULT_MULTICAST_TTL      = 0x1
+	IP_DF                         = 0x4000
+	IP_DHCPINIT_IF                = 0x45
+	IP_DONTFRAG                   = 0x1b
+	IP_DONTROUTE                  = 0x105
+	IP_DROP_MEMBERSHIP            = 0x14
+	IP_DROP_SOURCE_MEMBERSHIP     = 0x18
+	IP_HDRINCL                    = 0x2
+	IP_MAXPACKET                  = 0xffff
+	IP_MF                         = 0x2000
+	IP_MSS                        = 0x240
+	IP_MULTICAST_IF               = 0x10
+	IP_MULTICAST_LOOP             = 0x12
+	IP_MULTICAST_TTL              = 0x11
+	IP_NEXTHOP                    = 0x19
+	IP_OPTIONS                    = 0x1
+	IP_PKTINFO                    = 0x1a
+	IP_RECVDSTADDR                = 0x7
+	IP_RECVIF                     = 0x9
+	IP_RECVOPTS                   = 0x5
+	IP_RECVPKTINFO                = 0x1a
+	IP_RECVRETOPTS                = 0x6
+	IP_RECVSLLA                   = 0xa
+	IP_RECVTTL                    = 0xb
+	IP_RETOPTS                    = 0x8
+	IP_REUSEADDR                  = 0x104
+	IP_SEC_OPT                    = 0x22
+	IP_TOS                        = 0x3
+	IP_TTL                        = 0x4
+	IP_UNBLOCK_SOURCE             = 0x16
+	IP_UNSPEC_SRC                 = 0x42
+	ISIG                          = 0x1
+	ISTRIP                        = 0x20
+	IXANY                         = 0x800
+	IXOFF                         = 0x1000
+	IXON                          = 0x400
+	MADV_ACCESS_DEFAULT           = 0x6
+	MADV_ACCESS_LWP               = 0x7
+	MADV_ACCESS_MANY              = 0x8
+	MADV_ACCESS_MANY_PSET         = 0x9
+	MADV_DONTNEED                 = 0x4
+	MADV_FREE                     = 0x5
+	MADV_NORMAL                   = 0x0
+	MADV_RANDOM                   = 0x1
+	MADV_SEQUENTIAL               = 0x2
+	MADV_WILLNEED                 = 0x3
+	MAP_ADI                       = 0x8000
+	MAP_ALIGN                     = 0x200
+	MAP_ANON                      = 0x100
+	MAP_ANONYMOUS                 = 0x100
+	MAP_FIXED                     = 0x10
+	MAP_INITDATA                  = 0x800
+	MAP_LOW32                     = 0x80
+	MAP_NORESERVE                 = 0x40
+	MAP_PRIVATE                   = 0x2
+	MAP_RENAME                    = 0x20
+	MAP_SHARED                    = 0x1
+	MAP_TEXT                      = 0x400
+	MAP_TYPE                      = 0xf
+	MCL_CURRENT                   = 0x1
+	MCL_FUTURE                    = 0x2
+	MSG_CTRUNC                    = 0x10
+	MSG_DONTROUTE                 = 0x4
+	MSG_DONTWAIT                  = 0x80
+	MSG_DUPCTRL                   = 0x800
+	MSG_EOR                       = 0x8
+	MSG_MAXIOVLEN                 = 0x10
+	MSG_NOSIGNAL                  = 0x200
+	MSG_NOTIFICATION              = 0x100
+	MSG_OOB                       = 0x1
+	MSG_PEEK                      = 0x2
+	MSG_TRUNC                     = 0x20
+	MSG_WAITALL                   = 0x40
+	MSG_XPG4_2                    = 0x8000
+	MS_ASYNC                      = 0x1
+	MS_INVALIDATE                 = 0x2
+	MS_NOHIGHMAPS                 = 0x8
+	MS_OLDSYNC                    = 0x0
+	MS_SYNC                       = 0x4
+	M_FLUSH                       = 0x86
+	NOFLSH                        = 0x80
+	OCRNL                         = 0x8
+	OFDEL                         = 0x80
+	OFILL                         = 0x40
+	ONLCR                         = 0x4
+	ONLRET                        = 0x20
+	ONOCR                         = 0x10
+	OPENFAIL                      = -0x1
+	OPOST                         = 0x1
+	O_ACCMODE                     = 0x600003
+	O_APPEND                      = 0x8
+	O_CLOEXEC                     = 0x800000
+	O_CLOFORK                     = 0x10000000
+	O_CREAT                       = 0x100
+	O_DIRECTORY                   = 0x1000000
+	O_DSYNC                       = 0x40
+	O_EXCL                        = 0x400
+	O_EXEC                        = 0x400000
+	O_LARGEFILE                   = 0x2000
+	O_NDELAY                      = 0x4
+	O_NOCTTY                      = 0x800
+	O_NOFOLLOW                    = 0x20000
+	O_NOLINKS                     = 0x40000
+	O_NONBLOCK                    = 0x80
+	O_RDONLY                      = 0x0
+	O_RDWR                        = 0x2
+	O_RSYNC                       = 0x8000
+	O_SEARCH                      = 0x200000
+	O_SIOCGIFCONF                 = -0x3ff796ec
+	O_SIOCGLIFCONF                = -0x3fef9688
+	O_SYNC                        = 0x10
+	O_TPDSAFE                     = 0x4000000
+	O_TRUNC                       = 0x200
+	O_TTY_INIT                    = 0x2000000
+	O_WRONLY                      = 0x1
+	O_XATTR                       = 0x4000
+	O_XPG4OPEN                    = 0x8000000
+	PARENB                        = 0x100
+	PAREXT                        = 0x100000
+	PARMRK                        = 0x8
+	PARODD                        = 0x200
+	PENDIN                        = 0x4000
+	PRIO_PGRP                     = 0x1
+	PRIO_PROCESS                  = 0x0
+	PRIO_USER                     = 0x2
+	PROT_EXEC                     = 0x4
+	PROT_NONE                     = 0x0
+	PROT_READ                     = 0x1
+	PROT_WRITE                    = 0x2
+	RLIMIT_AS                     = 0x6
+	RLIMIT_CORE                   = 0x4
+	RLIMIT_CPU                    = 0x0
+	RLIMIT_DATA                   = 0x2
+	RLIMIT_FSIZE                  = 0x1
+	RLIMIT_NOFILE                 = 0x5
+	RLIMIT_STACK                  = 0x3
+	RLIM_INFINITY                 = -0x3
+	RTAX_AUTHOR                   = 0x6
+	RTAX_BRD                      = 0x7
+	RTAX_DST                      = 0x0
+	RTAX_GATEWAY                  = 0x1
+	RTAX_GENMASK                  = 0x3
+	RTAX_IFA                      = 0x5
+	RTAX_IFP                      = 0x4
+	RTAX_MAX                      = 0x9
+	RTAX_NETMASK                  = 0x2
+	RTAX_SRC                      = 0x8
+	RTA_AUTHOR                    = 0x40
+	RTA_BRD                       = 0x80
+	RTA_DST                       = 0x1
+	RTA_GATEWAY                   = 0x2
+	RTA_GENMASK                   = 0x8
+	RTA_IFA                       = 0x20
+	RTA_IFP                       = 0x10
+	RTA_NETMASK                   = 0x4
+	RTA_NUMBITS                   = 0x9
+	RTA_SRC                       = 0x100
+	RTF_BLACKHOLE                 = 0x1000
+	RTF_CLONING                   = 0x100
+	RTF_DONE                      = 0x40
+	RTF_DYNAMIC                   = 0x10
+	RTF_GATEWAY                   = 0x2
+	RTF_HOST                      = 0x4
+	RTF_INDIRECT                  = 0x40000
+	RTF_KERNEL                    = 0x80000
+	RTF_LLINFO                    = 0x400
+	RTF_MASK                      = 0x80
+	RTF_MODIFIED                  = 0x20
+	RTF_MULTIRT                   = 0x10000
+	RTF_NOACCEPT                  = 0x200000
+	RTF_PRIVATE                   = 0x2000
+	RTF_PROTO1                    = 0x8000
+	RTF_PROTO2                    = 0x4000
+	RTF_REJECT                    = 0x8
+	RTF_SETSRC                    = 0x20000
+	RTF_STATIC                    = 0x800
+	RTF_UP                        = 0x1
+	RTF_XRESOLVE                  = 0x200
+	RTF_ZONE                      = 0x100000
+	RTM_ADD                       = 0x1
+	RTM_CHANGE                    = 0x3
+	RTM_CHGADDR                   = 0xf
+	RTM_DELADDR                   = 0xd
+	RTM_DELETE                    = 0x2
+	RTM_FREEADDR                  = 0x10
+	RTM_GET                       = 0x4
+	RTM_IFINFO                    = 0xe
+	RTM_LOCK                      = 0x8
+	RTM_LOSING                    = 0x5
+	RTM_MISS                      = 0x7
+	RTM_NEWADDR                   = 0xc
+	RTM_OLDADD                    = 0x9
+	RTM_OLDDEL                    = 0xa
+	RTM_REDIRECT                  = 0x6
+	RTM_RESOLVE                   = 0xb
+	RTM_VERSION                   = 0x3
+	RTV_EXPIRE                    = 0x4
+	RTV_HOPCOUNT                  = 0x2
+	RTV_MTU                       = 0x1
+	RTV_RPIPE                     = 0x8
+	RTV_RTT                       = 0x40
+	RTV_RTTVAR                    = 0x80
+	RTV_SPIPE                     = 0x10
+	RTV_SSTHRESH                  = 0x20
+	RT_AWARE                      = 0x1
+	RUSAGE_CHILDREN               = -0x1
+	RUSAGE_SELF                   = 0x0
+	SCM_RIGHTS                    = 0x1010
+	SCM_TIMESTAMP                 = 0x1013
+	SCM_UCRED                     = 0x1012
+	SHUT_RD                       = 0x0
+	SHUT_RDWR                     = 0x2
+	SHUT_WR                       = 0x1
+	SIG2STR_MAX                   = 0x20
+	SIOCADDMULTI                  = -0x7fdf96cf
+	SIOCADDRT                     = -0x7fcf8df6
+	SIOCATMARK                    = 0x40047307
+	SIOCCLIFFLAGS                 = -0x3f87966d
+	SIOCDARP                      = -0x7fdb96e0
+	SIOCDELMULTI                  = -0x7fdf96ce
+	SIOCDELRT                     = -0x7fcf8df5
+	SIOCDXARP                     = -0x7fff9658
+	SIOCGARP                      = -0x3fdb96e1
+	SIOCGDSTINFO                  = -0x3fff965c
+	SIOCGENADDR                   = -0x3fdf96ab
+	SIOCGENPSTATS                 = -0x3fdf96c7
+	SIOCGETLSGCNT                 = -0x3fef8deb
+	SIOCGETNAME                   = 0x40107334
+	SIOCGETPEER                   = 0x40107335
+	SIOCGETPROP                   = -0x3fff8f44
+	SIOCGETSGCNT                  = -0x3feb8deb
+	SIOCGETSYNC                   = -0x3fdf96d3
+	SIOCGETVIFCNT                 = -0x3feb8dec
+	SIOCGHIWAT                    = 0x40047301
+	SIOCGIFADDR                   = -0x3fdf96f3
+	SIOCGIFBRDADDR                = -0x3fdf96e9
+	SIOCGIFCONF                   = -0x3ff796a4
+	SIOCGIFDSTADDR                = -0x3fdf96f1
+	SIOCGIFFLAGS                  = -0x3fdf96ef
+	SIOCGIFHWADDR                 = -0x3fdf9647
+	SIOCGIFINDEX                  = -0x3fdf96a6
+	SIOCGIFMEM                    = -0x3fdf96ed
+	SIOCGIFMETRIC                 = -0x3fdf96e5
+	SIOCGIFMTU                    = -0x3fdf96ea
+	SIOCGIFMUXID                  = -0x3fdf96a8
+	SIOCGIFNETMASK                = -0x3fdf96e7
+	SIOCGIFNUM                    = 0x40046957
+	SIOCGIP6ADDRPOLICY            = -0x3fff965e
+	SIOCGIPMSFILTER               = -0x3ffb964c
+	SIOCGLIFADDR                  = -0x3f87968f
+	SIOCGLIFBINDING               = -0x3f879666
+	SIOCGLIFBRDADDR               = -0x3f879685
+	SIOCGLIFCONF                  = -0x3fef965b
+	SIOCGLIFDADSTATE              = -0x3f879642
+	SIOCGLIFDSTADDR               = -0x3f87968d
+	SIOCGLIFFLAGS                 = -0x3f87968b
+	SIOCGLIFGROUPINFO             = -0x3f4b9663
+	SIOCGLIFGROUPNAME             = -0x3f879664
+	SIOCGLIFHWADDR                = -0x3f879640
+	SIOCGLIFINDEX                 = -0x3f87967b
+	SIOCGLIFLNKINFO               = -0x3f879674
+	SIOCGLIFMETRIC                = -0x3f879681
+	SIOCGLIFMTU                   = -0x3f879686
+	SIOCGLIFMUXID                 = -0x3f87967d
+	SIOCGLIFNETMASK               = -0x3f879683
+	SIOCGLIFNUM                   = -0x3ff3967e
+	SIOCGLIFSRCOF                 = -0x3fef964f
+	SIOCGLIFSUBNET                = -0x3f879676
+	SIOCGLIFTOKEN                 = -0x3f879678
+	SIOCGLIFUSESRC                = -0x3f879651
+	SIOCGLIFVRID                  = -0x3f87963c
+	SIOCGLIFZONE                  = -0x3f879656
+	SIOCGLOWAT                    = 0x40047303
+	SIOCGMSFILTER                 = -0x3ffb964e
+	SIOCGPGRP                     = 0x40047309
+	SIOCGSTAMP                    = -0x3fef9646
+	SIOCGXARP                     = -0x3fff9659
+	SIOCIFDETACH                  = -0x7fdf96c8
+	SIOCILB                       = -0x3ffb9645
+	SIOCLIFADDIF                  = -0x3f879691
+	SIOCLIFATFWIFG                = -0x7f87966b
+	SIOCLIFCAFWIFG                = -0x7f87966c
+	SIOCLIFCLFWIFG                = -0x7f879669
+	SIOCLIFDELND                  = -0x7f879673
+	SIOCLIFDTFWIFG                = -0x7f87966a
+	SIOCLIFFWIFGINFO              = -0x3fcf9668
+	SIOCLIFGETND                  = -0x3f879672
+	SIOCLIFREMOVEIF               = -0x7f879692
+	SIOCLIFSETND                  = -0x7f879671
+	SIOCLOWER                     = -0x7fdf96d7
+	SIOCSARP                      = -0x7fdb96e2
+	SIOCSCTPCTX                   = -0x7fef963f
+	SIOCSCTPGOPT                  = -0x3fef9653
+	SIOCSCTPPEELOFF               = -0x3ffb9652
+	SIOCSCTPSNDV                  = -0x3fef963e
+	SIOCSCTPSOPT                  = -0x7fef9654
+	SIOCSETPROP                   = -0x7ffb8f43
+	SIOCSETSYNC                   = -0x7fdf96d4
+	SIOCSHIWAT                    = -0x7ffb8d00
+	SIOCSIFADDR                   = -0x7fdf96f4
+	SIOCSIFBRDADDR                = -0x7fdf96e8
+	SIOCSIFDSTADDR                = -0x7fdf96f2
+	SIOCSIFFLAGS                  = -0x7fdf96f0
+	SIOCSIFINDEX                  = -0x7fdf96a5
+	SIOCSIFMEM                    = -0x7fdf96ee
+	SIOCSIFMETRIC                 = -0x7fdf96e4
+	SIOCSIFMTU                    = -0x7fdf96eb
+	SIOCSIFMUXID                  = -0x7fdf96a7
+	SIOCSIFNAME                   = -0x7fdf96b7
+	SIOCSIFNETMASK                = -0x7fdf96e6
+	SIOCSIP6ADDRPOLICY            = -0x7fff965d
+	SIOCSIPMSFILTER               = -0x7ffb964b
+	SIOCSLGETREQ                  = -0x3fdf96b9
+	SIOCSLIFADDR                  = -0x7f879690
+	SIOCSLIFBRDADDR               = -0x7f879684
+	SIOCSLIFDSTADDR               = -0x7f87968e
+	SIOCSLIFFLAGS                 = -0x7f87968c
+	SIOCSLIFGROUPNAME             = -0x7f879665
+	SIOCSLIFINDEX                 = -0x7f87967a
+	SIOCSLIFLNKINFO               = -0x7f879675
+	SIOCSLIFMETRIC                = -0x7f879680
+	SIOCSLIFMTU                   = -0x7f879687
+	SIOCSLIFMUXID                 = -0x7f87967c
+	SIOCSLIFNAME                  = -0x3f87967f
+	SIOCSLIFNETMASK               = -0x7f879682
+	SIOCSLIFPREFIX                = -0x3f879641
+	SIOCSLIFSUBNET                = -0x7f879677
+	SIOCSLIFTOKEN                 = -0x7f879679
+	SIOCSLIFUSESRC                = -0x7f879650
+	SIOCSLIFVRID                  = -0x3f87963d
+	SIOCSLIFZONE                  = -0x7f879655
+	SIOCSLOWAT                    = -0x7ffb8cfe
+	SIOCSLSTAT                    = -0x7fdf96b8
+	SIOCSMSFILTER                 = -0x7ffb964d
+	SIOCSPGRP                     = -0x7ffb8cf8
+	SIOCSPROMISC                  = -0x7ffb96d0
+	SIOCSQPTR                     = -0x3ffb9648
+	SIOCSSDSTATS                  = -0x3fdf96d2
+	SIOCSSESTATS                  = -0x3fdf96d1
+	SIOCSXARP                     = -0x7fff965a
+	SIOCTMYADDR                   = -0x3ff79670
+	SIOCTMYSITE                   = -0x3ff7966e
+	SIOCTONLINK                   = -0x3ff7966f
+	SIOCUPPER                     = -0x7fdf96d8
+	SIOCX25RCV                    = -0x3fdf96c4
+	SIOCX25TBL                    = -0x3fdf96c3
+	SIOCX25XMT                    = -0x3fdf96c5
+	SIOCXPROTO                    = 0x20007337
+	SOCK_DGRAM                    = 0x1
+	SOCK_FLOW_PROP_VERSION1       = 0x1
+	SOCK_RAW                      = 0x4
+	SOCK_RDM                      = 0x5
+	SOCK_SEQPACKET                = 0x6
+	SOCK_STREAM                   = 0x2
+	SOL_FILTER                    = 0xfffc
+	SOL_PACKET                    = 0xfffd
+	SOL_ROUTE                     = 0xfffe
+	SOL_SOCKET                    = 0xffff
+	SOMAXCONN                     = 0x80
+	SO_ACCEPTCONN                 = 0x2
+	SO_ALL                        = 0x3f
+	SO_ALLZONES                   = 0x1014
+	SO_ANON_MLP                   = 0x100a
+	SO_ATTACH_FILTER              = 0x40000001
+	SO_BAND                       = 0x4000
+	SO_BROADCAST                  = 0x20
+	SO_COPYOPT                    = 0x80000
+	SO_DEBUG                      = 0x1
+	SO_DELIM                      = 0x8000
+	SO_DETACH_FILTER              = 0x40000002
+	SO_DGRAM_ERRIND               = 0x200
+	SO_DOMAIN                     = 0x100c
+	SO_DONTLINGER                 = -0x81
+	SO_DONTROUTE                  = 0x10
+	SO_ERROPT                     = 0x40000
+	SO_ERROR                      = 0x1007
+	SO_EXCLBIND                   = 0x1015
+	SO_FLOW_NAME                  = 0x1020
+	SO_FLOW_SLA                   = 0x1018
+	SO_FWGETSOCKNAME              = 0x40000003
+	SO_HIWAT                      = 0x10
+	SO_ISNTTY                     = 0x800
+	SO_ISTTY                      = 0x400
+	SO_KEEPALIVE                  = 0x8
+	SO_LINGER                     = 0x80
+	SO_LISTENQLIMIT               = 0x1019
+	SO_LOWAT                      = 0x20
+	SO_MAC_EXEMPT                 = 0x100b
+	SO_MAC_IMPLICIT               = 0x1016
+	SO_MAXBLK                     = 0x100000
+	SO_MAXPSZ                     = 0x8
+	SO_MINPSZ                     = 0x4
+	SO_MREADOFF                   = 0x80
+	SO_MREADON                    = 0x40
+	SO_NDELOFF                    = 0x200
+	SO_NDELON                     = 0x100
+	SO_NET_KERNEL_BYPASS          = 0x101a
+	SO_NET_KERNEL_BYPASS_STATS    = 0x101b
+	SO_NODELIM                    = 0x10000
+	SO_OOBINLINE                  = 0x100
+	SO_PASSIVE_CONNECT            = 0x100f
+	SO_PROTOTYPE                  = 0x1009
+	SO_RCVBUF                     = 0x1002
+	SO_RCVLOWAT                   = 0x1004
+	SO_RCVPSH                     = 0x100d
+	SO_RCVTIMEO                   = 0x1006
+	SO_READOPT                    = 0x1
+	SO_RECVUCRED                  = 0x400
+	SO_REUSEADDR                  = 0x4
+	SO_REUSEPORT                  = 0x100e
+	SO_SECATTR                    = 0x1011
+	SO_SNDBUF                     = 0x1001
+	SO_SNDLOWAT                   = 0x1003
+	SO_SNDTIMEO                   = 0x1005
+	SO_STRHOLD                    = 0x20000
+	SO_TAIL                       = 0x200000
+	SO_TIMESTAMP                  = 0x1013
+	SO_TONSTOP                    = 0x2000
+	SO_TOSTOP                     = 0x1000
+	SO_TYPE                       = 0x1008
+	SO_USELOOPBACK                = 0x40
+	SO_VRRP                       = 0x1017
+	SO_WROFF                      = 0x2
+	TCFLSH                        = 0x5407
+	TCIFLUSH                      = 0x0
+	TCIOFLUSH                     = 0x2
+	TCOFLUSH                      = 0x1
+	TCP_ABORT_THRESHOLD           = 0x11
+	TCP_ANONPRIVBIND              = 0x20
+	TCP_CONGESTION                = 0x23
+	TCP_CONN_ABORT_THRESHOLD      = 0x13
+	TCP_CONN_NOTIFY_THRESHOLD     = 0x12
+	TCP_CORK                      = 0x18
+	TCP_EXCLBIND                  = 0x21
+	TCP_INFO                      = 0x22
+	TCP_INIT_CWND                 = 0x15
+	TCP_KEEPALIVE                 = 0x8
+	TCP_KEEPALIVE_ABORT_THRESHOLD = 0x17
+	TCP_KEEPALIVE_THRESHOLD       = 0x16
+	TCP_KEEPCNT                   = 0x1f
+	TCP_KEEPIDLE                  = 0x1d
+	TCP_KEEPINTVL                 = 0x1e
+	TCP_LINGER2                   = 0x1c
+	TCP_MAXSEG                    = 0x2
+	TCP_MD5SIG                    = 0x24
+	TCP_MSS                       = 0x218
+	TCP_NODELAY                   = 0x1
+	TCP_NOTIFY_THRESHOLD          = 0x10
+	TCP_RECVDSTADDR               = 0x14
+	TCP_RTO_INITIAL               = 0x19
+	TCP_RTO_MAX                   = 0x1b
+	TCP_RTO_MIN                   = 0x1a
+	TCSAFLUSH                     = 0x5410
+	TIOC                          = 0x5400
+	TIOCCBRK                      = 0x747a
+	TIOCCDTR                      = 0x7478
+	TIOCCILOOP                    = 0x746c
+	TIOCEXCL                      = 0x740d
+	TIOCFLUSH                     = 0x7410
+	TIOCGETC                      = 0x7412
+	TIOCGETD                      = 0x7400
+	TIOCGETP                      = 0x7408
+	TIOCGLTC                      = 0x7474
+	TIOCGPGRP                     = 0x7414
+	TIOCGPPS                      = 0x547d
+	TIOCGPPSEV                    = 0x547f
+	TIOCGSID                      = 0x7416
+	TIOCGSOFTCAR                  = 0x5469
+	TIOCGWINSZ                    = 0x5468
+	TIOCHPCL                      = 0x7402
+	TIOCKBOF                      = 0x5409
+	TIOCKBON                      = 0x5408
+	TIOCLBIC                      = 0x747e
+	TIOCLBIS                      = 0x747f
+	TIOCLGET                      = 0x747c
+	TIOCLSET                      = 0x747d
+	TIOCMBIC                      = 0x741c
+	TIOCMBIS                      = 0x741b
+	TIOCMGET                      = 0x741d
+	TIOCMSET                      = 0x741a
+	TIOCM_CAR                     = 0x40
+	TIOCM_CD                      = 0x40
+	TIOCM_CTS                     = 0x20
+	TIOCM_DSR                     = 0x100
+	TIOCM_DTR                     = 0x2
+	TIOCM_LE                      = 0x1
+	TIOCM_RI                      = 0x80
+	TIOCM_RNG                     = 0x80
+	TIOCM_RTS                     = 0x4
+	TIOCM_SR                      = 0x10
+	TIOCM_ST                      = 0x8
+	TIOCNOTTY                     = 0x7471
+	TIOCNXCL                      = 0x740e
+	TIOCOUTQ                      = 0x7473
+	TIOCREMOTE                    = 0x741e
+	TIOCSBRK                      = 0x747b
+	TIOCSCTTY                     = 0x7484
+	TIOCSDTR                      = 0x7479
+	TIOCSETC                      = 0x7411
+	TIOCSETD                      = 0x7401
+	TIOCSETN                      = 0x740a
+	TIOCSETP                      = 0x7409
+	TIOCSIGNAL                    = 0x741f
+	TIOCSILOOP                    = 0x746d
+	TIOCSLTC                      = 0x7475
+	TIOCSPGRP                     = 0x7415
+	TIOCSPPS                      = 0x547e
+	TIOCSSOFTCAR                  = 0x546a
+	TIOCSTART                     = 0x746e
+	TIOCSTI                       = 0x7417
+	TIOCSTOP                      = 0x746f
+	TIOCSWINSZ                    = 0x5467
+	TOSTOP                        = 0x100
+	VCEOF                         = 0x8
+	VCEOL                         = 0x9
+	VDISCARD                      = 0xd
+	VDSUSP                        = 0xb
+	VEOF                          = 0x4
+	VEOL                          = 0x5
+	VEOL2                         = 0x6
+	VERASE                        = 0x2
+	VINTR                         = 0x0
+	VKILL                         = 0x3
+	VLNEXT                        = 0xf
+	VMIN                          = 0x4
+	VQUIT                         = 0x1
+	VREPRINT                      = 0xc
+	VSTART                        = 0x8
+	VSTOP                         = 0x9
+	VSUSP                         = 0xa
+	VSWTCH                        = 0x7
+	VT0                           = 0x0
+	VT1                           = 0x4000
+	VTDLY                         = 0x4000
+	VTIME                         = 0x5
+	VWERASE                       = 0xe
+	WCONTFLG                      = 0xffff
+	WCONTINUED                    = 0x8
+	WCOREFLG                      = 0x80
+	WEXITED                       = 0x1
+	WNOHANG                       = 0x40
+	WNOWAIT                       = 0x80
+	WOPTMASK                      = 0xcf
+	WRAP                          = 0x20000
+	WSIGMASK                      = 0x7f
+	WSTOPFLG                      = 0x7f
+	WSTOPPED                      = 0x4
+	WTRAPPED                      = 0x2
+	WUNTRACED                     = 0x4
+)
+
+// Errors
+const (
+	E2BIG           = Errno(0x7)
+	EACCES          = Errno(0xd)
+	EADDRINUSE      = Errno(0x7d)
+	EADDRNOTAVAIL   = Errno(0x7e)
+	EADI            = Errno(0x4b)
+	EADV            = Errno(0x44)
+	EAFNOSUPPORT    = Errno(0x7c)
+	EAGAIN          = Errno(0xb)
+	EALREADY        = Errno(0x95)
+	EBADE           = Errno(0x32)
+	EBADF           = Errno(0x9)
+	EBADFD          = Errno(0x51)
+	EBADMSG         = Errno(0x4d)
+	EBADR           = Errno(0x33)
+	EBADRQC         = Errno(0x36)
+	EBADSLT         = Errno(0x37)
+	EBFONT          = Errno(0x39)
+	EBUSY           = Errno(0x10)
+	ECANCELED       = Errno(0x2f)
+	ECHILD          = Errno(0xa)
+	ECHRNG          = Errno(0x25)
+	ECOMM           = Errno(0x46)
+	ECONNABORTED    = Errno(0x82)
+	ECONNREFUSED    = Errno(0x92)
+	ECONNRESET      = Errno(0x83)
+	EDEADLK         = Errno(0x2d)
+	EDEADLOCK       = Errno(0x38)
+	EDESTADDRREQ    = Errno(0x60)
+	EDOM            = Errno(0x21)
+	EDQUOT          = Errno(0x31)
+	EEXIST          = Errno(0x11)
+	EFAULT          = Errno(0xe)
+	EFBIG           = Errno(0x1b)
+	EHOSTDOWN       = Errno(0x93)
+	EHOSTUNREACH    = Errno(0x94)
+	EIDRM           = Errno(0x24)
+	EILSEQ          = Errno(0x58)
+	EINPROGRESS     = Errno(0x96)
+	EINTR           = Errno(0x4)
+	EINVAL          = Errno(0x16)
+	EIO             = Errno(0x5)
+	EISCONN         = Errno(0x85)
+	EISDIR          = Errno(0x15)
+	EL2HLT          = Errno(0x2c)
+	EL2NSYNC        = Errno(0x26)
+	EL3HLT          = Errno(0x27)
+	EL3RST          = Errno(0x28)
+	ELIBACC         = Errno(0x53)
+	ELIBBAD         = Errno(0x54)
+	ELIBEXEC        = Errno(0x57)
+	ELIBMAX         = Errno(0x56)
+	ELIBSCN         = Errno(0x55)
+	ELNRNG          = Errno(0x29)
+	ELOCKUNMAPPED   = Errno(0x48)
+	ELOOP           = Errno(0x5a)
+	EMFILE          = Errno(0x18)
+	EMLINK          = Errno(0x1f)
+	EMSGSIZE        = Errno(0x61)
+	EMULTIHOP       = Errno(0x4a)
+	ENAMETOOLONG    = Errno(0x4e)
+	ENETDOWN        = Errno(0x7f)
+	ENETRESET       = Errno(0x81)
+	ENETUNREACH     = Errno(0x80)
+	ENFILE          = Errno(0x17)
+	ENOANO          = Errno(0x35)
+	ENOBUFS         = Errno(0x84)
+	ENOCSI          = Errno(0x2b)
+	ENODATA         = Errno(0x3d)
+	ENODEV          = Errno(0x13)
+	ENOENT          = Errno(0x2)
+	ENOEXEC         = Errno(0x8)
+	ENOLCK          = Errno(0x2e)
+	ENOLINK         = Errno(0x43)
+	ENOMEM          = Errno(0xc)
+	ENOMSG          = Errno(0x23)
+	ENONET          = Errno(0x40)
+	ENOPKG          = Errno(0x41)
+	ENOPROTOOPT     = Errno(0x63)
+	ENOSPC          = Errno(0x1c)
+	ENOSR           = Errno(0x3f)
+	ENOSTR          = Errno(0x3c)
+	ENOSYS          = Errno(0x59)
+	ENOTACTIVE      = Errno(0x49)
+	ENOTBLK         = Errno(0xf)
+	ENOTCONN        = Errno(0x86)
+	ENOTDIR         = Errno(0x14)
+	ENOTEMPTY       = Errno(0x5d)
+	ENOTRECOVERABLE = Errno(0x3b)
+	ENOTSOCK        = Errno(0x5f)
+	ENOTSUP         = Errno(0x30)
+	ENOTTY          = Errno(0x19)
+	ENOTUNIQ        = Errno(0x50)
+	ENXIO           = Errno(0x6)
+	EOPNOTSUPP      = Errno(0x7a)
+	EOVERFLOW       = Errno(0x4f)
+	EOWNERDEAD      = Errno(0x3a)
+	EPERM           = Errno(0x1)
+	EPFNOSUPPORT    = Errno(0x7b)
+	EPIPE           = Errno(0x20)
+	EPROTO          = Errno(0x47)
+	EPROTONOSUPPORT = Errno(0x78)
+	EPROTOTYPE      = Errno(0x62)
+	ERANGE          = Errno(0x22)
+	EREMCHG         = Errno(0x52)
+	EREMOTE         = Errno(0x42)
+	ERESTART        = Errno(0x5b)
+	EROFS           = Errno(0x1e)
+	ESHUTDOWN       = Errno(0x8f)
+	ESOCKTNOSUPPORT = Errno(0x79)
+	ESPIPE          = Errno(0x1d)
+	ESRCH           = Errno(0x3)
+	ESRMNT          = Errno(0x45)
+	ESTALE          = Errno(0x97)
+	ESTRPIPE        = Errno(0x5c)
+	ETIME           = Errno(0x3e)
+	ETIMEDOUT       = Errno(0x91)
+	ETOOMANYREFS    = Errno(0x90)
+	ETXTBSY         = Errno(0x1a)
+	EUNATCH         = Errno(0x2a)
+	EUSERS          = Errno(0x5e)
+	EWOULDBLOCK     = Errno(0xb)
+	EXDEV           = Errno(0x12)
+	EXFULL          = Errno(0x34)
+)
+
+// Signals
+const (
+	SIGABRT      = Signal(0x6)
+	SIGAIOCANCEL = Signal(0x21)
+	SIGALRM      = Signal(0xe)
+	SIGBUS       = Signal(0xa)
+	SIGCANCEL    = Signal(0x24)
+	SIGCHLD      = Signal(0x12)
+	SIGCLD       = Signal(0x12)
+	SIGCONT      = Signal(0x19)
+	SIGEMT       = Signal(0x7)
+	SIGFPE       = Signal(0x8)
+	SIGFREEZE    = Signal(0x22)
+	SIGHUP       = Signal(0x1)
+	SIGILL       = Signal(0x4)
+	SIGINT       = Signal(0x2)
+	SIGIO        = Signal(0x16)
+	SIGIOT       = Signal(0x6)
+	SIGJVM1      = Signal(0x27)
+	SIGJVM2      = Signal(0x28)
+	SIGKILL      = Signal(0x9)
+	SIGLOST      = Signal(0x25)
+	SIGLWP       = Signal(0x21)
+	SIGPIPE      = Signal(0xd)
+	SIGPOLL      = Signal(0x16)
+	SIGPROF      = Signal(0x1d)
+	SIGPWR       = Signal(0x13)
+	SIGQUIT      = Signal(0x3)
+	SIGSEGV      = Signal(0xb)
+	SIGSTOP      = Signal(0x17)
+	SIGSYS       = Signal(0xc)
+	SIGTERM      = Signal(0xf)
+	SIGTHAW      = Signal(0x23)
+	SIGTRAP      = Signal(0x5)
+	SIGTSTP      = Signal(0x18)
+	SIGTTIN      = Signal(0x1a)
+	SIGTTOU      = Signal(0x1b)
+	SIGURG       = Signal(0x15)
+	SIGUSR1      = Signal(0x10)
+	SIGUSR2      = Signal(0x11)
+	SIGVTALRM    = Signal(0x1c)
+	SIGWAITING   = Signal(0x20)
+	SIGWINCH     = Signal(0x14)
+	SIGXCPU      = Signal(0x1e)
+	SIGXFSZ      = Signal(0x1f)
+	SIGXRES      = Signal(0x26)
+)
+
+// Error table
+var errors = [...]string{
+	1:   "not owner",
+	2:   "no such file or directory",
+	3:   "no such process",
+	4:   "interrupted system call",
+	5:   "I/O error",
+	6:   "no such device or address",
+	7:   "arg list too long",
+	8:   "exec format error",
+	9:   "bad file number",
+	10:  "no child processes",
+	11:  "resource temporarily unavailable",
+	12:  "not enough space",
+	13:  "permission denied",
+	14:  "bad address",
+	15:  "block device required",
+	16:  "device busy",
+	17:  "file exists",
+	18:  "cross-device link",
+	19:  "no such device",
+	20:  "not a directory",
+	21:  "is a directory",
+	22:  "invalid argument",
+	23:  "file table overflow",
+	24:  "too many open files",
+	25:  "inappropriate ioctl for device",
+	26:  "text file busy",
+	27:  "file too large",
+	28:  "no space left on device",
+	29:  "illegal seek",
+	30:  "read-only file system",
+	31:  "too many links",
+	32:  "broken pipe",
+	33:  "argument out of domain",
+	34:  "result too large",
+	35:  "no message of desired type",
+	36:  "identifier removed",
+	37:  "channel number out of range",
+	38:  "level 2 not synchronized",
+	39:  "level 3 halted",
+	40:  "level 3 reset",
+	41:  "link number out of range",
+	42:  "protocol driver not attached",
+	43:  "no CSI structure available",
+	44:  "level 2 halted",
+	45:  "deadlock situation detected/avoided",
+	46:  "no record locks available",
+	47:  "operation canceled",
+	48:  "operation not supported",
+	49:  "disc quota exceeded",
+	50:  "checksum failure",
+	51:  "too fragmented",
+	52:  "message tables full",
+	53:  "cryptographic key not available",
+	54:  "bad request code",
+	55:  "invalid slot",
+	56:  "file locking deadlock",
+	57:  "bad font file format",
+	58:  "owner of the lock died",
+	59:  "lock is not recoverable",
+	60:  "not a stream device",
+	61:  "no data available",
+	62:  "timer expired",
+	63:  "out of stream resources",
+	64:  "machine is not on the network",
+	65:  "package not installed",
+	66:  "object is remote",
+	67:  "link has been severed",
+	68:  "advertise error",
+	69:  "srmount error",
+	70:  "communication error on send",
+	71:  "protocol error",
+	72:  "locked lock was unmapped ",
+	73:  "facility is not active",
+	74:  "multihop attempted",
+	75:  "application Data Integrity mismatch detected",
+	77:  "not a data message",
+	78:  "file name too long",
+	79:  "value too large for defined data type",
+	80:  "name not unique on network",
+	81:  "file descriptor in bad state",
+	82:  "remote address changed",
+	83:  "can not access a needed shared library",
+	84:  "accessing a corrupted shared library",
+	85:  ".lib section in a.out corrupted",
+	86:  "attempting to link in more shared libraries than system limit",
+	87:  "can not exec a shared library directly",
+	88:  "illegal byte sequence",
+	89:  "operation not applicable",
+	90:  "number of symbolic links encountered during path name traversal exceeds MAXSYMLINKS",
+	91:  "error 91",
+	92:  "error 92",
+	93:  "directory not empty",
+	94:  "too many users",
+	95:  "socket operation on non-socket",
+	96:  "destination address required",
+	97:  "message too long",
+	98:  "protocol wrong type for socket",
+	99:  "option not supported by protocol",
+	120: "protocol not supported",
+	121: "socket type not supported",
+	122: "operation not supported on transport endpoint",
+	123: "protocol family not supported",
+	124: "address family not supported by protocol family",
+	125: "address already in use",
+	126: "cannot assign requested address",
+	127: "network is down",
+	128: "network is unreachable",
+	129: "network dropped connection because of reset",
+	130: "software caused connection abort",
+	131: "connection reset by peer",
+	132: "no buffer space available",
+	133: "transport endpoint is already connected",
+	134: "transport endpoint is not connected",
+	143: "cannot send after socket shutdown",
+	144: "too many references: cannot splice",
+	145: "connection timed out",
+	146: "connection refused",
+	147: "host is down",
+	148: "no route to host",
+	149: "operation already in progress",
+	150: "operation now in progress",
+	151: "stale NFS file handle",
+}
+
+// Signal table
+var signals = [...]string{
+	1:  "hangup",
+	2:  "interrupt",
+	3:  "quit",
+	4:  "illegal Instruction",
+	5:  "trace/Breakpoint Trap",
+	6:  "abort",
+	7:  "emulation Trap",
+	8:  "arithmetic Exception",
+	9:  "killed",
+	10: "bus Error",
+	11: "segmentation Fault",
+	12: "bad System Call",
+	13: "broken Pipe",
+	14: "alarm Clock",
+	15: "terminated",
+	16: "user Signal 1",
+	17: "user Signal 2",
+	18: "child Status Changed",
+	19: "power-Fail/Restart",
+	20: "window Size Change",
+	21: "urgent Socket Condition",
+	22: "pollable Event",
+	23: "stopped (signal)",
+	24: "stopped (user)",
+	25: "continued",
+	26: "stopped (tty input)",
+	27: "stopped (tty output)",
+	28: "virtual Timer Expired",
+	29: "profiling Timer Expired",
+	30: "cpu Limit Exceeded",
+	31: "file Size Limit Exceeded",
+	32: "no runnable lwp",
+	33: "inter-lwp signal",
+	34: "checkpoint Freeze",
+	35: "checkpoint Thaw",
+	36: "thread Cancellation",
+	37: "resource Lost",
+	38: "resource Control Exceeded",
+	39: "reserved for JVM 1",
+	40: "reserved for JVM 2",
+}
diff --git a/src/syscall/zsyscall_solaris_sparc64.go b/src/syscall/zsyscall_solaris_sparc64.go
new file mode 100644
index 0000000..e2108f6
--- /dev/null
+++ b/src/syscall/zsyscall_solaris_sparc64.go
@@ -0,0 +1,1097 @@
+// mksyscall_solaris.pl syscall_solaris.go syscall_solaris_sparc64.go
+// MACHINE GENERATED BY THE COMMAND ABOVE; DO NOT EDIT
+
+// +build sparc64,solaris
+
+package syscall
+
+import "unsafe"
+
+//go:cgo_import_dynamic libc_Getcwd getcwd "libc.so"
+//go:cgo_import_dynamic libc_getgroups getgroups "libc.so"
+//go:cgo_import_dynamic libc_setgroups setgroups "libc.so"
+//go:cgo_import_dynamic libc_fcntl fcntl "libc.so"
+//go:cgo_import_dynamic libc_accept accept "libsocket.so"
+//go:cgo_import_dynamic libc_sendmsg sendmsg "libsocket.so"
+//go:cgo_import_dynamic libc_Access access "libc.so"
+//go:cgo_import_dynamic libc_Adjtime adjtime "libc.so"
+//go:cgo_import_dynamic libc_Chdir chdir "libc.so"
+//go:cgo_import_dynamic libc_Chmod chmod "libc.so"
+//go:cgo_import_dynamic libc_Chown chown "libc.so"
+//go:cgo_import_dynamic libc_Chroot chroot "libc.so"
+//go:cgo_import_dynamic libc_Close close "libc.so"
+//go:cgo_import_dynamic libc_Dup dup "libc.so"
+//go:cgo_import_dynamic libc_Exit exit "libc.so"
+//go:cgo_import_dynamic libc_Fchdir fchdir "libc.so"
+//go:cgo_import_dynamic libc_Fchmod fchmod "libc.so"
+//go:cgo_import_dynamic libc_Fchown fchown "libc.so"
+//go:cgo_import_dynamic libc_Fpathconf fpathconf "libc.so"
+//go:cgo_import_dynamic libc_Fstat fstat "libc.so"
+//go:cgo_import_dynamic libc_Getdents getdents "libc.so"
+//go:cgo_import_dynamic libc_Getgid getgid "libc.so"
+//go:cgo_import_dynamic libc_Getpid getpid "libc.so"
+//go:cgo_import_dynamic libc_Geteuid geteuid "libc.so"
+//go:cgo_import_dynamic libc_Getegid getegid "libc.so"
+//go:cgo_import_dynamic libc_Getppid getppid "libc.so"
+//go:cgo_import_dynamic libc_Getpriority getpriority "libc.so"
+//go:cgo_import_dynamic libc_Getrlimit getrlimit "libc.so"
+//go:cgo_import_dynamic libc_Gettimeofday gettimeofday "libc.so"
+//go:cgo_import_dynamic libc_Getuid getuid "libc.so"
+//go:cgo_import_dynamic libc_Kill kill "libc.so"
+//go:cgo_import_dynamic libc_Lchown lchown "libc.so"
+//go:cgo_import_dynamic libc_Link link "libc.so"
+//go:cgo_import_dynamic libc_listen listen "libsocket.so"
+//go:cgo_import_dynamic libc_Lstat lstat "libc.so"
+//go:cgo_import_dynamic libc_Mkdir mkdir "libc.so"
+//go:cgo_import_dynamic libc_Mknod mknod "libc.so"
+//go:cgo_import_dynamic libc_Nanosleep nanosleep "libc.so"
+//go:cgo_import_dynamic libc_Open open "libc.so"
+//go:cgo_import_dynamic libc_Pathconf pathconf "libc.so"
+//go:cgo_import_dynamic libc_Pread pread "libc.so"
+//go:cgo_import_dynamic libc_Pwrite pwrite "libc.so"
+//go:cgo_import_dynamic libc_read read "libc.so"
+//go:cgo_import_dynamic libc_Readlink readlink "libc.so"
+//go:cgo_import_dynamic libc_Rename rename "libc.so"
+//go:cgo_import_dynamic libc_Rmdir rmdir "libc.so"
+//go:cgo_import_dynamic libc_lseek lseek "libc.so"
+//go:cgo_import_dynamic libc_sendfile sendfile "libsendfile.so"
+//go:cgo_import_dynamic libc_Setegid setegid "libc.so"
+//go:cgo_import_dynamic libc_Seteuid seteuid "libc.so"
+//go:cgo_import_dynamic libc_Setgid setgid "libc.so"
+//go:cgo_import_dynamic libc_Setpgid setpgid "libc.so"
+//go:cgo_import_dynamic libc_Setpriority setpriority "libc.so"
+//go:cgo_import_dynamic libc_Setregid setregid "libc.so"
+//go:cgo_import_dynamic libc_Setreuid setreuid "libc.so"
+//go:cgo_import_dynamic libc_Setrlimit setrlimit "libc.so"
+//go:cgo_import_dynamic libc_Setsid setsid "libc.so"
+//go:cgo_import_dynamic libc_Setuid setuid "libc.so"
+//go:cgo_import_dynamic libc_shutdown shutdown "libsocket.so"
+//go:cgo_import_dynamic libc_Stat stat "libc.so"
+//go:cgo_import_dynamic libc_Symlink symlink "libc.so"
+//go:cgo_import_dynamic libc_Sync sync "libc.so"
+//go:cgo_import_dynamic libc_Truncate truncate "libc.so"
+//go:cgo_import_dynamic libc_Fsync fsync "libc.so"
+//go:cgo_import_dynamic libc_Ftruncate ftruncate "libc.so"
+//go:cgo_import_dynamic libc_Umask umask "libc.so"
+//go:cgo_import_dynamic libc_Unlink unlink "libc.so"
+//go:cgo_import_dynamic libc_Utimes utimes "libc.so"
+//go:cgo_import_dynamic libc_bind bind "libsocket.so"
+//go:cgo_import_dynamic libc_connect connect "libsocket.so"
+//go:cgo_import_dynamic libc_mmap mmap "libc.so"
+//go:cgo_import_dynamic libc_munmap munmap "libc.so"
+//go:cgo_import_dynamic libc_sendto sendto "libsocket.so"
+//go:cgo_import_dynamic libc_socket socket "libsocket.so"
+//go:cgo_import_dynamic libc_socketpair socketpair "libsocket.so"
+//go:cgo_import_dynamic libc_write write "libc.so"
+//go:cgo_import_dynamic libc_getsockopt getsockopt "libsocket.so"
+//go:cgo_import_dynamic libc_getpeername getpeername "libsocket.so"
+//go:cgo_import_dynamic libc_getsockname getsockname "libsocket.so"
+//go:cgo_import_dynamic libc_setsockopt setsockopt "libsocket.so"
+//go:cgo_import_dynamic libc_recvfrom recvfrom "libsocket.so"
+//go:cgo_import_dynamic libc_recvmsg recvmsg "libsocket.so"
+
+//go:linkname libc_Getcwd libc_Getcwd
+//go:linkname libc_getgroups libc_getgroups
+//go:linkname libc_setgroups libc_setgroups
+//go:linkname libc_fcntl libc_fcntl
+//go:linkname libc_accept libc_accept
+//go:linkname libc_sendmsg libc_sendmsg
+//go:linkname libc_Access libc_Access
+//go:linkname libc_Adjtime libc_Adjtime
+//go:linkname libc_Chdir libc_Chdir
+//go:linkname libc_Chmod libc_Chmod
+//go:linkname libc_Chown libc_Chown
+//go:linkname libc_Chroot libc_Chroot
+//go:linkname libc_Close libc_Close
+//go:linkname libc_Dup libc_Dup
+//go:linkname libc_Exit libc_Exit
+//go:linkname libc_Fchdir libc_Fchdir
+//go:linkname libc_Fchmod libc_Fchmod
+//go:linkname libc_Fchown libc_Fchown
+//go:linkname libc_Fpathconf libc_Fpathconf
+//go:linkname libc_Fstat libc_Fstat
+//go:linkname libc_Getdents libc_Getdents
+//go:linkname libc_Getgid libc_Getgid
+//go:linkname libc_Getpid libc_Getpid
+//go:linkname libc_Geteuid libc_Geteuid
+//go:linkname libc_Getegid libc_Getegid
+//go:linkname libc_Getppid libc_Getppid
+//go:linkname libc_Getpriority libc_Getpriority
+//go:linkname libc_Getrlimit libc_Getrlimit
+//go:linkname libc_Gettimeofday libc_Gettimeofday
+//go:linkname libc_Getuid libc_Getuid
+//go:linkname libc_Kill libc_Kill
+//go:linkname libc_Lchown libc_Lchown
+//go:linkname libc_Link libc_Link
+//go:linkname libc_listen libc_listen
+//go:linkname libc_Lstat libc_Lstat
+//go:linkname libc_Mkdir libc_Mkdir
+//go:linkname libc_Mknod libc_Mknod
+//go:linkname libc_Nanosleep libc_Nanosleep
+//go:linkname libc_Open libc_Open
+//go:linkname libc_Pathconf libc_Pathconf
+//go:linkname libc_Pread libc_Pread
+//go:linkname libc_Pwrite libc_Pwrite
+//go:linkname libc_read libc_read
+//go:linkname libc_Readlink libc_Readlink
+//go:linkname libc_Rename libc_Rename
+//go:linkname libc_Rmdir libc_Rmdir
+//go:linkname libc_lseek libc_lseek
+//go:linkname libc_sendfile libc_sendfile
+//go:linkname libc_Setegid libc_Setegid
+//go:linkname libc_Seteuid libc_Seteuid
+//go:linkname libc_Setgid libc_Setgid
+//go:linkname libc_Setpgid libc_Setpgid
+//go:linkname libc_Setpriority libc_Setpriority
+//go:linkname libc_Setregid libc_Setregid
+//go:linkname libc_Setreuid libc_Setreuid
+//go:linkname libc_Setrlimit libc_Setrlimit
+//go:linkname libc_Setsid libc_Setsid
+//go:linkname libc_Setuid libc_Setuid
+//go:linkname libc_shutdown libc_shutdown
+//go:linkname libc_Stat libc_Stat
+//go:linkname libc_Symlink libc_Symlink
+//go:linkname libc_Sync libc_Sync
+//go:linkname libc_Truncate libc_Truncate
+//go:linkname libc_Fsync libc_Fsync
+//go:linkname libc_Ftruncate libc_Ftruncate
+//go:linkname libc_Umask libc_Umask
+//go:linkname libc_Unlink libc_Unlink
+//go:linkname libc_Utimes libc_Utimes
+//go:linkname libc_bind libc_bind
+//go:linkname libc_connect libc_connect
+//go:linkname libc_mmap libc_mmap
+//go:linkname libc_munmap libc_munmap
+//go:linkname libc_sendto libc_sendto
+//go:linkname libc_socket libc_socket
+//go:linkname libc_socketpair libc_socketpair
+//go:linkname libc_write libc_write
+//go:linkname libc_getsockopt libc_getsockopt
+//go:linkname libc_getpeername libc_getpeername
+//go:linkname libc_getsockname libc_getsockname
+//go:linkname libc_setsockopt libc_setsockopt
+//go:linkname libc_recvfrom libc_recvfrom
+//go:linkname libc_recvmsg libc_recvmsg
+
+type libcFunc uintptr
+
+var (
+	libc_Getcwd,
+	libc_getgroups,
+	libc_setgroups,
+	libc_fcntl,
+	libc_accept,
+	libc_sendmsg,
+	libc_Access,
+	libc_Adjtime,
+	libc_Chdir,
+	libc_Chmod,
+	libc_Chown,
+	libc_Chroot,
+	libc_Close,
+	libc_Dup,
+	libc_Exit,
+	libc_Fchdir,
+	libc_Fchmod,
+	libc_Fchown,
+	libc_Fpathconf,
+	libc_Fstat,
+	libc_Getdents,
+	libc_Getgid,
+	libc_Getpid,
+	libc_Geteuid,
+	libc_Getegid,
+	libc_Getppid,
+	libc_Getpriority,
+	libc_Getrlimit,
+	libc_Gettimeofday,
+	libc_Getuid,
+	libc_Kill,
+	libc_Lchown,
+	libc_Link,
+	libc_listen,
+	libc_Lstat,
+	libc_Mkdir,
+	libc_Mknod,
+	libc_Nanosleep,
+	libc_Open,
+	libc_Pathconf,
+	libc_Pread,
+	libc_Pwrite,
+	libc_read,
+	libc_Readlink,
+	libc_Rename,
+	libc_Rmdir,
+	libc_lseek,
+	libc_sendfile,
+	libc_Setegid,
+	libc_Seteuid,
+	libc_Setgid,
+	libc_Setpgid,
+	libc_Setpriority,
+	libc_Setregid,
+	libc_Setreuid,
+	libc_Setrlimit,
+	libc_Setsid,
+	libc_Setuid,
+	libc_shutdown,
+	libc_Stat,
+	libc_Symlink,
+	libc_Sync,
+	libc_Truncate,
+	libc_Fsync,
+	libc_Ftruncate,
+	libc_Umask,
+	libc_Unlink,
+	libc_Utimes,
+	libc_bind,
+	libc_connect,
+	libc_mmap,
+	libc_munmap,
+	libc_sendto,
+	libc_socket,
+	libc_socketpair,
+	libc_write,
+	libc_getsockopt,
+	libc_getpeername,
+	libc_getsockname,
+	libc_setsockopt,
+	libc_recvfrom,
+	libc_recvmsg libcFunc
+)
+
+func Getcwd(buf []byte) (n int, err error) {
+	var _p0 *byte
+	if len(buf) > 0 {
+		_p0 = &buf[0]
+	}
+	r0, _, e1 := sysvicall6(uintptr(unsafe.Pointer(&libc_Getcwd)), 2, uintptr(unsafe.Pointer(_p0)), uintptr(len(buf)), 0, 0, 0, 0)
+	n = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+func getgroups(ngid int, gid *_Gid_t) (n int, err error) {
+	r0, _, e1 := rawSysvicall6(uintptr(unsafe.Pointer(&libc_getgroups)), 2, uintptr(ngid), uintptr(unsafe.Pointer(gid)), 0, 0, 0, 0)
+	n = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+func setgroups(ngid int, gid *_Gid_t) (err error) {
+	_, _, e1 := rawSysvicall6(uintptr(unsafe.Pointer(&libc_setgroups)), 2, uintptr(ngid), uintptr(unsafe.Pointer(gid)), 0, 0, 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+func fcntl(fd int, cmd int, arg int) (val int, err error) {
+	r0, _, e1 := sysvicall6(uintptr(unsafe.Pointer(&libc_fcntl)), 3, uintptr(fd), uintptr(cmd), uintptr(arg), 0, 0, 0)
+	val = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+func accept(s int, rsa *RawSockaddrAny, addrlen *_Socklen) (fd int, err error) {
+	r0, _, e1 := sysvicall6(uintptr(unsafe.Pointer(&libc_accept)), 3, uintptr(s), uintptr(unsafe.Pointer(rsa)), uintptr(unsafe.Pointer(addrlen)), 0, 0, 0)
+	fd = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+func sendmsg(s int, msg *Msghdr, flags int) (n int, err error) {
+	r0, _, e1 := sysvicall6(uintptr(unsafe.Pointer(&libc_sendmsg)), 3, uintptr(s), uintptr(unsafe.Pointer(msg)), uintptr(flags), 0, 0, 0)
+	n = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+func Access(path string, mode uint32) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	_, _, e1 := sysvicall6(uintptr(unsafe.Pointer(&libc_Access)), 2, uintptr(unsafe.Pointer(_p0)), uintptr(mode), 0, 0, 0, 0)
+	use(unsafe.Pointer(_p0))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+func Adjtime(delta *Timeval, olddelta *Timeval) (err error) {
+	_, _, e1 := sysvicall6(uintptr(unsafe.Pointer(&libc_Adjtime)), 2, uintptr(unsafe.Pointer(delta)), uintptr(unsafe.Pointer(olddelta)), 0, 0, 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+func Chdir(path string) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	_, _, e1 := sysvicall6(uintptr(unsafe.Pointer(&libc_Chdir)), 1, uintptr(unsafe.Pointer(_p0)), 0, 0, 0, 0, 0)
+	use(unsafe.Pointer(_p0))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+func Chmod(path string, mode uint32) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	_, _, e1 := sysvicall6(uintptr(unsafe.Pointer(&libc_Chmod)), 2, uintptr(unsafe.Pointer(_p0)), uintptr(mode), 0, 0, 0, 0)
+	use(unsafe.Pointer(_p0))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+func Chown(path string, uid int, gid int) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	_, _, e1 := sysvicall6(uintptr(unsafe.Pointer(&libc_Chown)), 3, uintptr(unsafe.Pointer(_p0)), uintptr(uid), uintptr(gid), 0, 0, 0)
+	use(unsafe.Pointer(_p0))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+func Chroot(path string) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	_, _, e1 := sysvicall6(uintptr(unsafe.Pointer(&libc_Chroot)), 1, uintptr(unsafe.Pointer(_p0)), 0, 0, 0, 0, 0)
+	use(unsafe.Pointer(_p0))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+func Close(fd int) (err error) {
+	_, _, e1 := sysvicall6(uintptr(unsafe.Pointer(&libc_Close)), 1, uintptr(fd), 0, 0, 0, 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+func Dup(fd int) (nfd int, err error) {
+	r0, _, e1 := sysvicall6(uintptr(unsafe.Pointer(&libc_Dup)), 1, uintptr(fd), 0, 0, 0, 0, 0)
+	nfd = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+func Exit(code int) {
+	sysvicall6(uintptr(unsafe.Pointer(&libc_Exit)), 1, uintptr(code), 0, 0, 0, 0, 0)
+	return
+}
+
+func Fchdir(fd int) (err error) {
+	_, _, e1 := sysvicall6(uintptr(unsafe.Pointer(&libc_Fchdir)), 1, uintptr(fd), 0, 0, 0, 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+func Fchmod(fd int, mode uint32) (err error) {
+	_, _, e1 := sysvicall6(uintptr(unsafe.Pointer(&libc_Fchmod)), 2, uintptr(fd), uintptr(mode), 0, 0, 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+func Fchown(fd int, uid int, gid int) (err error) {
+	_, _, e1 := sysvicall6(uintptr(unsafe.Pointer(&libc_Fchown)), 3, uintptr(fd), uintptr(uid), uintptr(gid), 0, 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+func Fpathconf(fd int, name int) (val int, err error) {
+	r0, _, e1 := sysvicall6(uintptr(unsafe.Pointer(&libc_Fpathconf)), 2, uintptr(fd), uintptr(name), 0, 0, 0, 0)
+	val = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+func Fstat(fd int, stat *Stat_t) (err error) {
+	_, _, e1 := sysvicall6(uintptr(unsafe.Pointer(&libc_Fstat)), 2, uintptr(fd), uintptr(unsafe.Pointer(stat)), 0, 0, 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+func Getdents(fd int, buf []byte, basep *uintptr) (n int, err error) {
+	var _p0 *byte
+	if len(buf) > 0 {
+		_p0 = &buf[0]
+	}
+	r0, _, e1 := sysvicall6(uintptr(unsafe.Pointer(&libc_Getdents)), 4, uintptr(fd), uintptr(unsafe.Pointer(_p0)), uintptr(len(buf)), uintptr(unsafe.Pointer(basep)), 0, 0)
+	n = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+func Getgid() (gid int) {
+	r0, _, _ := rawSysvicall6(uintptr(unsafe.Pointer(&libc_Getgid)), 0, 0, 0, 0, 0, 0, 0)
+	gid = int(r0)
+	return
+}
+
+func Getpid() (pid int) {
+	r0, _, _ := rawSysvicall6(uintptr(unsafe.Pointer(&libc_Getpid)), 0, 0, 0, 0, 0, 0, 0)
+	pid = int(r0)
+	return
+}
+
+func Geteuid() (euid int) {
+	r0, _, _ := sysvicall6(uintptr(unsafe.Pointer(&libc_Geteuid)), 0, 0, 0, 0, 0, 0, 0)
+	euid = int(r0)
+	return
+}
+
+func Getegid() (egid int) {
+	r0, _, _ := sysvicall6(uintptr(unsafe.Pointer(&libc_Getegid)), 0, 0, 0, 0, 0, 0, 0)
+	egid = int(r0)
+	return
+}
+
+func Getppid() (ppid int) {
+	r0, _, _ := sysvicall6(uintptr(unsafe.Pointer(&libc_Getppid)), 0, 0, 0, 0, 0, 0, 0)
+	ppid = int(r0)
+	return
+}
+
+func Getpriority(which int, who int) (n int, err error) {
+	r0, _, e1 := sysvicall6(uintptr(unsafe.Pointer(&libc_Getpriority)), 2, uintptr(which), uintptr(who), 0, 0, 0, 0)
+	n = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+func Getrlimit(which int, lim *Rlimit) (err error) {
+	_, _, e1 := rawSysvicall6(uintptr(unsafe.Pointer(&libc_Getrlimit)), 2, uintptr(which), uintptr(unsafe.Pointer(lim)), 0, 0, 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+func Gettimeofday(tv *Timeval) (err error) {
+	_, _, e1 := rawSysvicall6(uintptr(unsafe.Pointer(&libc_Gettimeofday)), 1, uintptr(unsafe.Pointer(tv)), 0, 0, 0, 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+func Getuid() (uid int) {
+	r0, _, _ := rawSysvicall6(uintptr(unsafe.Pointer(&libc_Getuid)), 0, 0, 0, 0, 0, 0, 0)
+	uid = int(r0)
+	return
+}
+
+func Kill(pid int, signum Signal) (err error) {
+	_, _, e1 := sysvicall6(uintptr(unsafe.Pointer(&libc_Kill)), 2, uintptr(pid), uintptr(signum), 0, 0, 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+func Lchown(path string, uid int, gid int) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	_, _, e1 := sysvicall6(uintptr(unsafe.Pointer(&libc_Lchown)), 3, uintptr(unsafe.Pointer(_p0)), uintptr(uid), uintptr(gid), 0, 0, 0)
+	use(unsafe.Pointer(_p0))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+func Link(path string, link string) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	var _p1 *byte
+	_p1, err = BytePtrFromString(link)
+	if err != nil {
+		return
+	}
+	_, _, e1 := sysvicall6(uintptr(unsafe.Pointer(&libc_Link)), 2, uintptr(unsafe.Pointer(_p0)), uintptr(unsafe.Pointer(_p1)), 0, 0, 0, 0)
+	use(unsafe.Pointer(_p0))
+	use(unsafe.Pointer(_p1))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+func Listen(s int, backlog int) (err error) {
+	_, _, e1 := sysvicall6(uintptr(unsafe.Pointer(&libc_listen)), 2, uintptr(s), uintptr(backlog), 0, 0, 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+func Lstat(path string, stat *Stat_t) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	_, _, e1 := sysvicall6(uintptr(unsafe.Pointer(&libc_Lstat)), 2, uintptr(unsafe.Pointer(_p0)), uintptr(unsafe.Pointer(stat)), 0, 0, 0, 0)
+	use(unsafe.Pointer(_p0))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+func Mkdir(path string, mode uint32) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	_, _, e1 := sysvicall6(uintptr(unsafe.Pointer(&libc_Mkdir)), 2, uintptr(unsafe.Pointer(_p0)), uintptr(mode), 0, 0, 0, 0)
+	use(unsafe.Pointer(_p0))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+func Mknod(path string, mode uint32, dev int) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	_, _, e1 := sysvicall6(uintptr(unsafe.Pointer(&libc_Mknod)), 3, uintptr(unsafe.Pointer(_p0)), uintptr(mode), uintptr(dev), 0, 0, 0)
+	use(unsafe.Pointer(_p0))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+func Nanosleep(time *Timespec, leftover *Timespec) (err error) {
+	_, _, e1 := sysvicall6(uintptr(unsafe.Pointer(&libc_Nanosleep)), 2, uintptr(unsafe.Pointer(time)), uintptr(unsafe.Pointer(leftover)), 0, 0, 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+func Open(path string, mode int, perm uint32) (fd int, err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	r0, _, e1 := sysvicall6(uintptr(unsafe.Pointer(&libc_Open)), 3, uintptr(unsafe.Pointer(_p0)), uintptr(mode), uintptr(perm), 0, 0, 0)
+	use(unsafe.Pointer(_p0))
+	fd = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+func Pathconf(path string, name int) (val int, err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	r0, _, e1 := sysvicall6(uintptr(unsafe.Pointer(&libc_Pathconf)), 2, uintptr(unsafe.Pointer(_p0)), uintptr(name), 0, 0, 0, 0)
+	use(unsafe.Pointer(_p0))
+	val = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+func Pread(fd int, p []byte, offset int64) (n int, err error) {
+	var _p0 *byte
+	if len(p) > 0 {
+		_p0 = &p[0]
+	}
+	r0, _, e1 := sysvicall6(uintptr(unsafe.Pointer(&libc_Pread)), 4, uintptr(fd), uintptr(unsafe.Pointer(_p0)), uintptr(len(p)), uintptr(offset), 0, 0)
+	n = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+func Pwrite(fd int, p []byte, offset int64) (n int, err error) {
+	var _p0 *byte
+	if len(p) > 0 {
+		_p0 = &p[0]
+	}
+	r0, _, e1 := sysvicall6(uintptr(unsafe.Pointer(&libc_Pwrite)), 4, uintptr(fd), uintptr(unsafe.Pointer(_p0)), uintptr(len(p)), uintptr(offset), 0, 0)
+	n = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+func read(fd int, p []byte) (n int, err error) {
+	var _p0 *byte
+	if len(p) > 0 {
+		_p0 = &p[0]
+	}
+	r0, _, e1 := sysvicall6(uintptr(unsafe.Pointer(&libc_read)), 3, uintptr(fd), uintptr(unsafe.Pointer(_p0)), uintptr(len(p)), 0, 0, 0)
+	n = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+func Readlink(path string, buf []byte) (n int, err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	var _p1 *byte
+	if len(buf) > 0 {
+		_p1 = &buf[0]
+	}
+	r0, _, e1 := sysvicall6(uintptr(unsafe.Pointer(&libc_Readlink)), 3, uintptr(unsafe.Pointer(_p0)), uintptr(unsafe.Pointer(_p1)), uintptr(len(buf)), 0, 0, 0)
+	use(unsafe.Pointer(_p0))
+	n = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+func Rename(from string, to string) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(from)
+	if err != nil {
+		return
+	}
+	var _p1 *byte
+	_p1, err = BytePtrFromString(to)
+	if err != nil {
+		return
+	}
+	_, _, e1 := sysvicall6(uintptr(unsafe.Pointer(&libc_Rename)), 2, uintptr(unsafe.Pointer(_p0)), uintptr(unsafe.Pointer(_p1)), 0, 0, 0, 0)
+	use(unsafe.Pointer(_p0))
+	use(unsafe.Pointer(_p1))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+func Rmdir(path string) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	_, _, e1 := sysvicall6(uintptr(unsafe.Pointer(&libc_Rmdir)), 1, uintptr(unsafe.Pointer(_p0)), 0, 0, 0, 0, 0)
+	use(unsafe.Pointer(_p0))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+func Seek(fd int, offset int64, whence int) (newoffset int64, err error) {
+	r0, _, e1 := sysvicall6(uintptr(unsafe.Pointer(&libc_lseek)), 3, uintptr(fd), uintptr(offset), uintptr(whence), 0, 0, 0)
+	newoffset = int64(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+func sendfile(outfd int, infd int, offset *int64, count int) (written int, err error) {
+	r0, _, e1 := sysvicall6(uintptr(unsafe.Pointer(&libc_sendfile)), 4, uintptr(outfd), uintptr(infd), uintptr(unsafe.Pointer(offset)), uintptr(count), 0, 0)
+	written = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+func Setegid(egid int) (err error) {
+	_, _, e1 := rawSysvicall6(uintptr(unsafe.Pointer(&libc_Setegid)), 1, uintptr(egid), 0, 0, 0, 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+func Seteuid(euid int) (err error) {
+	_, _, e1 := rawSysvicall6(uintptr(unsafe.Pointer(&libc_Seteuid)), 1, uintptr(euid), 0, 0, 0, 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+func Setgid(gid int) (err error) {
+	_, _, e1 := rawSysvicall6(uintptr(unsafe.Pointer(&libc_Setgid)), 1, uintptr(gid), 0, 0, 0, 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+func Setpgid(pid int, pgid int) (err error) {
+	_, _, e1 := rawSysvicall6(uintptr(unsafe.Pointer(&libc_Setpgid)), 2, uintptr(pid), uintptr(pgid), 0, 0, 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+func Setpriority(which int, who int, prio int) (err error) {
+	_, _, e1 := sysvicall6(uintptr(unsafe.Pointer(&libc_Setpriority)), 3, uintptr(which), uintptr(who), uintptr(prio), 0, 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+func Setregid(rgid int, egid int) (err error) {
+	_, _, e1 := rawSysvicall6(uintptr(unsafe.Pointer(&libc_Setregid)), 2, uintptr(rgid), uintptr(egid), 0, 0, 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+func Setreuid(ruid int, euid int) (err error) {
+	_, _, e1 := rawSysvicall6(uintptr(unsafe.Pointer(&libc_Setreuid)), 2, uintptr(ruid), uintptr(euid), 0, 0, 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+func Setrlimit(which int, lim *Rlimit) (err error) {
+	_, _, e1 := rawSysvicall6(uintptr(unsafe.Pointer(&libc_Setrlimit)), 2, uintptr(which), uintptr(unsafe.Pointer(lim)), 0, 0, 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+func Setsid() (pid int, err error) {
+	r0, _, e1 := rawSysvicall6(uintptr(unsafe.Pointer(&libc_Setsid)), 0, 0, 0, 0, 0, 0, 0)
+	pid = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+func Setuid(uid int) (err error) {
+	_, _, e1 := rawSysvicall6(uintptr(unsafe.Pointer(&libc_Setuid)), 1, uintptr(uid), 0, 0, 0, 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+func Shutdown(s int, how int) (err error) {
+	_, _, e1 := sysvicall6(uintptr(unsafe.Pointer(&libc_shutdown)), 2, uintptr(s), uintptr(how), 0, 0, 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+func Stat(path string, stat *Stat_t) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	_, _, e1 := sysvicall6(uintptr(unsafe.Pointer(&libc_Stat)), 2, uintptr(unsafe.Pointer(_p0)), uintptr(unsafe.Pointer(stat)), 0, 0, 0, 0)
+	use(unsafe.Pointer(_p0))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+func Symlink(path string, link string) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	var _p1 *byte
+	_p1, err = BytePtrFromString(link)
+	if err != nil {
+		return
+	}
+	_, _, e1 := sysvicall6(uintptr(unsafe.Pointer(&libc_Symlink)), 2, uintptr(unsafe.Pointer(_p0)), uintptr(unsafe.Pointer(_p1)), 0, 0, 0, 0)
+	use(unsafe.Pointer(_p0))
+	use(unsafe.Pointer(_p1))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+func Sync() (err error) {
+	_, _, e1 := sysvicall6(uintptr(unsafe.Pointer(&libc_Sync)), 0, 0, 0, 0, 0, 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+func Truncate(path string, length int64) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	_, _, e1 := sysvicall6(uintptr(unsafe.Pointer(&libc_Truncate)), 2, uintptr(unsafe.Pointer(_p0)), uintptr(length), 0, 0, 0, 0)
+	use(unsafe.Pointer(_p0))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+func Fsync(fd int) (err error) {
+	_, _, e1 := sysvicall6(uintptr(unsafe.Pointer(&libc_Fsync)), 1, uintptr(fd), 0, 0, 0, 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+func Ftruncate(fd int, length int64) (err error) {
+	_, _, e1 := sysvicall6(uintptr(unsafe.Pointer(&libc_Ftruncate)), 2, uintptr(fd), uintptr(length), 0, 0, 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+func Umask(newmask int) (oldmask int) {
+	r0, _, _ := sysvicall6(uintptr(unsafe.Pointer(&libc_Umask)), 1, uintptr(newmask), 0, 0, 0, 0, 0)
+	oldmask = int(r0)
+	return
+}
+
+func Unlink(path string) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	_, _, e1 := sysvicall6(uintptr(unsafe.Pointer(&libc_Unlink)), 1, uintptr(unsafe.Pointer(_p0)), 0, 0, 0, 0, 0)
+	use(unsafe.Pointer(_p0))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+func Utimes(path string, times *[2]Timeval) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	_, _, e1 := sysvicall6(uintptr(unsafe.Pointer(&libc_Utimes)), 2, uintptr(unsafe.Pointer(_p0)), uintptr(unsafe.Pointer(times)), 0, 0, 0, 0)
+	use(unsafe.Pointer(_p0))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+func bind(s int, addr unsafe.Pointer, addrlen _Socklen) (err error) {
+	_, _, e1 := sysvicall6(uintptr(unsafe.Pointer(&libc_bind)), 3, uintptr(s), uintptr(addr), uintptr(addrlen), 0, 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+func connect(s int, addr unsafe.Pointer, addrlen _Socklen) (err error) {
+	_, _, e1 := sysvicall6(uintptr(unsafe.Pointer(&libc_connect)), 3, uintptr(s), uintptr(addr), uintptr(addrlen), 0, 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+func mmap(addr uintptr, length uintptr, prot int, flag int, fd int, pos int64) (ret uintptr, err error) {
+	r0, _, e1 := sysvicall6(uintptr(unsafe.Pointer(&libc_mmap)), 6, uintptr(addr), uintptr(length), uintptr(prot), uintptr(flag), uintptr(fd), uintptr(pos))
+	ret = uintptr(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+func munmap(addr uintptr, length uintptr) (err error) {
+	_, _, e1 := sysvicall6(uintptr(unsafe.Pointer(&libc_munmap)), 2, uintptr(addr), uintptr(length), 0, 0, 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+func sendto(s int, buf []byte, flags int, to unsafe.Pointer, addrlen _Socklen) (err error) {
+	var _p0 *byte
+	if len(buf) > 0 {
+		_p0 = &buf[0]
+	}
+	_, _, e1 := sysvicall6(uintptr(unsafe.Pointer(&libc_sendto)), 6, uintptr(s), uintptr(unsafe.Pointer(_p0)), uintptr(len(buf)), uintptr(flags), uintptr(to), uintptr(addrlen))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+func socket(domain int, typ int, proto int) (fd int, err error) {
+	r0, _, e1 := sysvicall6(uintptr(unsafe.Pointer(&libc_socket)), 3, uintptr(domain), uintptr(typ), uintptr(proto), 0, 0, 0)
+	fd = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+func socketpair(domain int, typ int, proto int, fd *[2]int32) (err error) {
+	_, _, e1 := rawSysvicall6(uintptr(unsafe.Pointer(&libc_socketpair)), 4, uintptr(domain), uintptr(typ), uintptr(proto), uintptr(unsafe.Pointer(fd)), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+func write(fd int, p []byte) (n int, err error) {
+	var _p0 *byte
+	if len(p) > 0 {
+		_p0 = &p[0]
+	}
+	r0, _, e1 := sysvicall6(uintptr(unsafe.Pointer(&libc_write)), 3, uintptr(fd), uintptr(unsafe.Pointer(_p0)), uintptr(len(p)), 0, 0, 0)
+	n = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+func getsockopt(s int, level int, name int, val unsafe.Pointer, vallen *_Socklen) (err error) {
+	_, _, e1 := sysvicall6(uintptr(unsafe.Pointer(&libc_getsockopt)), 5, uintptr(s), uintptr(level), uintptr(name), uintptr(val), uintptr(unsafe.Pointer(vallen)), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+func getpeername(fd int, rsa *RawSockaddrAny, addrlen *_Socklen) (err error) {
+	_, _, e1 := rawSysvicall6(uintptr(unsafe.Pointer(&libc_getpeername)), 3, uintptr(fd), uintptr(unsafe.Pointer(rsa)), uintptr(unsafe.Pointer(addrlen)), 0, 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+func getsockname(fd int, rsa *RawSockaddrAny, addrlen *_Socklen) (err error) {
+	_, _, e1 := sysvicall6(uintptr(unsafe.Pointer(&libc_getsockname)), 3, uintptr(fd), uintptr(unsafe.Pointer(rsa)), uintptr(unsafe.Pointer(addrlen)), 0, 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+func setsockopt(s int, level int, name int, val unsafe.Pointer, vallen uintptr) (err error) {
+	_, _, e1 := sysvicall6(uintptr(unsafe.Pointer(&libc_setsockopt)), 5, uintptr(s), uintptr(level), uintptr(name), uintptr(val), uintptr(vallen), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+func recvfrom(fd int, p []byte, flags int, from *RawSockaddrAny, fromlen *_Socklen) (n int, err error) {
+	var _p0 *byte
+	if len(p) > 0 {
+		_p0 = &p[0]
+	}
+	r0, _, e1 := sysvicall6(uintptr(unsafe.Pointer(&libc_recvfrom)), 6, uintptr(fd), uintptr(unsafe.Pointer(_p0)), uintptr(len(p)), uintptr(flags), uintptr(unsafe.Pointer(from)), uintptr(unsafe.Pointer(fromlen)))
+	n = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+func recvmsg(s int, msg *Msghdr, flags int) (n int, err error) {
+	r0, _, e1 := sysvicall6(uintptr(unsafe.Pointer(&libc_recvmsg)), 3, uintptr(s), uintptr(unsafe.Pointer(msg)), uintptr(flags), 0, 0, 0)
+	n = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
diff --git a/src/syscall/zsysnum_solaris_sparc64.go b/src/syscall/zsysnum_solaris_sparc64.go
new file mode 100644
index 0000000..0ae4f74
--- /dev/null
+++ b/src/syscall/zsysnum_solaris_sparc64.go
@@ -0,0 +1,13 @@
+// Copyright 2016 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// +build sparc64,solaris
+
+package syscall
+
+// TODO(aram): remove these before Go 1.3.
+const (
+	SYS_EXECVE = 59
+	SYS_FCNTL  = 62
+)
diff --git a/src/syscall/ztypes_solaris_sparc64.go b/src/syscall/ztypes_solaris_sparc64.go
new file mode 100644
index 0000000..0151cea
--- /dev/null
+++ b/src/syscall/ztypes_solaris_sparc64.go
@@ -0,0 +1,366 @@
+// Created by cgo -godefs - DO NOT EDIT
+// cgo -godefs types_solaris.go
+
+package syscall
+
+const (
+	sizeofPtr      = 0x8
+	sizeofShort    = 0x2
+	sizeofInt      = 0x4
+	sizeofLong     = 0x8
+	sizeofLongLong = 0x8
+	PathMax        = 0x400
+)
+
+type (
+	_C_short     int16
+	_C_int       int32
+	_C_long      int64
+	_C_long_long int64
+)
+
+type Timespec struct {
+	Sec  int64
+	Nsec int64
+}
+
+type Timeval struct {
+	Sec  int64
+	Usec int64
+}
+
+type Timeval32 struct {
+	Sec  int32
+	Usec int32
+}
+
+type Rusage struct {
+	Utime    Timeval
+	Stime    Timeval
+	Maxrss   int64
+	Ixrss    int64
+	Idrss    int64
+	Isrss    int64
+	Minflt   int64
+	Majflt   int64
+	Nswap    int64
+	Inblock  int64
+	Oublock  int64
+	Msgsnd   int64
+	Msgrcv   int64
+	Nsignals int64
+	Nvcsw    int64
+	Nivcsw   int64
+}
+
+type Rlimit struct {
+	Cur uint64
+	Max uint64
+}
+
+type _Gid_t uint32
+
+const (
+	S_IFMT   = 0xf000
+	S_IFIFO  = 0x1000
+	S_IFCHR  = 0x2000
+	S_IFDIR  = 0x4000
+	S_IFBLK  = 0x6000
+	S_IFREG  = 0x8000
+	S_IFLNK  = 0xa000
+	S_IFSOCK = 0xc000
+	S_ISUID  = 0x800
+	S_ISGID  = 0x400
+	S_ISVTX  = 0x200
+	S_IRUSR  = 0x100
+	S_IWUSR  = 0x80
+	S_IXUSR  = 0x40
+)
+
+type Stat_t struct {
+	Dev       uint64
+	Ino       uint64
+	Mode      uint32
+	Nlink     uint32
+	Uid       uint32
+	Gid       uint32
+	Rdev      uint64
+	Size      int64
+	Atim      Timespec
+	Mtim      Timespec
+	Ctim      Timespec
+	Blksize   int32
+	Pad_cgo_0 [4]byte
+	Blocks    int64
+	Fstype    [16]int8
+}
+
+type Flock_t struct {
+	Type      int16
+	Whence    int16
+	Pad_cgo_0 [4]byte
+	Start     int64
+	Len       int64
+	Sysid     int32
+	Pid       int32
+	Pad       [4]int64
+}
+
+type Dirent struct {
+	Ino       uint64
+	Off       int64
+	Reclen    uint16
+	Name      [1]int8
+	Pad_cgo_0 [5]byte
+}
+
+type RawSockaddrInet4 struct {
+	Family uint16
+	Port   uint16
+	Addr   [4]byte /* in_addr */
+	Zero   [8]int8
+}
+
+type RawSockaddrInet6 struct {
+	Family         uint16
+	Port           uint16
+	Flowinfo       uint32
+	Addr           [16]byte /* in6_addr */
+	Scope_id       uint32
+	X__sin6_src_id uint32
+}
+
+type RawSockaddrUnix struct {
+	Family uint16
+	Path   [108]int8
+}
+
+type RawSockaddrDatalink struct {
+	Family uint16
+	Index  uint16
+	Type   uint8
+	Nlen   uint8
+	Alen   uint8
+	Slen   uint8
+	Data   [244]int8
+}
+
+type RawSockaddr struct {
+	Family uint16
+	Data   [14]int8
+}
+
+type RawSockaddrAny struct {
+	Addr RawSockaddr
+	Pad  [236]int8
+}
+
+type _Socklen uint32
+
+type Linger struct {
+	Onoff  int32
+	Linger int32
+}
+
+type Iovec struct {
+	Base *int8
+	Len  uint64
+}
+
+type IPMreq struct {
+	Multiaddr [4]byte /* in_addr */
+	Interface [4]byte /* in_addr */
+}
+
+type IPv6Mreq struct {
+	Multiaddr [16]byte /* in6_addr */
+	Interface uint32
+}
+
+type Msghdr struct {
+	Name         *byte
+	Namelen      uint32
+	Pad_cgo_0    [4]byte
+	Iov          *Iovec
+	Iovlen       int32
+	Pad_cgo_1    [4]byte
+	Accrights    *int8
+	Accrightslen int32
+	Pad_cgo_2    [4]byte
+}
+
+type Cmsghdr struct {
+	Len   uint32
+	Level int32
+	Type  int32
+}
+
+type Inet6Pktinfo struct {
+	Addr    [16]byte /* in6_addr */
+	Ifindex uint32
+}
+
+type IPv6MTUInfo struct {
+	Addr RawSockaddrInet6
+	Mtu  uint32
+}
+
+type ICMPv6Filter struct {
+	X__icmp6_filt [8]uint32
+}
+
+const (
+	SizeofSockaddrInet4    = 0x10
+	SizeofSockaddrInet6    = 0x20
+	SizeofSockaddrAny      = 0xfc
+	SizeofSockaddrUnix     = 0x6e
+	SizeofSockaddrDatalink = 0xfc
+	SizeofLinger           = 0x8
+	SizeofIPMreq           = 0x8
+	SizeofIPv6Mreq         = 0x14
+	SizeofMsghdr           = 0x30
+	SizeofCmsghdr          = 0xc
+	SizeofInet6Pktinfo     = 0x14
+	SizeofIPv6MTUInfo      = 0x24
+	SizeofICMPv6Filter     = 0x20
+)
+
+type FdSet struct {
+	Bits [1024]uint64
+}
+
+const (
+	SizeofIfMsghdr  = 0x54
+	SizeofIfData    = 0x44
+	SizeofIfaMsghdr = 0x14
+	SizeofRtMsghdr  = 0x4c
+	SizeofRtMetrics = 0x28
+)
+
+type IfMsghdr struct {
+	Msglen    uint16
+	Version   uint8
+	Type      uint8
+	Addrs     int32
+	Flags     int32
+	Index     uint16
+	Pad_cgo_0 [2]byte
+	Data      IfData
+}
+
+type IfData struct {
+	Type       uint8
+	Addrlen    uint8
+	Hdrlen     uint8
+	Pad_cgo_0  [1]byte
+	Mtu        uint32
+	Metric     uint32
+	Baudrate   uint32
+	Ipackets   uint32
+	Ierrors    uint32
+	Opackets   uint32
+	Oerrors    uint32
+	Collisions uint32
+	Ibytes     uint32
+	Obytes     uint32
+	Imcasts    uint32
+	Omcasts    uint32
+	Iqdrops    uint32
+	Noproto    uint32
+	Lastchange Timeval32
+}
+
+type IfaMsghdr struct {
+	Msglen    uint16
+	Version   uint8
+	Type      uint8
+	Addrs     int32
+	Flags     int32
+	Index     uint16
+	Pad_cgo_0 [2]byte
+	Metric    int32
+}
+
+type RtMsghdr struct {
+	Msglen    uint16
+	Version   uint8
+	Type      uint8
+	Index     uint16
+	Pad_cgo_0 [2]byte
+	Flags     int32
+	Addrs     int32
+	Pid       int32
+	Seq       int32
+	Errno     int32
+	Use       int32
+	Inits     uint32
+	Rmx       RtMetrics
+}
+
+type RtMetrics struct {
+	Locks    uint32
+	Mtu      uint32
+	Hopcount uint32
+	Expire   uint32
+	Recvpipe uint32
+	Sendpipe uint32
+	Ssthresh uint32
+	Rtt      uint32
+	Rttvar   uint32
+	Pksent   uint32
+}
+
+const (
+	SizeofBpfVersion = 0x4
+	SizeofBpfStat    = 0x80
+	SizeofBpfProgram = 0x10
+	SizeofBpfInsn    = 0x8
+	SizeofBpfHdr     = 0x14
+)
+
+type BpfVersion struct {
+	Major uint16
+	Minor uint16
+}
+
+type BpfStat struct {
+	Recv    uint64
+	Drop    uint64
+	Capt    uint64
+	Padding [13]uint64
+}
+
+type BpfProgram struct {
+	Len       uint32
+	Pad_cgo_0 [4]byte
+	Insns     *BpfInsn
+}
+
+type BpfInsn struct {
+	Code uint16
+	Jt   uint8
+	Jf   uint8
+	K    uint32
+}
+
+type BpfTimeval struct {
+	Sec  int32
+	Usec int32
+}
+
+type BpfHdr struct {
+	Tstamp    BpfTimeval
+	Caplen    uint32
+	Datalen   uint32
+	Hdrlen    uint16
+	Pad_cgo_0 [2]byte
+}
+
+type Termios struct {
+	Iflag     uint32
+	Oflag     uint32
+	Cflag     uint32
+	Lflag     uint32
+	Cc        [19]uint8
+	Pad_cgo_0 [1]byte
+}
diff --git a/test/chanlinear.go b/test/chanlinear.go
index 55fee4a..8245d67 100644
--- a/test/chanlinear.go
+++ b/test/chanlinear.go
@@ -1,4 +1,4 @@
-// +build darwin linux
+// +build darwin linux solaris
 // run
 
 // Copyright 2014 The Go Authors. All rights reserved.
diff --git a/test/gconly.go b/test/gconly.go
new file mode 100644
index 0000000..ee541fe
--- /dev/null
+++ b/test/gconly.go
@@ -0,0 +1,10 @@
+// run
+package main
+
+import (
+	"runtime"
+)
+
+func main() {
+        runtime.GC()
+}
diff --git a/test/gcsweep.go b/test/gcsweep.go
new file mode 100644
index 0000000..4db0b47
--- /dev/null
+++ b/test/gcsweep.go
@@ -0,0 +1,18 @@
+// run
+package main
+
+import (
+	. "reflect"
+	"runtime"
+)
+
+func main() {
+        type T int
+        st := SliceOf(TypeOf(T(1)))
+        v := MakeSlice(st, 1, 1)
+        runtime.GC()
+        for i := 0; i < v.Len(); i++ {
+                v.Index(i).Set(ValueOf(T(i)))
+                runtime.GC()
+        }
+}
diff --git a/test/map2.go b/test/map2.go
new file mode 100644
index 0000000..b3ba71a
--- /dev/null
+++ b/test/map2.go
@@ -0,0 +1,34 @@
+// run
+
+package main
+
+func main() {
+	const (
+		I = 103
+		N = 3
+	)
+	type T [N]int
+	m := make(map[T]int)
+	for i := 0; i < I; i++ {
+		var v T
+		for j := 0; j < N; j++ {
+			v[j] = i + j
+		}
+		m[v] = i
+	}
+
+	m[T{104, 105, 106}] = 104
+
+	t0 := T{100, 101, 102}
+	v := m[t0]
+	if v != 100 {
+		panic("map lookup failure")
+	}
+
+	m[T{105, 106, 107}] = 105
+
+	v = m[t0]
+	if v != 100 {
+		panic("map lookup failure")
+	}
+}
diff --git a/test/maplinear.go b/test/maplinear.go
index 34d0914..b8b1119 100644
--- a/test/maplinear.go
+++ b/test/maplinear.go
@@ -1,4 +1,4 @@
-// +build darwin linux
+// +build darwin linux solaris
 // run
 
 // Copyright 2013 The Go Authors. All rights reserved.
diff --git a/test/nilptr3.go b/test/nilptr3.go
index 8922729..de33c43 100644
--- a/test/nilptr3.go
+++ b/test/nilptr3.go
@@ -1,8 +1,8 @@
 // errorcheck -0 -d=nil
 // Fails on ppc64x because of incomplete optimization.
 // See issues 9058.
-// Same reason for mips64x and s390x.
-// +build !ppc64,!ppc64le,!mips64,!mips64le,!amd64,!s390x
+// Same reason for mips64x, s390x, and sparc64.
+// +build !ppc64,!ppc64le,!mips64,!mips64le,!amd64,!s390x,!sparc64
 
 // Copyright 2013 The Go Authors. All rights reserved.
 // Use of this source code is governed by a BSD-style
diff --git a/test/nosplit.go b/test/nosplit.go
index a58a645..a262fc9 100644
--- a/test/nosplit.go
+++ b/test/nosplit.go
@@ -1,4 +1,4 @@
-// +build !nacl
+// +build !nacl,!sparc64
 // run
 
 // Copyright 2014 The Go Authors. All rights reserved.
diff --git a/test/recover5.go b/test/recover5.go
new file mode 100644
index 0000000..944a833
--- /dev/null
+++ b/test/recover5.go
@@ -0,0 +1,13 @@
+// run
+
+package main
+
+func main() {
+	defer func() {
+		p := recover()
+		if p != "test panic" {
+			panic("recover failed")
+		}
+	}()
+	panic("test panic")
+}
diff --git a/test/sleep.go b/test/sleep.go
new file mode 100644
index 0000000..23eb103
--- /dev/null
+++ b/test/sleep.go
@@ -0,0 +1,22 @@
+// run
+
+package main
+
+import (
+	"time"
+)
+
+func main() {
+	const delay time.Duration = 123456789
+	go func() {
+		time.Sleep(delay / 2)
+	}()
+	start := time.Now()
+	time.Sleep(delay)
+	delayadj := delay
+	duration := time.Now().Sub(start)
+	if duration < delayadj {
+		print("time.Sleep(", delay, ") slept for only ", duration, "ns\n")
+		panic("FAIL")
+	}
+}
diff --git a/test/stackrecurse.go b/test/stackrecurse.go
new file mode 100644
index 0000000..6ca3120
--- /dev/null
+++ b/test/stackrecurse.go
@@ -0,0 +1,28 @@
+// run
+
+// Copyright 2016 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+package main
+
+import (
+	"fmt"
+	"regexp/syntax"
+)
+
+// Verify that stack growth works as expected for heavily-recursive
+// functions such as writeRegexp, the one triggered below, when printing a
+// regex to a buffer.
+func main() {
+	text := `^x{1,1000}y{1,1000}$`
+	re, err := syntax.Parse(text, syntax.Perl)
+	if err != nil {
+		panic(fmt.Sprintf("parse: %v", err))
+	}
+
+	sre := re.Simplify()
+	buf := fmt.Sprintf("	%+v\n", sre)
+	if len(buf) != 11993 {
+		panic(fmt.Sprintf("simplified regex not expected length: %d", len(buf)))
+	}
+}
diff --git a/test/strindex.go b/test/strindex.go
new file mode 100644
index 0000000..25f1a3b
--- /dev/null
+++ b/test/strindex.go
@@ -0,0 +1,12 @@
+// run
+
+package main
+
+import "strings"
+
+func main() {
+	str := "hellolllo"
+	if strings.IndexByte(str, byte('l')) != 2 {
+		panic("strings.Index broken")
+	}
+}
